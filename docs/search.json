[{"path":"index.html","id":"bienvenida","chapter":"Bienvenida","heading":"Bienvenida","text":"\nEste libro incluye los contenidos habitualmente presentes en el currículo\nde asignaturas de Estadística de los grados Ciencias e Ingenierías de universidades españolas. Aunque aparezca en el título, el manual incluye también los contenidos de Probabilidad necesarios.\nSi bien existe abundante material bibliográfico\nque cubre los contenidos de estas asignaturas, quería elaborar un material\npropio que fuera solamente para mis clases sino algo más\nglobal. En los últimos años ya lo hice para asignaturas de grado y Máster en ADE.1 Por otra parte, motiva cubrir el hueco de los materiales\nde acceso gratuito con la opción de comprar una edición\nimpresa2 y con el enfoque\nque se menciona en el siguiente apartado. Por otra parte, los libros publicados\noriginalmente en inglés y traducidos al español menudo resultan lejanos\nnuestro idioma (por muy buenas que sean las traducciones, los ejemplos en acres\nson muy intuitivos para un lector español). Espero que también sirva para\nlectores de otros países de habla hispana.","code":""},{"path":"index.html","id":"estándares-y-software","chapter":"Bienvenida","heading":"Estándares y software","text":"Los contenidos de este libro se basan en dos paradigmas que están presentes\nen los intereses de investigación y docencia del autor: los estándares y\nel software libre. En lo que se refiere estándares, la notación utilizada,\ndefiniciones y fórmulas se ajustarán el máximo posible la utilizada en normas\nnacionales e internacionales sobre metodología estadística. Estas normas se\ncitarán pertinentemente lo largo del texto. En cuanto al software libre,\nse proporcionarán instrucciones para resolver los ejemplos\nque ilustran la teoría utilizando software libre.\nobstante, el uso del software es\nauxiliar al texto y se puede seguir sin necesidad de utilizar\nlos programas. Según lo que proceda en cada caso, se utilizará\nsoftware de hoja de cálculo, el software estadístico y lenguaje de\nprogramación\nR,3\ny el software de álgebra computacional Máxima4.\nRespecto al software de hoja de cálculo, las fórmulas utilizadas se han probado\nen el software libre LibreOffice5, en Hojas de Cálculo de Google6 y\ntambién en Microsoft EXCEL7 que,\naunque es software libre, su uso\nestá más que generalizado y normalmente los estudiantes disponen de licencia de uso\ntravés de su universidad. En caso de que el nombre de la función sea distinta\nen EXCEL, se indicará en el propio ejemplo.Las normas son clave para el desarrollo económico de un país. Estudios en diversos países,\nincluido España, han demostrado que la aportación de la normalización su economía es del 1% del PIB8. La\nAsociación Española de Normalización (UNE) es el organismo legalmente\nresponsable del desarrollo y difusión de las normas técnicas en España.\nAdemás, representa España en los organismos internacionales de normalización como\nISO9 y CEN10.Las normas sobre estadística que surgen de ISO las elabora el Technical Committee\nISO TC 6911 Statistical Methods.\nPor su parte, el subcomité técnico de normalización\nCTN 66/SC 312, Métodos Estadísticos,\nparticipa como miembro nacional en ese comité ISO.\nLas normas que son de interés en España, se ratifican en inglés o se traducen\nal español como normas UNE. Para una descripción más completa de la elaboración\nde normas, véase Emilio L. Cano, Javier M. Moguerza, Mariano Prieto Corcoba.13","code":""},{"path":"index.html","id":"estructura-del-libro","chapter":"Bienvenida","heading":"Estructura del libro","text":"Este libro se ha elaborado utilizando el lenguaje Markdown con el propio\nsoftware R y el paquete bookdown.14\nSe incluyen una gran cantidad de ejemplos resueltos tanto de forma analítica\ncomo mediante software. En algunos casos se proporciona el uso de funciones\nen hojas de cálculo (y el resultado obtenido con un recuadro).\nEn otros, código de R, que aparecen en el texto\nsombreados y con la sintaxis coloreada, como el fragmento continuación\ndonde se puede comprobar la sesión de R en la que ha sido generado este material.\nObsérvese que los resultados se muestran precedidos de los símbolos\n#>.Normalmente, la descripción o enunciado de los ejemplos se incluyen en bloques\ncon el siguiente aspecto:Esto es un ejemplo. continuación puede mostrarse código o . Los ejemplos\npueden ir precedidos por un icono para identificar su campo de aplicación, por\nejemplo  Biología,  Ciencia y tecnología de Alimentos, o  Ciencia e Ingeniería Ambiental.Cuando el ejemplo incluya explicaciones sobre cómo resolverlo con software,\nestas explicaciones aparecerán en bloques con el siguiente aspecto:HOJA DE CÁLCULOLa función FACT obtiene el factorial de un número x (\\(x!\\)):=FACT(5)\n\\(\\boxed{\\mathsf{120}}\\)También se incluirán con el formato anterior indicaciones para usar la calculadora\ncientífica, cuando esto sea posible.El texto incluye otros bloques con información de distinto tipo, como los siguientes:Este contenido se considera avanzado. El lector principiante puede saltarse estos apartados\ny volver sobre ellos en una segunda lectura.Estos bloques están pensados para incluir información curiosa o complementaria\npara poner en contexto las explicaciones.Este volumen cubre los contenidos de asignaturas básicas de Estadística en un\namplio rango de grados. Puede servir también como repaso\npara alumnos de posgrado o incluso egresados que necesiten refrescar\nconocimientos o aprender aplicarlos con software moderno. Un segundo volumen cubrirá en el futuro métodos y modelos avanzados para\nentornos más exigentes.El libro está dividido en 4 partes. La primera parte está dedicada la Estadística\nDescriptiva, y consta de un capítulo introductorio seguido de sendos capítulos\npara el análisis exploratorio univariante y bivariante. La segunda parte\ntrata la Probabilidad en 4 capítulos, uno introductorio, dos dedicados las\nvariables\naleatorias univariantes y bivariantes respectivamente, y finalmente un capítulo\nque trata los modelos de distribución de probabilidad. En la tercera parte se\naborda la inferencia estadística, con una introducción al muestreo y la\nestimación puntual, seguida de capítulos dedicados los contrastes de\ncomparación de grupos, análisis de regresión y diseño de experimentos. La última\nparte está dedicada al control estadístico de la calidad, en la que,\ntras un capítulo introductorio, se tratan las dos herramientas más importantes\nen este campo: el control estadístico de procesos (SPC, Statistical Process\nControl, por sus siglas en inglés) y los muestreos de aceptación o, dicho de\notra forma, la inspección por muestreo. Finalmente, una serie de apéndices\ncon diverso material complementan el libro en su conjunto.","code":"\nsessionInfo()\n#> R version 4.1.2 (2021-11-01)\n#> Platform: x86_64-apple-darwin17.0 (64-bit)\n#> Running under: macOS Big Sur 10.16\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] es_ES.UTF-8/es_ES.UTF-8/es_ES.UTF-8/C/es_ES.UTF-8/es_ES.UTF-8\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets \n#> [6] methods   base     \n#> \n#> other attached packages:\n#> [1] fontawesome_0.2.2\n#> \n#> loaded via a namespace (and not attached):\n#>  [1] bookdown_0.24.3 digest_0.6.29   R6_2.5.1       \n#>  [4] jsonlite_1.7.3  magrittr_2.0.1  evaluate_0.14  \n#>  [7] stringi_1.7.6   cachem_1.0.6    rlang_0.4.12   \n#> [10] fs_1.5.2        jquerylib_0.1.4 xml2_1.3.3     \n#> [13] bslib_0.3.1     rmarkdown_2.11  tools_4.1.2    \n#> [16] stringr_1.4.0   xfun_0.29       yaml_2.2.1     \n#> [19] fastmap_1.1.0   compiler_4.1.2  memoise_2.0.1  \n#> [22] htmltools_0.5.2 knitr_1.37      downlit_0.4.0  \n#> [25] sass_0.4.0"},{"path":"index.html","id":"sobre-el-autor","chapter":"Bienvenida","heading":"Sobre el autor","text":"Actualmente soy Profesor Contratado Doctor en la Escuela Técnica Superior de Ingeniería Informática e investigador en el Data Science Laboratory de la Universidad Rey Juan Carlos. Mis intereses de investigación incluyen Estadística Aplicada, Aprendizaje Estadístico y Metodologías para la Calidad. Previamente sido profesor e investigador en la Universidad de Castilla-La Mancha, donde sigo colaborando en docencia e investigación, y Estadístico en empresas del sector privado de diversos sectores.Presidente del subcomité técnico de normalización UNE (miembro de ISO) CTN 66/SC 3 (Métodos Estadísticos). Profesor en la Asociación Española para la Calidad (AEC). Presidente de la asociación Comunidad R Hispano.Más sobre mí, información actualizada y publicaciones: http://emilio.lcano.com.\nContacto: emilio@lcano.comEl material se proporciona bajo licencia CC--NC-ND.\nTodos los logotipos y marcas comerciales que puedan aparecer en este texto\nson propiedad de sus respectivos dueños y se incluyen en este texto únicamente\ncon fines formativos. Se ha puesto especial cuidado en la adecuada atribución\ndel material elaborado por el autor, véase el Apéndice F.\nAún así, si detecta algún uso\nindebido de material protegido póngase en contacto con el autor y será retirado.\nIgualmente, contacte con el autor si desea utilizar este material con fines\ncomerciales.Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional.","code":""},{"path":"index.html","id":"agradecimientos","chapter":"Bienvenida","heading":"Agradecimientos","text":"Este libro es el resultado de años de trabajo en la docencia, investigación\ny transferencia de conocimiento en el campo de la Estadística. Está construido\npartir de las contribuciones lo largo de los años de compañeros y amigos\ncomo Javier M. Moguerza, Andrés Redchuk, David Ríos, Felipe Ortega, Mariano Prieto,\nMiguel Ángel Tarancón, Víctor M. Casero, Virgilio Gómez-Rubio, Matías Gámez, y\nmuchos otros (perdón\nl@s omitid@s por ser más exhaustivo).Especial agradecimiento toda la comunidad del software libre y\nlenguaje de programación R, y en particular al R Core Team y al equipo\nde RStudio.","code":""},{"path":"intro.html","id":"intro","chapter":"Capítulo 1 Introducción","heading":"Capítulo 1 Introducción","text":"","code":""},{"path":"intro.html","id":"estadística-y-análisis-de-datos","chapter":"Capítulo 1 Introducción","heading":"1.1 Estadística y análisis de datos","text":"","code":""},{"path":"intro.html","id":"qué-es-la-estadística","chapter":"Capítulo 1 Introducción","heading":"1.1.1 ¿Qué es la Estadística?","text":"Antes de introducirnos en el estudio de la Estadística y sus métodos, vamos \nintentar tener una visión de todo lo que abarca. Así pues, ¿qué es la Estadística?\nLa primera fuente que podemos consultar es la definición de la Real Academia Española,\ny encontramos estas acepciones:estadístico, caLa forma f., del al. Statistik, y este der. del . statista ‘hombre de Estado.’adj. Perteneciente o relativo la estadística.y f. Especialista en estadística.\ny f. Especialista en estadística.Estudio de los datos cuantitativos de la población, de los recursos naturales e industriales, del tráfico o de cualquier otra manifestación de las sociedades humanas.\nEstudio de los datos cuantitativos de la población, de los recursos naturales e industriales, del tráfico o de cualquier otra manifestación de las sociedades humanas.Conjunto de datos estadísticos.\nConjunto de datos estadísticos.Rama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades.\nRama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades.RAELas acepciones que nos interesan son sobre todo la tercera y la cuarta, en las\nque aparecen conceptos\nque veremos en este capítulo introductorio y en los que profundizaremos en el resto\ndel libro. La tercera acepción, “Conjunto de datos estadísticos,” es lo que muchas\npersonas entienden cuando oyen la palabra Estadística: La estadística del paro,\nla estadística de los precios, etc. Pero la Estadística es mucho más amplia.\nEn primer lugar, esos “datos estadísticos” han tenido que ser recopilados y\ntratados de alguna forma antes de llegar su publicación. Además, los datos\nestadísticos así entendidos son el resultado de un estudio pormenorizado\n(acepción 3) y normalmente de la aplicación de técnicas de inferencia\n(acepción 5). Algunas de estas técnicas forma parte de lo que vulgarmente\nse conoce como “la cocina” de las estadísticas.Podemos hablar entonces de la Estadística, de forma muy resumida,\ncomo la ciencia de analizar datos. Encontramos menudo15 una definición\nde la Estadística como “la ciencia que establece los métodos necesarios para la recolección, organización, presentación y análisis de datos relativos \nun conjunto de elementos o individuos.” Pero esta definición se centra solo\nen los métodos. Una definición más completa sería la siguiente:[…] la estadística es la parte de la matemática que estudia la variabilidad y el proceso aleatorio que la genera siguiendo leyes de probabilidad.Esta variabilidad puede ser debida al azar, o bien estar producida por causas ajenas él, correspondiendo al razonamiento estadístico diferenciar entre la variabilidad casual y la variabilidad causal.R Ocaña-Riola16Aquí vemos uno de los conceptos clave que guiará todo el estudio y aplicación de\nla Estadística: la variabilidad es la clave de todo. Entender el concepto de\nvariabilidad ayudará enormemente entender los métodos por complejos que sean.Variation reason statisticsEmilio L. Cano, Javier M. Moguerza, Andrés Redchuk17La Estadística ha sido siempre importante en los estudios de Ciencias e\nIngeniería. obstante, en los últimos tiempos la alta disponibilidad\ntanto de datos como de tecnología para tratarlos, hace imprescindible\nun dominio de las técnicas estadísticas y su aplicación en el dominio\nespecífico.","code":""},{"path":"intro.html","id":"los-dos-grandes-bloques-de-la-estadística","chapter":"Capítulo 1 Introducción","heading":"1.1.2 Los dos grandes bloques de la Estadística","text":"La Estadística se divide en dos grandes bloques de estudio, que son\nla Estadística Descriptiva y la Inferencia Estadística. la\nEstadística Descriptiva también se la conoce como Análisis Exploratorio de Datos\n(EDA, Exploratory Data Analysis, por sus siglas en inglés).\nEsta disciplina tuvo un gran desarrollo gracias al trabajo de Tukey,18\nque todavía hoy es una referencia. Pero en los últimos años ha cobrado si cabe\nmás importancia por la alta disponibilidad de datos y la necesidad de analizarlos.La Estadística Descriptiva se aplica sobre un conjunto de datos concretos,\ndel que obtenemos resúmenes numéricos y visualización de datos través de\nlos gráficos apropiados. Con la Estadística Descriptiva se identifican relaciones\ny patrones, guiando el trabajo posterior de la Inferencia Estadística.La Estadística Inferencial utiliza los datos y su análisis anterior para, través de las\nLeyes de la Probabilidad, obtener conclusiones de diverso tipo, como explicación de fenómenos,\nconfirmación de relaciones de causa-efecto, realizar predicciones o comparar grupos.\nEn definitiva, tomar decisiones por medio de modelos estadísticos y basadas en\nlos datos.","code":""},{"path":"intro.html","id":"la-esencia-de-la-estadística","chapter":"Capítulo 1 Introducción","heading":"1.1.3 La esencia de la Estadística","text":"La figura 1.1 representa\nla esencia de la Estadística y sus métodos. Estudiamos alguna característica\nobservable en una serie de elementos (sujetos, individuos, …)\nidentificables y únicos. Los datos que analizamos,\nprovienen de una determinada población que es objeto de estudio.\nPero estos datos, son más que\nuna muestra, es decir, un subconjunto representativo de la población. Incluso\ncuando “creemos” que tenemos todos los datos, debemos tener presente que trabajamos\ncon muestras, ya que generalmente tomaremos decisiones o llegaremos conclusiones\nsobre el futuro, y esos datos seguro que los tenemos. Por eso es importante\nconsiderar siempre este paradigma población-muestra, donde la\npoblación es desconocida y sus propiedades teóricas. La Estadística Descriptiva\nse ocupa del análisis exploratorio de datos en sentido amplio, que aplicaremos\nsobre los datos concretos de la muestra en este unidad y la siguiente. La\nInferencia Estadística hace referencia los métodos mediante los cuales,\ntravés de los datos de la muestra, tomaremos decisiones, explicaremos relaciones,\no haremos predicciones sobre la población. Para ello, haremos uso de la\nProbabilidad, que veremos más adelante, aplicando\nel método más adecuado.\nEn estos métodos será muy importante considerar el método de obtención de la muestra\nque, en términos generales, debe ser representativa de la población para que las\nconclusiones sean válidas.\nFigura 1.1: La esencia de los métodos estadísticos\n En un ensayo clínico, se eligen una serie de participantes en el estudio los\nque se le suministran distintos tratamientos según el diseño del ensayo.\nLos participantes en el estudio son sujetos que constituyen la muestra.\ntravés de los resultados de esta muestra, obtendremos conclusiones\npara toda la población, que estará definida en el propio ensayo clínico.\nPor ejemplo, en el estudio del efecto de un determinado tratamiento para la\ndiabetes, la población serían todos los enfermos de diabetes.Otro concepto clave inherente la Estadística, es que casi siempre estaremos\ninvestigando sobre esta fórmula:\\[Y=f(X)\\]Es decir, buscamos encontrar la relación entre una variable respuesta \\(Y\\) y una o varias\nvariables explicativas \\(X\\). Casi toda la Ciencia de Datos consiste en encontrar esa \\(f\\).\nEs fundamental interiorizar este concepto para después aplicar el método adecuado,\nya que según sean la/s \\(Y\\), la/s \\(X\\) y el objetivo de nuestro estudio, los caminos\npueden ser muy diferentes.El origen del término Data Science se suele atribuir Bill Cleveland tras la publicación de su artículo “Data Science: Action Plan Expanding Technical Areas Field Statistics” en 2001,1920, aunque lo anticipó Tukey 40 años antes en “Future Data Analysis”21 . obstante, es partir del año 2010, con la irrupción del Big Data y la necesidad de analizar grandes cantidades de datos, cuando se empieza popularizar el término intentando dar una definición gráfica de la profesión (Data Scientist). Así, es muy común presentar la ciencia de datos como la intersección de los conocimientos informáticos, los conocimientos estadístico-matemáticos, y el conocimiento de la materia en estudio (negocio, campo científico, etc.). Así, la persona de ciencias o ingeniería, con evidentes conocimientos en su campo, que adquiera conocimientos de Estadística y sea capaz de utilizar software avanzado como R, es uno de los perfiles más demandados.Paralelamente la Ciencia de Datos, aparecen términos más recientes como Big Data, Internet Things o Industria 4.0. Detrás de todos ellos, está el análisis estadístico. Y la mayoría de las veces es suficiente aplicar los métodos más básicos para solucionar los problemas o demostrar las hipótesis.","code":""},{"path":"intro.html","id":"los-datos-y-su-organización","chapter":"Capítulo 1 Introducción","heading":"1.2 Los datos y su organización","text":"","code":""},{"path":"intro.html","id":"características-y-variables","chapter":"Capítulo 1 Introducción","heading":"1.2.1 Características y variables","text":"Las características que observamos en los elementos de la muestra\n(o que estudiamos en una población) pueden ser distintos tipos. Nos referiremos\ngenéricamente estas características como variables, aunque en en algunos\námbitos como el Control Estadístico de Procesos (SPC, Statistical Process Control\npor sus siglas en inglés) este término se refiere solo las variables continuas\nque ahora definiremos.Denotaremos las variables con letras mayúsculas del alfabeto latino (\\(X\\), \\(Y\\), \\(\\), …).\nCuando observamos la característica, la variable toma un valor. Estos valores\npueden ser agrupados en clases, de forma que cada posible valor\npertenezca una y solo una clase. En ocasiones los datos con los que trabajamos\nestán ya clasificados en clases. Las variables pueden tomar cualquier valor en su dominio, es decir, el conjunto de posibles valores que puede tomar la variable. Veremos más adelante cómo cuantificar esas posibilidades través de la Probabilidad.Cuando se recogen datos utilizando cuestionarios, menudo en las preguntas\npara recoger características cuantitativas se ofrece elegir un intervalo en vez\nde peguntar el valor exacto. Por ejemplo, al preguntar la edad de una\npersona, se pueden dar las opciones: 1) menos de 20 años; 2) entre 20 y 40 años;\n3) entre 40 y 60 años; 4) Más de 60 años. Así, si una persona tiene 30 años, el valor\nde la variable es 30 (en el caso de la encuesta lo conoceremos exactamente)\nque pertenece la clase “entre 20 y 40 años.”","code":""},{"path":"intro.html","id":"parámetros-y-estadísticos","chapter":"Capítulo 1 Introducción","heading":"1.2.2 Parámetros y estadísticos","text":"Distinguiremos la caracterización de las variables que estudiamos en la población de las observadas en la muestra denotándolas por parámetros y estadísticos respectivamente. Los parámetros son valores teóricos, casi siempre desconocidos, sobre los que haremos inferencia. Los denotaremos por letras griegas minúsculas, como por ejemplo \\(\\mu\\) para la media poblacional.\nUn estadístico es una función definida sobre los datos de una muestra. Pueden ser valores de más de una variable, y los resumiremos en un único valor, resultado de aplicar esa función. Los estadísticos tomarán valores distintos dependiendo de la muestra concreta. Esto hace que sean su vez variables, y\nque tengan una distribución en el muestreo que nos permitirá hacer inferencia sobre la población. Los denotaremos con letras latinas, como por ejemplo \\(\\bar x\\) para la media muestra.La figura 1.2 representa la esencia de la estadística relacionando parámetros y estadísticos. Además de la equivalencia entre parámetros y estadísticos, la distribución de frecuencias de los datos de la muestra representada en el histograma se corresponde con la distribución de probabilidad teórica de la población.\nFigura 1.2: La esencia de los métodos estadísticos\n","code":""},{"path":"intro.html","id":"la-inferencia-y-sus-métodos","chapter":"Capítulo 1 Introducción","heading":"1.2.3 La inferencia y sus métodos","text":"Existen dos grandes grupos de métodos para hacer la inferencia sobre la población. La estadística paramétrica asume que la característica sigue una\ndeterminada distribución de probabilidad. Esta distribución de probabilidad depende de unos parámetros (por ejemplo, la media\ny la desviación típica). La inferencia se hace en base esos parámetros, y se asumen ciertas hipótesis de partida que se deben comprobar.\nLa estadística paramétrica asume ninguna distribución de probabilidad para la\ncaracterística. Los métodos se basan en estadísticos de orden (cuantiles) y hace falta cumplir ninguna hipótesis.Por otra parte, se pueden seguir dos enfoques bien diferenciados la hora de hacer inferencia. Por una parte, el enfoque frecuentista asume que los parámetros son valores fijos desconocidos, de los que estimamos\nsu valor. Esta estimación está ligada una incertidumbre (error) derivada\ndel muestreo. Por otra parte, en el enfoque bayesiano los parámetros son valores fijos desconocidos, sino variables aleatorias de las que se estima su distribución de probabilidad. Y partir de esa distribución de probabilidad, se hace la inferencia. En este libro se tratarán los métodos bayesianos.","code":""},{"path":"intro.html","id":"organización-de-los-datos","chapter":"Capítulo 1 Introducción","heading":"1.2.4 Organización de los datos","text":"Hemos hablado de características de forma aislada. Pero normalmente estudiamos una sola característica de la población, sino que observamos varias características, teniendo así en la muestra\nun conjunto de variables relativas una serie de elementos. Cuando analizamos una única variable, aislada del resto, estaremos\nhaciendo análisis univariante. Cuando analizamos más de una variable, estaremos haciendo análisis multivariante. Casi siempre un estudio estadístico incluye análisis univariante y\nmultivariante.Para poder analizar los datos de forma eficiente, debemos organizarlos siguiendo los principios Tidy data. Así, dispondremos los datos en forma de tablas (datos rectangualares), donde tengamos una columna para cada variable (mismo tipo de datos) y una fila para cada observación (elemento, individuo).\nEl analista y software deben entender lo mismo, lo que podríamos decir que es preparar los datos para las máquinas y para los humanos. Esta sería la “capa de datos,” después puede haber una “capa de presentación,” independiente de la anterior. Aquí puede jugar un papel importante los metadatos: diccionarios de datos para consultar sobre las variables (unidades, descripciones, etc.)La tabla 1.1 muestra las primeras filas de una tabla de datos bien organizada. Cada fila representa un solo elemento, cada columna una sola variable, sin mezclar datos. Los nombres de las variables son cortos pero informativos.Tabla 1.1: Tabla rectangular bien organizada","code":""},{"path":"intro.html","id":"tipos-de-datos-y-escalas","chapter":"Capítulo 1 Introducción","heading":"1.2.5 Tipos de datos y escalas","text":"Las características que observamos pueden ser de distintos tipos. La correcta identificación del tipo de variable es crucial para hacer un correcto análisis, ya que los métodos pueden ser muy distintos.La primera diferenciación que haremos será entre variables cuantitativas y cualitativas. Las variables cuantitativas o numéricas se pueden expresar con un número que además tiene una escala métrica (se pueden medir diferencias entre individuos). su vez, pueden ser continuas o discretas. Las variables continuas pueden tomar cualquier valor en un intervalo (teóricamente infinitos valores). Las variables discretas pueden tomar un número de valores finito o infinito numerable, pero toma valores entre un valor y otro.Las variables cualitativas o categóricas son etiquetas sin sentido numérico en las que podemos clasificar los elementos. Si el número de posibles etiquetas son dos, estaremos ante variables dicotómicas, que en algunos casos podremos codificar como ceros y unos si presenta o presenta la característica principal. Las variables multinivel presentan más de dos posibles etiquetas. En ambos casos se trata de una escala nominal. Las variables ordinales son aquellas en las que las etiquetas se pueden ordenar, de forma que tenemos una escala ordinal.Además de las variables propiamente dichas, nuestro conjunto de datos puede tener otras características como marcas de tiempo e identificadores, que serán útiles para aplicar los métodos, pero serán objeto de análisis.En ocasiones es útil transformar las variables de un tipo otro. Por ejemplo:Fechas categóricas (etiqueta de mes, día de la semana, …)Cuantitativas cualitativas (clases, intervalos)Ordinales como numéricas: con precaución, sobre todo si hay pocos datos (<100). Se pueden combinar en índices.Variables calculadas con otras (por ejemplo, IMC)En los siguientes capítulos abordaremos el análisis de todos estos datos.","code":""},{"path":"intro.html","id":"la-estadística-y-el-método-científico","chapter":"Capítulo 1 Introducción","heading":"1.3 La Estadística y el método científico","text":"La estadística es un pilar fundamental del método científico. El método científico se aplica también en el desarrollo tecnológico. Por tanto, la correcta aplicación de los métodos estadísticos es imprescindible para el avance de la ciencia y la técnica.","code":""},{"path":"intro.html","id":"el-método-científico","chapter":"Capítulo 1 Introducción","heading":"1.3.1 El método científico","text":"El método científico se puede resumir en los siguientes pasos:Hacerse una preguntaHacerse una preguntaRealizar investigación de baseRealizar investigación de basePlantear una hipótesisPlantear una hipótesisComprobar la hipótesis con experimentosComprobar la hipótesis con experimentosAnalizar resultados y extraer conclusionesAnalizar resultados y extraer conclusionesComunicar resultadosComunicar resultadosLa pregunta que nos hacemos (1) depende del campo de aplicación, y aquí todavía aparece la Estadística (menos que sea una investigación sobre los propios métodos estadísticos). Durante la investigación de base (2), realizamos análisis exploratorio de datos e identificamos relaciones. Posiblemente, esta primera investigación nos hace cambiar la pregunta del primer paso. Plantear una hipótesis (3) significa formalizarla en términos de Hipóteis nula, \\(H_0\\), e hipótesis alternativa, \\(H_1\\), que se comprobarán con los datos empíricamente. El planteamiento de la hipótesis determina\nel método estadístico utilizar, y el diseño\ndel experimento (en sentido amplio). Para comprobar la hipótesis con experimentos (4) es fundamental un diseño adecuado para que los\nresultados sean válidos, así como la\ncorrecta organización de los datos recogidos según los\nprotocolos establecidos. Estos protocolos incluyen conceptos estadísticos como aleatorización y bloqueo, entre otros. Analizar resultados (5a) se puede hacer sino con técnicas estadísticas, y estos resultados deben contarle al experto la historia con suficiente evidencia para extraer conclusiones (5b). Intervienen aquí el análisis exploratorio, los contrastes de hipótesis y la validación de los modelos. Por último, podemos aprovechar las herramientas estadísticas modernas para comunicar resultados (6), por ejemplo mediante Informes reproducibles RMarkdown, Gráficos efectivos y resultados clave. Los resultados negativos (cuando conseguimos demostrar lo que buscábamos en la hipótesis) es un aspecto considerar también, para utilizar como lecciones aprendidas y conocimiento general.","code":""},{"path":"intro.html","id":"investigación-reproducible","chapter":"Capítulo 1 Introducción","heading":"1.3.2 Investigación reproducible","text":"Los informes reproducibles mencionados en el párrafo anterior hacen referencia al enfoque de Investigación reproducible en el cual se puedan reproducir los resultados, bien los mismos investigadores en otro momento, o terceras partes interesadas para verificar la validez de los resultados. Para esto es necesario utilizar software estadístico basado en scripts en los que se pueda consultar toda la lógica del análisis (frente software de “ventanas” donde se pierde la trazabilidad). Este código se puede mezclar con la propia narrativa del informe (antecedentes, interpretación, conclusiones, etc.) de forma que, dados los mismos datos, se obtenga el mismo informe. Incluso, dados otros datos, se podría replicar el estudio de forma instantánea. El enfoque “copy-paste” alternativo, en el que vamos añadiendo un informe los resultados en un momento dado, son fuente de inconsistencias, errores, desactualización y falta de reproducibilidad, y en los que cualquier cambio requiere mucho esfuerzo.","code":""},{"path":"intro.html","id":"estadística-calidad-y-sostenibilidad","chapter":"Capítulo 1 Introducción","heading":"1.4 Estadística, Calidad y Sostenibilidad","text":"La es una herramienta fundamental en muchos procedimientos\nrelacionados con la Calidad, y es por eso que se habla de\nControl Estadístico de la Calidad.","code":""},{"path":"intro.html","id":"calidad-y-variabilidad","chapter":"Capítulo 1 Introducción","heading":"1.4.1 Calidad y variabilidad","text":"Todos tenemos nuestra percepción de la calidad. Pero veamos primero la definición estandarizad de calidad que tenemos en la norma ISO 9001.Calidad: Grado en el que un conjunto de características inherentes de un objeto\ncumple con los requisitosISO 9001:2015 3.6.2Los requisitos son especificaciones de la característica, que pueden ser bilaterales o unilaterales.En la figura 1.3 vemos dos distribuciones de datos del tipo que vamos \nver en el libro22. Los dos conjuntos de datos correspondientes la medición\nde la variable peso tienen la misma media: 10 g.\nSin embargo, la de la izquierda tiene una desviación típica (medida de la variabilidad) igual \n0.6 g, menor que la de la derecha que es 1 g. Si las líneas rojas son nuestros\nlímites de especificación, podemos ver cómo en el proceso de la derecha algunos de los\nelementos de nuestro proceso satisfacen los requisitos. En este ejemplo se ve\nclaramente cómo reducir la variabilidad mejora la calidad ¡sin hacer nada\nmás! (ni nada menos).\nFigura 1.3: Procesos con la misma media y distinta variabilidad\nEn general,\nlas CTQs (Critical Quality características críticas para la calidad) tendrán un valor objetivo (target, \\(T\\)), o valor nominal, que es el ideal.\nAnte la imposibilidad de tener procesos exactos, se fijan unos límites de especificación\no límites de tolerancia dentro de los cuales el producto o servicio es conforme,\nmientras que es conforme cuando el valor de la CTQ está fuera de dichos límites.\nSe utilizan los símbolos \\(L\\) y \\(U\\) para designar los límites de control\ninferior y superior respectivamente.La Calidad se mide como la pérdida total que un producto causa la sociedadGenichi TaguchiDebemos considerar que la falta de calidad \nproduce pérdidas sólo cuando el producto cumple con las especificaciones, sino que,\nmedida que nos alejamos del valor objetivo, esa pérdida aumenta, y además lo\nhace de manera lineal, es decir, proporcional, sino que es mayor cuanto más nos\nalejamos del objetivo. Es lo que se conoce como la función de pérdida de Taguchi\n(Taguchi’s Loss Function).\nTaguchi consideraba la calidad como la consecución de un objetivo de\ncalidad, como una tolerancia, y la falta de calidad como una pérdida\npara la sociedad. El producto perfecto produce pérdidas (loss), mientras que\ncualquier desviación del objetivo produce una pérdida para la sociedad, que aumenta\nmedida que esa desviación es mayor.23 La figura 1.4\nrepresenta este coste para la sociedad (línea azul discontinua), que se produce\nsiempre que se consigue el objetivo, frente al coste contable (línea punteada gris),\nque solo se produce con las conformidades. El análisis de la función de\npérdida es una herramienta muy útil en proyectos de mejora, véase Cano, Moguerza, Redchuk.24\nFigura 1.4: Función de pérdida de Taguchi\n","code":""},{"path":"intro.html","id":"métodos-estadísticos-para-la-calidad","chapter":"Capítulo 1 Introducción","heading":"1.4.2 Métodos estadísticos para la calidad","text":"Existen métodos estadísticos específicos para el control y\nmejora de la calidad. Las dos principales herramientas del\nControl Estadístico de Procesos (SPC, Statistical Process Control)\nson los gráficos de control y el análisis de la capacidad del proceso.\nLa figura 1.5 muestra un ejemplo de ambas. El gráfico de control de la parte superior sirve para monitorizar las muestras (subgrupos de los que se calcula un estadístico) con el objetivo de detectar el cambio con respecto su situación de control estadístico. Así, los límites son “la voz del proceso.” La parte inferior representa “la voz de cliente,” comparando las especificaciones con la variabilidad del proceso, y calculando los índices de capacidad que son la medida real de calidad largo plazo (frente la mera contabilización de las unidades defectuosas y su cuantificación monetaria). Estas técnicas se combinan con otras tanto exploratorias como de inferencia para controlar y mejorar la calidad.\nFigura 1.5: Gráficos de control y capacidad del proceso\nOtra técnica de calidad en la que la Estadística juega un papel fundamental es la inspección por muestreo, también conocida como muestreos de aceptación. La aceptación de unidades o lotes de producto, se puede hacer con inspección completa, comprobando si los productos están dentro de los límites de especificación. Esto veces es muy caro o directamente imposible, por lo que se recurre al muestreo. El análisis se puede hacer por atributos (variables cualitativas y por variables (variables cuantitativas). La base de esto métodos reside en la probabilidad de aceptar/rechazar un lote defectuoso/correcto, desde el punto de vista del consumidor/productor. Existen una gran variedad de planes de muestreo específicos, como planes simples, planes dobles y múltiples o planes secuenciales. Muchos están descritos en las normas clásicas MIL-STD, que evolucionaron las series de normas ISO 2859 e ISO 3951.En los llamados ensayos inter-laboratorios también se aplican técnicas estadísticas como el análisis del sistema de medición (MSA, Measurement Systems Analysis), estudios de precisión y exactitud, estudios R&R (Reproducibility & Repeatibility), o validación de laboratorios. En la mayoría de los casos lo que se utiliza es Diseño y Análisis de Experimentos.","code":""},{"path":"intro.html","id":"metodologías-y-estándares","chapter":"Capítulo 1 Introducción","heading":"1.4.3 Metodologías y estándares","text":"Las normas sobre métodos estadísticos que elabora ISO emanan del comité ISO TC69, del que hay un subcomité “espejo” en UNE (entidad acreditada de normalización en España), el subcomité UNE CT66/SC3. La propia ISO 9000\nhace mención los métodos estadísticos, y existe un informe técnico, UNE-ISO TR 1017 sobre “Orientación sobre las técnicas estadísticas para la Norma ISO 9001:2020.” Algunas universidades disponen del catálogo de normas UNE en sus bases de datos para el acceso de docentes y estudiantes.La metodología Seis Sigma y el ciclo DMAIC aplican el método científico la mejora de la calidad, utilizando el lenguaje de las empresas. Lean Six Sigma es una evolución en la que se añade Seis Sigma los principios de Lean Manufacturing.","code":""},{"path":"intro.html","id":"objetivos-de-desarrollo-sostenible-ods","chapter":"Capítulo 1 Introducción","heading":"1.5 Objetivos de Desarrollo Sostenible (ODS)","text":"El 25 de septiembre de 2015, los líderes mundiales adoptaron un conjunto de objetivos globales para erradicar la pobreza, proteger el planeta y asegurar la prosperidad para todos como parte de una nueva agenda de desarrollo sostenible. Cada objetivo tiene metas específicas que deben alcanzarse en los próximos 15 años.Naciones Unidas","code":""},{"path":"intro.html","id":"los-17-ods","chapter":"Capítulo 1 Introducción","heading":"1.5.1 Los 17 ODS","text":"Esta iniciativa de la ONU (Sustainable Development Goals, SDG) plantea 17 objetivos generales, que se detallan en 169 metas concretas. Estos objetivos van más allá del medio ambiente, que probablemente es lo primero que nos viene la cabeza25. Los 17 objetivos son los siguientes, y se esquematizan en la figura 1.6.Fin de la pobreza - Poner fin la pobreza en todas sus formas en todo el mundoHambre cero- Poner fin al hambre, lograr la seguridad alimentaria y la mejora de la nutrición y promover la agricultura sostenibleSalud y bienestar- Garantizar una vida sana y promover el bienestar para todos en todas las edadesEducación de calidad- Garantizar una educación inclusiva, equitativa y de calidad y promover oportunidades de aprendizaje durante toda la vida para todosIgualdad de género- Lograr la igualdad entre los géneros y empoderar todas las mujeres y las niñas*Agua limpia y saneamiento**- Garantizar la disponibilidad de agua y su gestión sostenible y el saneamiento para todosEnergía asequible y contaminante- Garantizar el acceso una energía asequible, segura, sostenible y moderna para todosTrabajo decente y crecimiento económico- Promover el crecimiento económico sostenido, inclusivo y sostenible, el empleo pleno y productivo y el trabajo decente para todosIndustria, innovación e infraestructura- Construir infraestructuras resilientes, promover la industrialización inclusiva y sostenible y fomentar la innovaciónReducción de las desigualdades- Reducir la desigualdad en y entre los paísesCiudades y comunidades sostenibles- Lograr que las ciudades y los asentamientos humanos sean inclusivos, seguros, resilientes y sosteniblesProducción y consumo responsables- Garantizar modalidades de consumo y producción sosteniblesAcción por el clima- Adoptar medidas urgentes para combatir el cambio climático y sus efectosVida submarina- Conservar y utilizar en forma sostenible los océanos, los mares y los recursos marinos para el desarrollo sostenibleVida de ecosistemas terrestres- Proteger, restablecer y promover el uso sostenible de los ecosistemas terrestres, gestionar sosteniblemente los bosques, luchar contra la desertificación, detener e invertir la degradación de las tierras y detener la pérdida de biodiversidadPaz, justicia e instituciones sólidas- Promover sociedades, justas, pacíficas e inclusivas para el desarrollo sostenible, proporcionar todas las personas acceso la justicia y desarrollar instituciones eficaces, responsables e inclusivas en todos los nivelesAlianzas para lograr objetivos- Fortalecer los medios de ejecución y revitalizar la Alianza Mundial para el Desarrollo Sostenible\nFigura 1.6: Objetivos de Desarrollo Sostenible. Fuente: un.org\n","code":""},{"path":"intro.html","id":"estadística-y-sostenibilidad","chapter":"Capítulo 1 Introducción","heading":"1.5.2 Estadística y sostenibilidad","text":"La Estadística, y su aplicación en la Ciencia y la Ingeniería, puede hacerse presente en los ODS. Algunos ejemplos serían los siguientes:Al realizar investigación sobre algún aspecto de los ODS, irremediablemente utilizaremos la Estadística. Nos podemos proponer nuestras propias líneas de investigación y desarrollo tecnológico desde el punto de vista de uno o varios ODSAl realizar investigación sobre algún aspecto de los ODS, irremediablemente utilizaremos la Estadística. Nos podemos proponer nuestras propias líneas de investigación y desarrollo tecnológico desde el punto de vista de uno o varios ODSTener presentes los ODS para ser sostenible en los propios análisis. Por ejemplo reduciendo el uso de papel o energía, pero también utilizando lenguaje inclusivo o teniendo en cuenta minorías.Tener presentes los ODS para ser sostenible en los propios análisis. Por ejemplo reduciendo el uso de papel o energía, pero también utilizando lenguaje inclusivo o teniendo en cuenta minorías.Relacionar con ODS e intentar contribuir sea cual sea el objetivo de la\ninvestigaciónRelacionar con ODS e intentar contribuir sea cual sea el objetivo de la\ninvestigaciónSiempre podemos hacernos la pregunta: ¿Cómo puede contribuir este trabajo/estudio/investigación/…\nconseguir los Objetivos de Desarrollo Sostenible?Siempre podemos hacernos la pregunta: ¿Cómo puede contribuir este trabajo/estudio/investigación/…\nconseguir los Objetivos de Desarrollo Sostenible?","code":""},{"path":"aed-uni.html","id":"aed-uni","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"Capítulo 2 Análisis exploratorio univariante","text":"","code":""},{"path":"aed-uni.html","id":"la-importancia-del-análisis-exploratorio","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.1 La importancia del análisis exploratorio","text":"El análisis exploratorio de datos, y en particular su visualización,\nes el primer análisis que se debe hacer sobre cualquier conjunto de datos\nantes de abordar otras técnicas estadísticas, sean sencillas o complejas.\nLa “historia” que nos esté contando el gráfico de los datos, nos guiará\nhacia las técnicas de aprendizaje estadístico más adecuadas. Incluso,\nen muchas ocasiones será suficiente el análisis exploratorio para tomar\nuna decisión sobre el problema en estudio. La figura 1.1 representa\nla esencia de la Estadística y sus métodos. Los datos que analizamos,\nprovienen de una determinada población. Pero estos datos, son más que\nuna muestra, es decir, un subconjunto de toda la población. Incluso\ncuando “creemos” que tenemos todos los datos, debemos tener presente que trabajamos\ncon muestras, ya que generalmente tomaremos decisiones o llegaremos conclusiones\nsobre el futuro, y esos datos seguro que los tenemos. Por eso es importante\nconsiderar siempre este paradigma población-muestra. La Estadística Descriptiva\nse ocupa del análisis exploratorio de datos en sentido amplio, que aplicaremos\nsobre los datos concretos de la muestra en este unidad y la siguiente. La\nInferencia Estadística hace referencia los métodos mediante los cuales,\ntravés de los datos de la muestra, tomaremos decisiones, explicaremos relaciones,\no haremos predicciones sobre la población. Para ello, haremos uso de la\nProbabilidad, que veremos en la unidad 4, aplicando\nel método más adecuado.\nEn estos métodos será muy importante considerar el método de obtención de la muestra\nque, en términos generales, debe ser representativa de la población para que las\nconclusiones sean válidas.\nEn este tercer módulo del curso veremos algunos de estos métodos.\nFigura 1.1: La esencia de los métodos estadísticos\nEl análisis exploratorio se realiza básicamente mediante dos herramientas: los resúmenes numéricos\ny las visualizaciones gráficas.\nPero antes de aprender hacer análisis exploratorio con R, vamos resaltar la\nimportancia, dentro del análisis exploratorio, de las representaciones gráficas.\nPara ello utilizaremos un conjunto de datos llamado “el cuarteto de Anscombe,”26\ndisponible con el nombre anscombe en el paquete datasets de R base.\nLa tabla 2.1 muestra\neste conjunto de datos.Tabla 2.1: Conjunto de datos ‘anscombe’Son 11 filas de 8 variables numéricas, aunque las tres primeras son idénticas. Ya sabemos\nresumir los datos con la media de cada variable:Vemos que la media de las cuatro primeras variables es idéntica, 9. Pero los datos\nson muy distintos en la cuarta variable. Las cuatro últimas variables también tienen una media\nprácticamente idéntica. Sin embargo los datos también son muy distintos. La figura\n2.1 es un gráfico de los que aprenderemos hacer enseguida, y\nrepresenta en el eje vertical los valores de las variables, y en el eje horizontal\nlos nombres de cada variable. Vemos que, pesar de tener medias prácticamente iguales,\nlos datos son muy diferentes.\nFigura 2.1: Representación de las variables del cuarteto de Anscombe\nPero si en el análisis por separado ya se ve la necesidad de hacer un gráfico,\ncuando analizamos las variables conjuntamente, todavía es más evidente. La\nfigura 2.2 muestra los cuatro gráficos que constituyen\n“El cuarteto de Anscombe,” y que se puede obtener de la propia ayuda del\nconjunto de datos (example(anscombe)). La línea de regresión que se ajusta\nes prácticamente la misma (veremos la regresión en la unidad ??).\nAdemás, si calculáramos los coeficientes de correlación entre las variables “x” e\n“y” de los cuatro gráficos, obtendríamos el mismo valor: 0.8163.\nFigura 2.2: Los cuatro gráficos que constituyen ‘El cuarteto de Anscombe’\nEs evidente que la relación entre las variables es muy distinta en cada uno\nde los casos, y si visualizamos los datos para elegir el mejor modelo\nde regresión y después interpretarlo, podemos estar tomando decisiones\nerróneas.El cuarteto de Anscombe es muy ilustrativo, os animo explorar\ntambién Datasaurus Dozen:27 en https://www.autodeskresearch.com/publications/samestats.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nanscombe %>% summarise(across(.fns = mean))\n#>   x1 x2 x3 x4       y1       y2  y3       y4\n#> 1  9  9  9  9 7.500909 7.500909 7.5 7.500909"},{"path":"aed-uni.html","id":"calidad-de-datos","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.2 Calidad de datos","text":"Una vez hemos identificado los tipos de variables del problema de análisis\nde datos que queremos abordar, es necesario que tengamos los datos correctamente\nen el software que vamos utilizar, es decir, es muy importante\ncomprobar continuamente la calidad en los datos.\nLa importación de datos siempre puede dar problemas (y por Murphy, los dará).\nPor eso siempre deberíamos comprobar la estructura de los datos después de importar\nun conjunto de datos (al menos la primera vez). Uno de los errores\nmás comunes es que el tipo de datos importado se\ncorresponda con el que conceptualmente debe tener la variable. Esto produce\nningún error al importar, pero sí al analizar los datos. Otros problemas de calidad\ntienen que ver con valores atípicos (outliers) y con valores perdidos (missing).","code":""},{"path":"aed-uni.html","id":"datos-atípicos","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.2.1 Datos atípicos","text":"medida que llevamos el análisis de datos aplicaciones reales, es más fácil\nque aparezcan observaciones que estropean el análisis porque se salen de lo\nesperado en relación con el resto de datos.\nLa parte 4 de norma UNE-ISO 16269,28 un valor atípico es\nun “Miembro de un pequeño subconjunto de observaciones que parece ser inconsistente con el resto de una muestra dada.” La identificación de valores candidatos ser\nconsiderados como atípicos es una labor muy importante para el analista,\nya que pueden influir tanto en los resultados del análisis como en la\ntécnica utilizar. Estos valores identificados como posibles valores\natípicos deben ser investigados y determinar cuál es la causa de esta\nposible desviación. Se suele atribuir una de las siguientes causas:Error de medida o de registro. Esto puede ser debido la observación del dato\no al propio registro.Error de medida o de registro. Esto puede ser debido la observación del dato\no al propio registro.Contaminación. Los datos provienen de más de una distribución. Por ejemplo,\npor estar mezclando datos de grupos que tienen distintas medias. Entonces, los\nvalores de la distribución contaninante aparecerán como valores atípicos en la\ndistribución de interés.Contaminación. Los datos provienen de más de una distribución. Por ejemplo,\npor estar mezclando datos de grupos que tienen distintas medias. Entonces, los\nvalores de la distribución contaninante aparecerán como valores atípicos en la\ndistribución de interés.Suposición incorrecta sobre la distribución. La característica\nen estudio de la población se supone que sigue una determinada distribución\n(por ejemplo normal) pero en realidad sigue otra (por ejemnplo exponencial).\nEntonces los valores que parecen atípicos para la distribución normal,\nson perfectamente compatibles con la distribución verdadera.Suposición incorrecta sobre la distribución. La característica\nen estudio de la población se supone que sigue una determinada distribución\n(por ejemplo normal) pero en realidad sigue otra (por ejemnplo exponencial).\nEntonces los valores que parecen atípicos para la distribución normal,\nson perfectamente compatibles con la distribución verdadera.Observaciones excepcionales. Estos son verdaderos valores atípicos,\nsimplemente han ocurrido por azar, aunque sea muy improbable su ocurrencia.Observaciones excepcionales. Estos son verdaderos valores atípicos,\nsimplemente han ocurrido por azar, aunque sea muy improbable su ocurrencia.En el primer caso, hay que encontrar el valor correcto y si\nesto es posible, dar el valor por perdido (missing). En el segundo,\nhay que estratificar los datos y realizar el análisis por grupos, separando\nlas distribuciones. Si son solo unos pocos datos los que por error han contaminado la muestra, se pueden eliminar o dar por perdidos. En el tercer caso,\nse modifican las asunciones sobre el modelo de distribución subyacente en la\npoblación. En el último caso los valores deberían permanecer en la muestra,\naunque generalmente se etiquetan erróneamente como valores atípicos por su excepcionalidad.El análisis de los valores atípicos es importante por varios motivos. Por\nuna parte, puede dar lugar descubrimientos interesantes al investigar\npor qué han ocurrido (por ejemplo, se ha hecho algo diferente y un proceso\nha mejorado). Por otra parte, muchas medidas y métodos estadísticos son\nmuy sensibles observaciones atípicas, y entonces es posible que haya\nque usar alternativas robustas. Y en todo caso, nos ayuda determinar\nla adecuada distribución de probabilidad.La observación de los datos con métodos gráficos menudo proporciona\nsuficiente información para identificar valores candidatos ser\natípicos. En concreto, el gráfico de cajas diseñado por John W. Tukey29 y recogido en la norma UNE-ISO 1626930\nmarca estos valores de forma clara (véase el apartado ??\npara una completa explicación de su construcción e interpretación).Aparte de los métodos gráficos, existen diversos contrastes de hipótesis\npara determinar si existen valores atípicos en una muestra de datos\ndada una distribución de probabilidad. La norma UNE-ISO 1626931\nrecoge métodos para la distribución normal y también para otros modelos\nde distribución, así como un método general para distribuciones desconocidas\ny el test de Cochran para varianza atípica. El paquete outliers de R contiene\nvarias funciones para realizar contrastes de hipótesis sobre valores atípicos\nun conjunto de datos, incluidos el test de Grubbs y el test de Cochran.En cuanto al tratamiento de datos que contienen valores candidatos ser\natípicos pero de los que se ha podido identificar una causa válida para\neliminarlos, deberíamos recurrir al análisis de datos robusto, de forma\nque las observaciones atípicas influyan demasiado en los resultados, pero\nsin eliminarlas. Otra alternativa es realizar el análisis con y sin valores\ncandidatos ser atípicos y comprobar cómo varía ese resultado.Entre las medidas de centralización robustas se encuentran la mediana y la media recortada\n(véase ??), aunque hay otras. También para la\nestimación de la dispersión se encuentran estimadores robustos como la\nMediana de las medianas de las desviaciones absolutas de los pares.32Lo dicho hasta ahora sirve para detectar atípicos para una característica.\nEn conjuntos multivariantes, se pueden observar valores atípicos con respecto\nmás de una variable. En particular, en modelos de regresión puede haber\nobservaciones influyentes (que posiblemente son atípicas en la variable\naislada) que influyen en la estimación de los parámetros de forma que el\nresultado es representativo del conjunto de datos. Los gráficos de\ndiagnóstico de R para los modelos lineales proporcionan un gráfico señalando\nlas observaciones influyentes según la distancia de Cook. También el paquete\ncar contiene una función (outlierTest) con la que podemos obtener\nla observación más extrema para la regresión.Por último, podemos detectar observaciones atípicas con respecto todo un\nconjunto multivariante de datos en escala métrica. Para ello, lo que se hace\nes reducir este conjunto multivariate en univariante, obteniendo unas distancias\nde las observaciones la media muestral del conjunto de datos, estandarizada\nmediante la matriz de varianzas-covarianzas de la muestra. Entonces aquellas\nobservaciones muy alejadas de esos valores centrales pueden estudiarse como\ncandidatos ser valores atípicos multivariantes. En ISO33 se\nproporciona un contraste de hipótesis y un método gráfico para identificar\nestos valores atípicos. En el apartado ?? se proporcionan\nlas funciones necesarias para calcular la distancia de Mahalanobis.","code":""},{"path":"aed-uni.html","id":"valores-perdidos-missing-values","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.2.2 Valores perdidos (missing values)","text":"La ausencia de valores para determinadas observaciones de nuestra muestra\nes otro de los problemas habituales que surgen con los datos. Al igual\nque con los valores atípicos, un valor perdido puede ser fruto de un\nerror en la recogida o registro de los datos. Si ese error es recuperable,\nbastará con añadir el verdadero valor nuestro conjunto de datos. Si el\nvalor se da definitivamente por perdido, entonces podemos seguir dos caminos:Realizar el análisis sin considerar las observaciones con valores perdidos.Realizar el análisis sin considerar las observaciones con valores perdidos.Imputar un valor las observaciones perdidas.Imputar un valor las observaciones perdidas.El primer caso merece la siguiente consideración. Cuando estamos analizando\nuna sola característica, este camino es único. Por ejemplo, en un conjunto\nde 100 observaciones donde faltan 2, se calcula la media con las 98 restantes.\nO en un gráfico, se representan solo los valores existentes. Pero cuando\nestamos analizando un conjunto multivariante, podemos tener valores perdidos\nen todas las variables, o solo en algunas. Entonces podemos tomar diferentes\ndecisiones este respecto. Por ejemplo, si queremos calcular una matriz de\ncorrelaciones, podemos considerar solo las observaciones en las que hay valores\npara todas las variables, o eliminar solo los pares de observaciones relevantes\npara cada coeficiente de correlación entre dos variables34.El segundo camino es más complicado y requiere su vez elegir el método de\nimputación del valor perdido. La imputación más sencilla es simplemente asignar\nla media o la mediana como valor representativo de toda la variable. Pero cuando\ntenemos conjuntos multivariantes, puede ser más adecuado hacer una imputación\nen función de la información disponible en otras variables. Por ejemplo, si tenemos\nuna variable de tipo atributo, la media del grupo al que pertenece la\nobservación será generalmente más adecuada que la media global.En R tenemos varias alternativas para la imputación de valores perdidos.\nLa función impute del paquete Hmisc realiza imputaciones sencillas\n(por defecto la mediana). El paquete mice realiza imputaciones\nutilizando datos multivariantes con un buen número de opciones.La investigación de los valores perdidos y su tratamiento adecuado debe ser\nsiempre una fase importante del proyecto de análisis de datos. Además,\neste análisis se puede solapar con el análisis de los valores atípicos,\npor ejemplo cuando un valor atípico se determina que es un dato erróneo pero\npodemos asegurar cuál es el valor verdadero, entonces tenemos que considerarlo\ncomo perdido y aplicar lo aquí visto.","code":""},{"path":"aed-uni.html","id":"errores-comunes","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.2.3 Errores comunes","text":"Aparte de los errores en los datos que ya se han tratado, hay que evitar\nalgunos errores demasiado comunes la hora de abordar el análisis de datos,\ny especialmente la interpretación de resultados. En este apartado\nse mencionan algunos de los más importantes.1. Confundir correlación con causalidad.Cuando realizamos una regresión de una variable respuesta \\(Y\\) sobre una o\nvarias variables explicativas \\(X\\), tendemos pensar que \\(X\\) es la causa\nde la variación de \\(Y\\). Esto siempre es así, y deberíamos tenerlo presente\nincluso en aplicaciones en las que conocemos los procesos y “estamos seguro”\nde que es así. Para confirmar que una relación es de causa-efecto, deberíamos\nrecurrir al Diseño de Experimentos, donde además podremos estudiar las\ninteracciones.2. Falta de parsimonia.La parsimoniosidad es un principio científico (véase Wikipedia35)36 que,\naplicado la Estadística, significa seleccionar el modelo más reducido y\nsimple posible que consiga explicar el fenómeno estudiar, frente modelos\nmás complejos (con muchas variables) con una mínima o nula ganancia de poder\npredictivo. En modelos de regresión múltiple, por ejemplo, ninguna variable\ncuyo coeficiente sea significativo se debería incluir en el modelo\nfinal al que llega la investigación.2. Interpretación de porcentajes sin fijarse en el tamaño.Este error común viene explicado por la paradoja de Simpson.37\nEsta paradoja aparece cuando hay un atributo oculto que se tiene en cuenta\nla hora de interpretar porcentajes, pudiendo darse el caso de que otro atributo\npresenta un porcentaje mayor en una categoría, pero si se analizan por separado\nlos porcentajes para las categorías del atributo oculto, resulta que el porcentaje\nde la categoría que era mayor globalmente, es menor en TODOS los grupos del\natributo oculto.3. Informar los valores medios pero la dispersión.La media por sí sola debería llevar conclusión alguna. Siempre se debe\nanalizar conjuntamente la centralidad y dispersión de los datos, ya que\nun valor medio puede estar calculado con valores muy extremos y ocultar\nmucha información.4. Pasar por alto las hipótesis del modelo.Muchos modelos estadísticos requieren, para ser válidos, que se cumplan\nciertas condiciones. Si utilizamos un método que requiere normalidad,\ndebemos comprobar que los datos provienen de una distribución normal.\nAnte la duda, debemos comprobar que un método paramétrico conduce\nresultados similares.5. Sobreajuste (overfitting).El sobreajuste aparece cuando en un modelo predictivo conseguimos estimar\nperfectamente los valores de la muestra, pero el modelo utilizado\nsirve para generalizar nuevos casos. En Machine Learning es muy fácil\nconseguir un modelo perfecto para los datos utilizados, pero pésimo para\nnuevos casos. El paradigma de entrenamiento y validación consigue evitar\nel sobreajuste.6. Utilizar muestras sesgadas como si fueran aleatoriasLos métodos probabilísticos de uno u otro modo se basan en que los datos\nprovienen de muestras aleatorias. pesar de que en muchas situaciones de\nanálisis de datos esto lo podamos ni siquiera soñar, es importante tenerlo\nen mente para, la hora de interpretar resultados y llegar conclusiones,\nhacer una reflexión sobre cuánto nos estamos alejando de esa aleatoriedad.\nPor ejemplo, si estoy haciendo un estudio de los clientes de una empresa\ny solo analizo las transacciones de la primera semana del mes, tengo una\nmuestra sesgada porque tengo representado el resto del mes (posiblemente\ncon un comportamiento diferente).","code":""},{"path":"aed-uni.html","id":"componentes-de-un-gráfico","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.3 Componentes de un gráfico","text":"Dejando aparte las visualizaciones en tres dimensiones, animaciones 3D y realidad virtual,\nla visualización de datos que hacemos en la práctica totalidad de los casos es\nen dos dimensiones, es decir, en el plano. Vamos pensar en este plano como\nsi fuera un “lienzo” de pintor, independientemente de que el resultado lo vayamos\nver impreso en un papel o en una pantalla. Este lienzo se irá “poblando” de\n“capas” medida que el pintor vaya añadiendo cosas. Siguiendo con el símil,\nempezaremos preparando un espacio para los símbolos con los que representaremos\nlos datos, es decir, unos ejes: horizontal (X) y vertical (Y). partir de\naquí, representaremos los datos con algún símbolo geométrico, como un punto, una línea,\no cualquier otro. Podremos añadir colores los símbolos y otras características\ncomo transparencia o tamaño. También añadiremos anotaciones al gráfico, como\nlas marcas en los ejes, títulos o incluso texto dentro del gráfico.La figura 2.3 es una ilustración de Allison Horst38 que simboliza este paradigmna de lienzo y capas. Si pensamos en los distintos elementos del gráfico y los relacionamos con las variables que estamos analizando, será mucho más fácil hacer el gráfico adecuado e interpretarlo.\nFigura 2.3: El dispositivo gráfico como lienzo al que añadimos capas\n","code":""},{"path":"aed-uni.html","id":"notación","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.4 Notación","text":"Antes de comenzar hacer resúmenes de los datos, vamos definir la notación que\nutilizaremos. Representamos las variables con letras mayúsculas latinas del final el alfabeto como \\(X, Y, \\ldots\\)39. Cada uno de los posibles valores que toma la variable \\(X\\) se representa por \\(x_i\\). Así, \\(\\) es el identificador o índice para cada observación o clase. Si agrupamos los datos numéricos en intervalos (clases),\n\\(c_i\\) es la marca de clase, es decir, el punto central del intervalo. El número total de observaciones en la muestra lo representamos por \\(n\\), mientras que si tenemos una enumeración de toda la población en estudio, denotaremos el número total de individuos por \\(N\\). El número de clases o niveles de una variable categórica o numérica agrupado es \\(k\\).\n\\(n_i\\) es el número de observaciones en la clase \\(\\).Para representar los parámetros (recordemos, desconocidos) utilizamos letras griegas. Por ejemplo, \\(\\mu\\) es la media poblacional, y \\(\\sigma^2\\) la varianza poblacional. Para representar estadísticos (recordemos, calculados con los datos de la muestra) se representan con letras minúsculas. Por ejemplo, \\(\\bar{x}\\):es la media muestral de la variable \\(X\\), y \\(s^2\\): representa la varianza muestral (cuasivarianza).\n\\(s\\) es la desviación típica muestralPara representar que un estadístico es un estimador, utilizamos la notación\n\\(\\hat{[\\cdot]}\\), que simboliza un estimador de \\(\\cdot\\). Por ejemplo,\n\\(s = \\hat{\\sigma}\\) quiere decir que la desviación típica muestral\n\\(s\\) es un estimador de la desviación típica poblacional \\(\\sigma\\). Supongamos que tenemos que hacer un estudio de las emisiones de dióxido de carbono (CO\\(_2\\)) en las granjas de porcino de una determinada región.","code":""},{"path":"aed-uni.html","id":"análisis-exploratorio-de-variables-cualitativas","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.5 Análisis exploratorio de variables cualitativas","text":"Cuando nuestra variable se expresa con números, sino con\netiquetas de una determinada característica observada en cada uno\nde los elementos en los que se observa la característica, el resumen numérico\nque utilizamos es la tabla de frecuencias. Esta tabla de frecuencias\nse puede representar gráficamente con un gráfico de barras o con un gráfico\nde sectores. Este último es recomendable ya que proporciona la misma\ninformación que el gráfico de barras y es mucho más difícil para el ojo\nhumano distinguir ángulos que alturas.\nEn variables cualitativas, llamamos la categoría más frecuente moda de la variable.Para construir la tabla de frecuencias, contamos el número de elementos de cada clase \\((n_i)\\) que pertenecen cada uno de los valores \\((x_i)\\) o de las clases \\((c_i)\\), que son\nlas frecuencias absolutas. Se pueden calcular también frecuencias relativas \\((f_i=n_i/n)\\) y acumuladas,\ntanto para las absolutas \\((N_i)\\) como para las relativas \\((F_i)\\).Los datos que se utilizarán en este capítulo para ilustrar los ejemplos se pueden descargar e importar con el siguiente código. El laboratorio de una fábrica de quesos recoge datos de los análisis realizados muestras de quesos de su producción. Se dispone de un conjunto de datos con 1171 filas y\n12 columnas. La tabla 2.2 muestra las primeras filas de este conjunto de datos.La columna tipo toma tres valores: , B y C. La tabla 2.3 muestra una tabla de frecuencias completa, donde se puede ver de un vistazo, por ejemplo, que la clase con más quesos en el conjunto de datos el tipo C. Las frecuencias relativas se pueden traducir fácilmente porcentajes.Tabla 2.2: Ejemplo datos de laboratorioTabla 2.3: Tabla de frecuencias del tipo de quesoRLa función table de R crea tablas de frecuencias absolutas. Si el resultado se lo pasamos la función prop.table, las convierte en tabla de frecuencias relativas. La función admargins añade totales. Para obtener frecuencias acumuladas, podemos usar la función cumsum.Las expresiones siguiente son ejemplos de uso de estas funciones. La tabla 2.3 se ha obtenido utilizando funciones del paquete dplyr:La representación gráfica adecuada para variables cualitativas es el gráfico de barras. En este gráfico, representamos las categorías en el eje horizontal (X) y las frecuencias en el eje vertical (Y), y representamos barras cuya altura representa la frecuencia. Se pueden representar frecuencias absolutas o relativas. Los gráficos de sectores también pueden representar variables cualitativas, aunque se recomiendan porque el ojo humano es tan bueno distinguiendo ángulos como alturas. En todo caso, si se usa se deberían incluir los valores (frecuencias o porcentajes). El gráfico de barras se puede representar también invirtiendo los ejes (veces mejora la visualización de las etiquetas), representando líneas en vez de barras, u ordenando las barras según la frecuencia (por defecto este orden es arbitrario, muy menudo alfabético según las etiquetas).Un aspecto importante de los gráficos de barras es que debe haber un espacio entre las barras, puesto que son variables cualitativas en las que tiene sentido representar la continuidad que expresarían las barras adyacentes.RLa tabla de frecuencias 2.3 se puede representar con el siguiente código cuyo resultado se muestra en la figura ??.El segundo fragmento de código produce la fugura 2.4 muestra un ejemplo de gráfico de sectores que muestra las etiquetas e incluso algunos estadísticos y contrastes de hipótesis con el paquete {ggstatsplot}.\n(#fig:barras1, )Ejemplo gráfico de barras variable cualitativa\n\nFigura 2.4: Gráfico de sectores con etiquetas\n","code":"\nlibrary(dplyr)\ndownload.file(\"https://lcano.com/data/eaci/lab.xlsx\", \"lab.xlsx\")\nlab <- readxl::read_excel(\"lab.xlsx\") |> \n  mutate(fecha = as.Date(fecha))lab |> count(tipo) |> \n  mutate(f = n/nrow(lab), N = cumsum(n),\n         F = cumsum(f))\ntable(lab$tipo)\n#> \n#>   A   B   C \n#> 175 148 848\nprop.table(table(lab$tipo))\n#> \n#>         A         B         C \n#> 0.1494449 0.1263877 0.7241674\naddmargins(table(lab$tipo))\n#> \n#>    A    B    C  Sum \n#>  175  148  848 1171\ncumsum(table(lab$tipo))\n#>    A    B    C \n#>  175  323 1171\nlibrary(ggplot2)\nlab |> \n  ggplot(aes(x = tipo)) +\n  geom_bar(fill = \"#CB0017\") +\n  theme_bw() + \n  labs(title = \"Tipos de queso\", \n       x = \"Tipo\",\n       y = \"Frecuencia absoluta\")\nlibrary(ggstatsplot)\n#> You can cite this package as:\n#>      Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n#>      Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nlab %>% ggpiestats(x = tipo, title = \"Fabricación de quesos\",\n                  legend.title = \"Tipo de queso\", \n                  bf.message = FALSE,\n                  results.subtitle = FALSE)"},{"path":"aed-uni.html","id":"análisis-exploratorio-de-variables-cuantitativas","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.6 Análisis exploratorio de variables cuantitativas","text":"","code":""},{"path":"aed-uni.html","id":"resúmenes-de-variables-discretas","chapter":"Capítulo 2 Análisis exploratorio univariante","heading":"2.7 Resúmenes de variables discretas","text":"En el caso de variables discretas,\nse puede realizar el mismo análisis exploratorio que para las variables\ncategóricas, es decir, una tabla de frecuencias y su correspondiente gráfico\nde barras. La diferencia principal es que en este caso la tabla y el gráfico deben estar ordenados\nde mayor menor según los valores numéricos que toma la variable. De nuevo\naquí es importante decir que debe haber una separación entre las barras, porque\npor su naturaleza, hay valores entre un valor y otro de la variable, y así queda\nbien representado que es una variable discreta.Cuando el número de posibles valores es muy grande, aunque la variable sea discreta\nse puede tratar como si fuera continua, resumiendo en tablas de frecuencias por intervalos\ne histogramas, para facilitar su interpretación. Pero se debe perder nunca de vista la naturaleza de la variable.En variables discretas, también podemos resumir los datos con el valor más frecuente,\nes decir, la moda. También se podrán resumir los datos mediante los estadísticos\ny con el gráfico de cajas que se explicarán en el apartado siguiente de variables continuas., podemos calcular las siguientes medidas\nde centralización que resumen los datos:Media: Función mean.Mediana: Función median.La mediana es una medida de posición central que indica el valor que divide los datos\nen dos mitades: los que son menores que la mediana y los que son mayores que la mediana.\nLa mediana es el cuantil 0.5, y se puede calcular cualquier cuantil con\nla función quantile. Los cuantiles 0.25 y 0.75 se conocen como cuartiles\n(primer cuartil Q1 y tercer cuartil Q3 respectivamente).\nEl mínimo y el máximo son también medidas de posición (funciones min)\ny max respectivamente.Las medidas de centralización y posición son suficientes para resumir una\nvariable, se debe acompañar de medidas de dispersión. Las más importantes:Varianza: Función varDesviación típica: Función sdRango intercuartílico: Función IQRDesviación absoluta mediana: Función madCoeficiente de Variación (CV): Se calcula fácilmente como sd(x)/mean(x)La función summary de R base es una función de las llamadas “genéricas,”\nque proporciona distintos resultados según la clase del objeto que se le\nproporcione como argumento. Para un vector numérico, devuelve\nel mínimo, Q1, mediana, media, Q3 y máximo.Un análisis exploratorio de una variable suele incluir medidas de asimetría\n(skewness) y aplanamiento (kurtosis), así como información del número total\nde datos y porcentaje de datos válidos. Se pueden calcular todas estas medidas\ncon las funciones indicadas, y otras existentes en diversos paquetes. El\npaquete summarytools proporciona un resumen completo de un vector numérico\ncon la función descr.Los resúmenes numéricos anteriores sirven tanto para variables discretas como\npara continuas. Sin embargo, en el caso de las\nvariables continuas, se cuenta el número de observaciones (frecuencia) por intervalos (bins). Estos\nintervalos se pueden calcular utilizando diferentes métodos. Habitualmente,\nprimero se elige el número de intervalos (entre 5 y 20) y después se calcula la\namplitud de los intervalos dividiendo el rango de valores entre ese número.\nUna posible regla\nsería el método de Sturges, y se puede obtener el número de intervalos con la función nclass.Sturges. Este es el método que utiliza por defecto la función hist\nde R base, que además redondea la amplitud del intervalo para facilitar la\ninterpretación. Otra regla muy sencilla es tomar como número de intervalos\nentorno la raíz cuadrada del número total de datos.\nPara obtener una tabla de frecuencias de una variable continua basta con\n“discretizarla” con la función cut y obtener una tabla de frecuencias\nigual que con las variables categóricas.La representación de la tabla de frecuencias de una variable continua\nes el histograma. Nótese que hay una diferencia importante con el\ngráfico de barras en variables discretas. En el histograma, las barras\nson contiguas, y si hubiera un espacio sin barra significaría que en\nese intervalo hay ninguna observación. En el gráfico de barras, las\nbarras están separadas por un espacio, y hay una barra para cada valor\nque toma la variable, intervalos. Una variable discreta también se\npuede visualizar mediante un histograma, que se preferirá cuando el\nnúmero de posibles valores sea grande.El histograma nos da mucha información sobre la variable:Si es aproximadamente simétricaSi tiene forma de campana (se parece la distribución normal, que veremos en\nla unidad ??)Si hay valores extremos (alejados del centro) y cómo son de frecuentesSi puede haber mezcla de poblaciones (más de una moda)La función geom_histogram del paquete ggplot2 añade una capa con un\nhistograma al gráfico. Solamente necesita el mapeo de una variable,\npero los datos siempre se tienen que proporcionar como tabla (data.frame o tibble).\nEl color de las barras se controla con el aesthetics fill,\ny la altura puede representar las frecuencias absolutas (recuentos) o\nrelativas (proporciones). Al contrario que la función hist de R base,\nel autor de ggplot2 ha preferido que el número de barras por defecto sea\nun valor inadecuado en la mayoría de las ocasiones (30) argumentando\nque el analista debería explorar los datos y experimentar varias opciones\npara descubrir la historia completa detrás de los datos. Una vez\ndecidido el número de intervalos, se lo podemos explicitar con el argumento\nbins, o alternativamente la anchura de intervalo con bin_width.Una representación alternativa al histograma es la línea de densidad, que sustituye\nlas barras por una línea continua, generalmente suavizada, que nos da una idea\nde la forma de la distribución de forma más esquemática. La función de ggplot2\nque debemos utilizar en este caso es geom_density.Otra representación gráfica muy útil de las variables continuas es el\ngráfico de cajas. La función geom_boxplot se encarga de producir este objeto\ngeométrico, que representa la posición de los datos: máximo y mínimo (extremos de las líneas o “bigotes”), primer y\ntercer cuartil (bordes de la caja) y mediana (línea que cruza la caja).\nSi existen valores extremos, candidatos ser considerados valores atípicos,\nse representan separados de las líneas, que llegan al último valor extremo.El gráfico de\ncajas por sí solo puede estar ocultando información, por lo que se pueden utilizar\nvariantes como los gráficos de violín, o representar también los puntos con\ncierto desplazamiento aleatorio en el eje para una mejor visualización.Otra visualización básica para una variable numérica es la visualización\nsecuencial de las observaciones, bien través de puntos (geom_point) o través de\nlíneas (geom_line). El orden de las observaciones nos pueden indicar cuándo se ha producido\nun cambio u otros patrones.","code":""},{"path":"aed-bi.html","id":"aed-bi","chapter":"Capítulo 3 Análisis exploratorio bivariante","heading":"Capítulo 3 Análisis exploratorio bivariante","text":"Representación gráficaCorrelaciónRegresiónIntro multivarianteEn preparación.","code":""},{"path":"introp.html","id":"introp","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"Capítulo 4 Introducción a la Probabilidad","text":"","code":""},{"path":"introp.html","id":"sec-introprob","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.1 Introducción","text":"En los capítulos anteriores, hemos visto cómo mediante la Estadística Descriptiva\nestudiamos variables estadísticas describiéndolas y representándolas. Mediante la\nEstadística Inferencial lo que tratamos es de inferir (estimar, predecir)\nlas propiedades de una población basándonos en una muestra de\ndatos40. La Teoría de\nProbabilidades y el Cálculo de Probabilidades son las bases en las que\nse sustentan estos métodos, partiendo de la estimación del modelo de\ndatos, es decir, la distribución de probabilidad de una determinada\ncaracterística en la población.\nEn este capítulo estudiaremos los conceptos fundamentales del\nCálculo de Probabilidades.","code":""},{"path":"introp.html","id":"estándares-de-aplicación","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"Estándares de aplicación","text":"En este capítulo se han aplicado los siguientes estándares:UNE-ISO 3534-1: Estadística. Vocabulario y símbolos. Parte 1, Términos estadísticos generales y términos empleados en el cálculo de probabilidades","code":""},{"path":"introp.html","id":"estadística-y-cálculo-de-probabilidades","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"Estadística y Cálculo de Probabilidades","text":"La figura 4.1 representa una especie de dogma de la Estadística, esto es,\nsu relación con la probabilidad y la inferencia, través de la población y la muestra.\nFigura 4.1: Relación entre la Estadística Descriptiva, el Cálculo de Probabilidades y la Estadística Inferencial\nEs decir, partiendo de los datos de la muestra, estimaremos el modelo de\ndistribución de probabilidad que sigue la variable en estudio en toda la\npoblación. partir de ahí, podremos estimar sus parámetros,\ncalcular probabilidades y realizar contrastes de hipótesis usando técnicas\nde inferencia estadística. La Estadística Descriptiva sobre los datos de la\nmuestra es una tarea permanente.\nNecesitamos en primer lugar una\ndefinición de la Probabilidad y sus propiedades.","code":""},{"path":"introp.html","id":"sucesos-aleatorios","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.2 Sucesos aleatorios","text":"Definamos un experimento como cualquier actividad\nque deriva en un resultado observable e identificable, al que llamaremos suceso. Estos\nresultados pueden ser deterministas o aleatorios.\nSucesos deterministas son los resultados de aquellos experimentos que,\nbajo las mismas condiciones,\nproducen el mismo resultado. Por ejemplo, si observamos el número de eclipses de sol\nque se producen en los próximos 12 meses, el resultado es determinista.\nPor contra, Sucesos aleatorios son aquellos que están sujetos incertidumbre. La\nmayoría de los experimentos son deterministas sino aleatorios. Por ejemplo,\nel resultado al lanzar un dado, observar si un cliente compra o al entrar \nuna tienda, etc.Llamamos sucesos elementales\ncada uno de los resultados posibles de un experimento. Al ser aleatorios,\nconocemos cuál de ellos va ser el resultado final del experimento, pero sí\npodemos conocer la probabilidad de que se produzca cada uno de los\nresultados41.\nPor ejemplo: en una clase de 50 alumnos, si observamos\nel número de alumnos que obtiene sobresaliente en un curso, sabemos cuántos\nvan ser. Pero sí podemos saber cuál es la probabilidad de cada uno de los\nresultados posibles, en este caso entre 0 (ninguno) y 50 (todos) en base \nlo que ha sucedido en años anteriores.Así, la Probabilidad es una medida del grado de incertidumbre\nsobre el resultado de un experimento aleatorio. Los posibles resultados de un experimento\naleatorio forman un conjunto, y la teoría de probabilidades se sustenta en la\nteoría de conjuntos.\ncontinuación vamos definir\nformalmente los sucesos en términos de conjuntos.Espacio muestral, \\(\\Omega\\)Conjunto de todos los resultados posibles— ISO 3534-1 2.1\\(\\Omega\\) estará formado por los posibles resultados del experimento o\nsucesos elementales \\(\\omega_i\\).Suceso, \\(\\)Subconjunto del espacio muestral— ISO 3534-1 2.2Suceso complementario, \\(^c\\)Espacio muestral excluyendo el suceso dado— ISO 3534-1 2.3Así, un suceso cualquiera estará formado por uno o varios sucesos elementales \\(\\omega_i\\)\ndel espacio muestral. Un suceso \\(\\) ocurre si ocurre alguno de los sucesos\nelementales que lo componen.","code":""},{"path":"introp.html","id":"sucesos-notables","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.2.1 Sucesos notables","text":"Los siguientes sucesos tienen especial importancia en el cálculo de probabilidades:Suceso \\(\\subseteq \\Omega\\).Suceso complementario42 \\(^c\\).Suceso seguro \\(\\Omega\\).Suceso imposible \\(\\emptyset\\).La figura 4.2 representa el espacio muestral, un suceso cualquiera \\(\\) y su\ncomplementario \\(^c\\). El suceso imposible aparece representado, pero en\nrealidad sería:\\[\\emptyset = \\Omega^c\\]\nFigura 4.2: Representación del espacio muestral, un suceso cualquiera y su complementario\nHabitualmente se utilizan ejemplos de juegos de azar para introducir el\ncálculo de probabilidades, como lanzamiento de monedas y dados, o\ncombinaciones de cartas en barajas de naipes. Los ejemplos con juegos de azar\ntienen la ventaja de que son fáciles de comprender.La aplicación de la probabilidad en casos distintos\nlos juegos de azar, sigue las mismas leyes, y los ejemplos se pueden asimilar\nsituaciones reales de la empresa o cualquier otro ámbito. continuación\nse describe un ejemplo ilustrativo que,\naunque totalmente inventado, se puede encontrar el lector\nen el futuro con ligeras variaciones según su ámbito de actuación.\nUtilizaremos en lo posible las cifras usadas en los problemas de azar\npara ver la utilidad de aquéllos ejemplos en casos más prácticos.En un estudio se cuenta con un conjunto de 52 sujetos,\nlos cuales están clasificados\nsegún alguna característica.\nVamos considerar el experimento de observar un sujeto\n(por ejemplo cuando entra en la página web del estudio) y clasificarlo\nsegún un criterio determinado. Tendremos los siguientes sucesos:52 posibles sujetos en estudio, \\((\\Omega)\\)La mitad son mujeres \\((M)\\)4 investigadores \\(()\\) , 12 técnicos \\((T)\\), resto pacientes \\((P)\\)13 jóvenes \\((J)\\), 26 adultos \\(()\\), 13 mayores \\((R)\\); 5, 18 y 3 mujeres en cada\ngrupo respectivamente1 de cada seis hombres \\((H)\\) responderá al tratamiento \\((S)\\), el doble si es mujer¿Con qué juegos de azar relacionarías cada uno de los sucesos anteriores?\nPiensa algunos ejemplos de sucesos en el entorno empresarial con datos similares.\nEl siguiente puede ser un ejemplo más real.CALCULADORA5 \\(\\boxed{\\mathsf{nCr}}\\) 2 \\(\\rightarrow\\) 10HOJA DE CÁLCULO=COMBIN(5;2) \\(\\boxed{\\mathsf{10}}\\)\n[EXCEL] =COMBINAT(5;2) \\(\\boxed{\\mathsf{10}}\\)R","code":"\nchoose(5, 2)\n#> [1] 10"},{"path":"introp.html","id":"operaciones-con-sucesos","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.2.2 Operaciones con sucesos","text":"Como se ha comentado anteriormente, los sucesos son conjuntos. Y como\ntales, aplican las operaciones y propiedades de la teoría de conjuntos.Unión de sucesos. Dados dos sucesos \\(\\) y \\(B\\), definimos\n\\(\\cup B\\)43\ncomo\nel suceso que se cumple si:Ocurre \\(\\), oOcurre \\(B\\), oOcurren \\(\\) y \\(B\\) la vezEl suceso unión contiene los sucesos elementales comunes y los comunes,\nvéase la figura 4.3.\nFigura 4.3: Representación de la unión de dos sucesos\nIntersección de sucesos. Dados dos sucesos \\(\\) y \\(B\\), definimos\n\\(\\cap B\\)44\ncomo el suceso que se cumple si ocurren \\(\\) y \\(B\\) simultáneamente. El suceso\nintersección contiene únicamente los sucesos elementales comunes ambos sucesos,\nvéase la figura 4.4Las operaciones de unión e intersección entre dos ducesos se extienden\ninmediatamente más de dos sucesos.\nFigura 4.4: Representación de la intersección de dos sucesos\nSucesos disjuntos. Dos sucesos \\(\\) y \\(B\\) son disjuntos o mutuamente excluyentes si:\\[\\cap B = \\emptyset.\\]Un suceso \\(\\) está contenido en otro suceso \\(B\\), \\(\\subset B\\) si siempre que se\nproduce \\(\\), se produce también \\(B\\).Diferencia de sucesos. El suceso diferencia \\(-B\\) es el suceso que se produce cuando\nocurre \\(\\) y ocurre \\(B\\). Se verifica:\\[-B = \\cap B^c.\\]La figura 4.5 muestra una representación de sucesos disjuntos, sucesos incluidos en otros sucesos y diferencia de sucesos.\nFigura 4.5: Representación de sucesos disjuntos (izquierda), suceso contenido en otro suceso (centro) y diferencia de sucesos (derecha)\nPartición del espacio muestral. Dada una colección de sucesos \\(A_1, A_2, \\ldots\\),\ndecimos que es una partición del espacio muestral \\(\\Omega\\) si:\\(A_1, A_2, \\ldots: \\quad A_i \\subset \\Omega \\; \\forall \\)\\(A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\),\\(\\displaystyle \\underset{}\\bigcup A_i = \\Omega\\).La figura 4.6 representa gráficamente una partición del\nespacio muestral \\(\\Omega\\) en cinco sucesos \\(A_1, \\ldots, A_5\\).Nótese que los sucesos elementales de un experimento \\(\\omega_i\\) constituyen una\npartición del espacio muestral.\nFigura 4.6: Representación de una partición del espacio muestral\nDe la teoría de conjuntos se deducen fácilmente las siguientes propiedades\nde las operaciones con sucesos:Conmutativa:\n\\(\\cup B= B\\cup \\).\n\\(\\cap B= B\\cap \\).\n\\(\\cup B= B\\cup \\).\\(\\cap B= B\\cap \\).Asociativa:\n\\(\\cup (B \\cup C) = (\\cup B) \\cup C\\).\n\\(\\cap (B \\cap C) = (\\cap B) \\cap C\\).\n\\(\\cup (B \\cup C) = (\\cup B) \\cup C\\).\\(\\cap (B \\cap C) = (\\cap B) \\cap C\\).Distributiva:\n\\(\\cup (B \\cap C) = (\\cup B) \\cap (\\cup C)\\).\n\\(\\cap (B \\cup C) = (\\cap B) \\cup (\\cap C)\\).\n\\(\\cup (B \\cap C) = (\\cup B) \\cap (\\cup C)\\).\\(\\cap (B \\cup C) = (\\cap B) \\cup (\\cap C)\\).Leyes de De Morgan:\n\\((\\cup B)^c = ^c \\cap B^c\\).\n\\((\\cap B)^c = ^c \\cup B^c\\).\n\\((\\cup B)^c = ^c \\cap B^c\\).\\((\\cap B)^c = ^c \\cup B^c\\).\\(\\cup = \\cap = \\cup \\emptyset = \\cap \\Omega = \\).\\(\\cup \\Omega = \\Omega\\).\\(\\cap \\emptyset = \\emptyset\\).","code":""},{"path":"introp.html","id":"clasificación-de-los-espacios-muestrales","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.2.3 Clasificación de los espacios muestrales","text":"La primera clasificación que haremos de un espacio muestral es en función\nde su tamaño:Finito: consta de un número finito de sucesos elementales. Por ejemplo\nel lanzamiento de un dado: \\(\\Omega = \\{1, 2, 3, 4, 5, 6 \\}\\).Finito: consta de un número finito de sucesos elementales. Por ejemplo\nel lanzamiento de un dado: \\(\\Omega = \\{1, 2, 3, 4, 5, 6 \\}\\).Infinito numerable: el resultado del experimento tiene (al menos teóricamente)\ninfinitos posibles resultados, pero se pueden numerar. Por ejemplo el número\nde piezas correctas hasta que se\nproduce un fallo: \\(\\Omega = \\{ 0, 1, 2, 3, \\ldots \\}\\).Infinito numerable: el resultado del experimento tiene (al menos teóricamente)\ninfinitos posibles resultados, pero se pueden numerar. Por ejemplo el número\nde piezas correctas hasta que se\nproduce un fallo: \\(\\Omega = \\{ 0, 1, 2, 3, \\ldots \\}\\).Infinito numerable: el resultado del experimento tiene\ninfinitos posibles resultados, que se pueden numerar.\nPor ejemplo el tiempo hasta el fallo en el\nejemplo\nanterior45:\n\\(\\Omega = [0, \\infty)\\).Infinito numerable: el resultado del experimento tiene\ninfinitos posibles resultados, que se pueden numerar.\nPor ejemplo el tiempo hasta el fallo en el\nejemplo\nanterior45:\n\\(\\Omega = [0, \\infty)\\).Definimos una sigma álgebra de sucesos \\(\\sigma\\)-álgebra o \\(\\aleph\\) (aleph) como un\nconjunto de sucesos que verifican las siguientes propiedades:Pertenecen \\(\\aleph\\),Si un suceso pertenece \\(\\aleph\\), entonces su suceso complementario también pertenece \\(\\aleph\\),Si \\(\\{A_i\\}\\) es un conjunto de sucesos en \\(\\aleph\\), entonces la unión \\(\\displaystyle \\underset{}\\bigcup A_i\\) y\nla intersección \\(\\displaystyle \\underset{}\\bigcap A_i\\) pertenecen \\(\\aleph\\).Nótese la diferencia entre \\(\\Omega\\) y \\(\\aleph\\). Mientras el espacio muestral \\(\\Omega\\) es el\nconjunto de todos los sucesos elementales del experimento, la \\(\\sigma\\)-álgebra de\nsucesos \\(\\aleph\\) es el conjunto de todos los sucesos que podemos crear partir\ndel espacio muestral \\(\\Omega\\) y las operaciones de unión, intersección y\ncomplementariedad con esos sucesos. El par \\((\\Omega, \\aleph)\\) se dice que es un espacio probabilizable.Observamos al azar el tipo de participante en el estudio de uno tomado al azar.\nEntonces los posibles resultados del experimento o\nsucesos elementales es:\\[\\Omega = \\{, T, P\\}\\]Haciendo todas las operaciones posibles de unión, intersección y complementariedad,\npodemos llegar fácilmente la siguiente \\(\\sigma\\)-álgebra de\nsucesos:","code":""},{"path":"introp.html","id":"definiciones-de-probabilidad-y-sus-propiedades","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3 Definiciones de probabilidad y sus propiedades","text":"Ya hemos dicho anteriormente que la probabilidad es una medida del grado de\nincertidumbre sobre el resultado de un experimento. Ahora necesitamos formalizar\nla definición de probabilidad con el fin de trabajar matemáticamente\ncon ella.","code":""},{"path":"introp.html","id":"definición-clásica-o-de-laplace","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3.1 Definición clásica o de Laplace","text":"La definición clásica de la probabilidad, también conocida\ncomo definición de\nLaplace46,\nrequiere disponer de un espacio muestral finito\nreferido un experimento en el que todos los resultados posibles son igualmente probables.\nBajo estas condiciones, la probabilidad de un suceso cualquiera \\(\\) se obtiene\ncomo el cociente entre el número de casos favorables al suceso, dividido\npor el número total de casos posibles del experimento. Así:\\[P() = \\frac{\\text{casos favorables } }{\\text{casos posibles}}.\\]Utilizaremos la definición de Laplace para asignar probabilidades sucesos\ncuando tengamos una enumeración completa del espacio muestral como en los\nejemplos anteriores.En el lanzamiento de un dado equilibrado de seis caras, la probabilidad de sacar\nun seis es igual al cociente entre los casos favorables sacar un 6 (1) y los\ncasos posibles del experimento (6):\\[:\\text{ Sacar un 6 en el lanzamiento de un dado}\\]En el ejemplo de los sujetos en estudio, la probabilidad de que\nun sujeto al azar sea investigador es el cociente entre los casos favorables\nser investigador (4) y los casos posibles (52):","code":""},{"path":"introp.html","id":"ch07-defempirica","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3.2 Definición frecuentista o empírica","text":"La definición clásica de probabilidad se encuentra con dificultades para\nasignar probabilidades medida que los experimentos alcanzan cierta complejidad.\nPor una parte, siempre tenemos una descripción completa del espacio muestral,\no, simplemente, es infinito, con lo cual podemos aplicar la fórmula de Laplace.\notras veces tenemos la información disponible necesaria. Pensemos en la\nsituación habitual descrita en la figura 1.1 al principio de este\ncapítulo. Queremos asignar\nuna probabilidad un suceso referido nuestra población objeto de estudio.\nSin embargo, tenemos información de los casos posibles y favorables la\nocurrencia del suceso. lo sumo, tenemos acceso una muestra de datos\nde la población, la que podemos aplicar el experimento y obtener las\nfrecuencias de ocurrencia de los sucesos en cuestión. Pues bien, la\ndefinición frecuentista nos dice que si observamos la frecuencia\nde ocurrencia del suceso \\(\\), llamémosle \\(n()\\), en un número grande\nde experimentos \\(n\\), la frecuencia relativa de ocurrencia del suceso \\(\\) tiende\nla probabilidad del suceso \\(\\). Matemáticamente:\\[P() = \\lim\\limits_{n \\\\infty} \\frac{n()}{n}.\\]En experimentos fáciles de realizar, se puede comprobar empíricamente. Por\nejemplo, podemos lanzar una moneda e ir anotando la frecuencia\nde caras con cada repetición. Este tipo de experimentos son también\nfáciles de realizar mediante simulación. En la siguiente aplicación\nse puede simular la elección de elementos de un conjunto48.En la práctica, utilizaremos esta definición para asignar probabilidades\nsucesos en base datos históricos, experiencia previa, etc. En muchas\nocasiones, estos datos están disponibles en forma de porcentajes, y bastará\ncon dividir por 100 para transformarlos en una frecuencia relativa, que\nse tomará como probabilidad.","code":""},{"path":"introp.html","id":"definición-subjetivista","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3.3 Definición subjetivista","text":"En las dos definiciones anteriores de probabilidad, hemos asignado probabilidades\nsucesos en base unos determinados datos, bien de recuento de posibilidades,\nbien de frecuencias relativas. En ocasiones, se dispone de absolutamente\nningún dato de este tipo. Entonces las probabilidades se han de asignar de\nforma subjetiva, fijadas por un individuo en particular como su\ngrado de creencia acerca de la ocurrencia de un suceso. El individuo fija\nun valor entre cero y uno en base la evidencia de que dispone, que puede\nincluir juicios personales, y también interpretaciones priori sobre las dos\nconcepciones anteriores de la probabilidad, clásica y frecuentista. Por ejemplo,\npuede considerar la frecuencia relativa de fenómenos similares, y combinar esta\ninformación con sus conocimientos y percepciones sobre la materia de estudio.El enfoque subjetivista tiene especial interés en fenómenos que se prestan\nrepetición, así como en métodos de estadística Bayesiana, donde se fija\nuna probabilidad priori de los parámetros de la\npoblación49.\nExisten métodos\nespecíficos para asignar probabilidades subjetivas\nde forma racional, que quedan fuera de los objetivos de este libro, véase, por\nejemplo, Bruno de Finetti.50","code":""},{"path":"introp.html","id":"definición-en-iso-3534-1","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3.4 Definición en ISO 3534-1","text":"La definición estandarizada que proporciona la norma UNE-ISO 3534-1 es la\nsiguiente para la probabilidad de un suceso \\(\\):Probabilidad de un suceso \\(\\); \\(P()\\)Número real del intervalo cerrado \\([0, 1]\\) asignado un suceso— ISO 3534-1 2.5Nótese que en el estándar se entra en detalles matemáticos por el bien\nde la aplicabilidad en los procesos empresariales. obstante, esta\ndefinición es en esencia compatible y congruente con el resto de definiciones\nde probabilidad.","code":""},{"path":"introp.html","id":"definición-axiomática","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.3.5 Definición axiomática","text":"Si bien todas las definiciones anteriores son válidas y útiles en determinados\ncontextos, todas presentaban problemas para desarrollar una teoría de\nprobabilidades que se pudiera aplicar cualquier espacio probabilizable. La\nsiguiente definición\naxiomática51\nresolvió estos problemas.Una probabilidad \\(\\wp\\) es una función:\\[\n\\begin{split}\n\\wp: & \\; \\aleph \\longrightarrow [0, 1]\\\\\n& \\longrightarrow P()\n\\end{split}\n\\]que cumple:Primer axioma: \\(\\forall \\\\aleph \\; \\exists \\; P() \\geq 0\\).Primer axioma: \\(\\forall \\\\aleph \\; \\exists \\; P() \\geq 0\\).Segundo axioma: \\(P(\\Omega) = 1\\).Segundo axioma: \\(P(\\Omega) = 1\\).Tercer axioma: Dada la sucesión \\(A_1, \\ldots, A_i, \\ldots: A_i \\\\aleph \\; \\forall\\, , A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\), se cumple:Tercer axioma: Dada la sucesión \\(A_1, \\ldots, A_i, \\ldots: A_i \\\\aleph \\; \\forall\\, , A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\), se cumple:\\[P \\left (\\bigcup\\limits_{=1}^{\\infty} A_i \\right ) = \\sum\\limits_{=1}^{\\infty} P(A_i).\\]En lenguaje natural, el primer axioma indica que cada suceso le podemos asignar un número negativo llamado “probabilidad del suceso \\(\\)”; el segundo axioma asigna al suceso seguro una\nprobabilidad igual 1; el tercer axioma establece la forma de calcular probabilidades la\nunión de sucesos disjuntos o mutuamente excluyentes, mediante la suma de sus\nrespectivas probabilidades. Nótese que la formulación del axioma es válida para espacios\nmuestrales infinitos (numerables y numerables).partir de estos tres axiomas, se deducen los siguientes teoremas:Dados \\(n\\) sucesos disjuntos dos dos \\(A_1, \\ldots, A_n: A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\):\\[P \\left (\\bigcup\\limits_{=1}^{n} A_i \\right ) = \\sum\\limits_{=1}^{n} P(A_i).\\]\\(P(^c)=1-P()\\).\\(P(^c)=1-P()\\).\\(P(\\emptyset) = 0\\).\\(P(\\emptyset) = 0\\).Dados \\(A_1, A_2: A_1 \\subset A_2 \\implies P(A_1) \\leq P(A_2)\\).Dados \\(A_1, A_2: A_1 \\subset A_2 \\implies P(A_1) \\leq P(A_2)\\).\\(P(\\cup B) = P() + P(B) - P(\\cap B)\\).\\(P(\\cup B) = P() + P(B) - P(\\cap B)\\).\\(P(\\bigcup\\limits_{=1}^n A_i) = \\sum\\limits_{=1}^n P(A_i) - \\sum\\limits_{<j} P(A_i \\cap A_j) + \\sum\\limits_{<j<k} P(A_i \\cap A_j \\cap A_k) - \\ldots + (-1)^{n-1} P \\left(\\bigcap\\limits_{=1}^n A_i\\right )\\).\\(P(\\bigcup\\limits_{=1}^n A_i) = \\sum\\limits_{=1}^n P(A_i) - \\sum\\limits_{<j} P(A_i \\cap A_j) + \\sum\\limits_{<j<k} P(A_i \\cap A_j \\cap A_k) - \\ldots + (-1)^{n-1} P \\left(\\bigcap\\limits_{=1}^n A_i\\right )\\).El primer teorema particulariza el tercer axioma un conjunto finito de sucesos disjuntos del espacio muestral. El segundo teorema es una de las propiedades que más aplicaremos en cálculo\nde probabilidades, y nos indica cómo calcular la probabilidad de un suceso restándole 1 la probabilidad de su complementario. El tercer teorema es una consecuencia del anterior y del primer axioma, por los cuales la probabilidad del suceso imposible es cero. El cuarto teorema es\nde vital importancia cuando trabajemos con variables aleatorias y nos viene decir que si un\nsuceso está contenido en otro, la probabilidad del primero puede ser mayor que la del segundo.\nLos teoremas quinto y sexto nos permiten calcular probabilidades de la unión de cualesquiera\nconjuntos, sean o disjuntos. Una consecuencia fundamental de las propiedades de la probabilidad es:\\[ \\boxed{0 \\leq P() \\leq 1}.\\]La demostración de estos teoremas se puede encontrar, entre otros, en\nM. D. Ugarte, . F. Militino, . T. Arnholt.52 Asímismo, se puede comprobar fácilmente cómo las definiciones clásicas y frecuentistas cumplen todas estas\npropiedades y por lo tanto son coherentes con la definición axiomática de la probabilidad.Lanzamiento de un dado de seis caras. Sean los siguientes sucesos:\\(A_1:\\) “número impar”; \\(A_1 = \\{1, 3, 5\\}\\).\\(A_2:\\) “número par”; \\(A_2 = \\{2, 4, 6\\}\\).\\(A_3:\\) “número mayor que 4”; \\(A_3 = \\{5, 6\\}\\).\\(A_4:\\) “número menor o igual que 4”; \\(A_4 = \\{1, 2, 3, 4\\}\\).Podemos calcular cualquiera de estas probabilidades por la definición de Laplace, ya\nque los resultados elementales del experimento son equiprobables. Así:\\[P(A_1) = \\frac{1}{2}=0.5=P(A_2); P(A_3) = \\frac{2}{6}\\simeq 0.3333; P(A_4)=\\frac{4}{6}\\simeq 0.6667.\\]Por simple enumeración de los casos posibles podemos calcular las probabilidades de los siguientes sucesos:\\(A_1 \\cup A_3:\\) “número impar o mayor que cuatro”; \\(A_1 \\cup A_3=\\{1,3,5,6\\}\\); \\(P(A_1 \\cup A_3)=\\frac{4}{6} \\simeq 0.6667\\).\\(A_1 \\cup A_3:\\) “número impar o mayor que cuatro”; \\(A_1 \\cup A_3=\\{1,3,5,6\\}\\); \\(P(A_1 \\cup A_3)=\\frac{4}{6} \\simeq 0.6667\\).\\(A_1 \\cap A_3:\\) “número impar y mayor que cuatro”; \\(A_1 \\cap A_3=\\{5\\}\\);\n\\(P(A_1 \\cap A_3)=\\frac{1}{6} \\simeq 0.1667\\).\\(A_1 \\cap A_3:\\) “número impar y mayor que cuatro”; \\(A_1 \\cap A_3=\\{5\\}\\);\n\\(P(A_1 \\cap A_3)=\\frac{1}{6} \\simeq 0.1667\\).Y así sucesivemente para cada posible suceso \\(\\) subconjunto del espacio muestral \\(\\Omega=\\{1,2,3,4,5,6\\}\\).Y así sucesivemente para cada posible suceso \\(\\) subconjunto del espacio muestral \\(\\Omega=\\{1,2,3,4,5,6\\}\\).Ahora bien, también podemos aplicar las propiedades de la probabilidad sin necesidad de enumerar\no contar todas las posibilidades. Por ejemplo, conocidos \\(P(A_1), P(A_3)\\) Y \\(P(A_1\\cap A_3)\\):\\(P(A_1 \\cup A_3)=P(A_1) + P(A_3) - P(A_1 \\cap A_3) = 0.5 + 0.3333 - 0.1667 \\simeq 0.6667\\),que conduce, obviamente, al mismo resultado. medida que aumentan la complejidad de los experimentos, con espacios muestrales más grandes, o incluso infinitos, se hace dificultoso o imposible trabajar con enumeraciones, y es donde hay que aplicar la defición axiomática de la probabilidad.En nuestro ejemplo del estudio, podríamos estar\ninteresados en el suceso “ser mujer o joven.” Este suceso se\ncorrespondería con el suceso \\(M \\cup J\\). Para calcular esta probabilidad,\ntendríamos en cuenta, según los datos del ejemplo, que \\(P(M) = \\frac{1}{2}=0.5\\), \\(P(J) = \\frac{13}{52}=0.25\\), y \\(P(M \\cap J)=\\frac{5}{52}\\simeq 0.0962\\). Entonces:\\[P(M \\cup J)=P(M)+P(J)-P(M\\cap J)=0.5+0.25-0.0962 \\simeq 0.6538.\\]En los anteriores ejemplos hemos utilizado solamente el teorema referido\nla probabilidad de la unión de sucesos. El teorema de la probabilidad\ndel suceso complementario va ser la propiedad que más utilizaremos en\ncálculo de probabilidades, dado que, en muchas ocasiones, es más sencillo\nabordar el problema desde el punto de vista del suceso complementario.\nUn ejemplo es la paradoja de los cumpleaños.Si el día de nuestro cumpleaños asistimos algún evento en el que haya\nmás de 30 personas, es muy probable que nos canten el cumpleaños feliz\nmás de una persona.\nSupongamos una clase de 30 alumnos.\n¿Cuál es la probabilidad de que al menos dos alumnos\ncumplan años el mismo, día?.\nAbordar el problema directamente implicaría gran cantidad\nde consideraciones y costosos cálculos hasta llegar la solución, porque\nhabría que considerar todos los casos posibles y después calcular probabilidades\nde uniones e intersecciones. Sin embargo,\nse resuelve de forma casi inmediata sin consideramos la probabilidad del suceso\ncomplementario. Es decir, si:\\[: \\text{Al menos dos personas de un grupo de 30 cumplen años el mismo día,}\\]entonces el suceso complementario es:\\[^c: \\text{hay dos personas en un grupo de 30 que cumplen años el mismo día.}\\]Nótese cómo la probababilidad sería igual 1 si el grupo de personas fuera\nde 365 personas o\nmás53,\nya que en ese caso el suceso sería un suceso seguro. En este caso, el espacio muestral\nestará compuesto por el número de maneras que tendríamos de ordenar 30\nfechas de nacimiento dentro de un año (día-mes), para un conjunto total de 365 días\ndiferentes que tiene el año. Obviamente se pueden repetir las fechas, y por\ntanto el número total de casos posibles se corresponde con las variaciones con\nrepetición de 365 elementos tomados de 30 en 30:\\[\\mathit{VR}_{m,n} = m^n = 365^{30} \\simeq  7.392\\cdot 10^{76}.\\]Para calcular el número de casos favorables que nadie cumpla años el mismo día, fijamos el\ncumpleaños de la primera persona. Entonces la siguiente persona pueden cumplir años cualquiera\nde los 364 días restantes; fijados los dos primeros, la tercera persona puede cumplir años cualquiera de los 363 días restantes, y así sucesivamente. Por tanto, los casos favorables son las variaciones (sin repetición):\\[\\mathit{V}_{m,n}=365\\times 364 \\times \\ldots \\times (365-30+1) \\simeq 2.171\\cdot 10^{76}\\]y entonces:Para obtener los casos favorables, si intentamos utilizar la fórmula de\nlas variaciones utilizando los factoriales (ver apéndice C.2),\nla calculadora y el software pueden devolver un error, por poder calcular\nel factorial de 365.HOJA DE CÁLCULODisponemos en el rango A1:A30 los números del 365 (m) al 336 (m - n + 1). Entonces\npodemos obtener la probabilidad del ejemplo como:=1-PRODUCTO(A1:A30)/(365^30)MAXIMAMaxima sí puede trabajar con números grandes, la siguiente expresión devuelve\nla probabilidad pedida:1 - (factorial(365)/factorial(365-30))/365^30;REl siguiente código realiza los cálculos paso paso y devuelve la probabilidad pedida. Cambiando el valor 30 por otro número de personas cualquiera, se\npuede ver cómo aumenta la probabilidad.Una vez definida la medida de probabilidad \\(\\wp\\) con los axiomas y propiedades\nanteriores, llamamos espacio de probabilidad la terna:\\[(\\Omega, \\aleph, \\wp).\\]El estándar UNE-ISO 3534-1 recoge la definición axiomática de la probabilidad\nde la siguiente forma:sigma álgebra de sucesos; \\(\\sigma\\)-álgebra; sigma campo; \\(\\sigma\\)-campo; \\(\\aleph\\)Conjunto de sucesos con las siguientes propiedades:Pertenecen \\(\\aleph\\);Pertenecen \\(\\aleph\\);Si un suceso pertenece \\(\\aleph\\), entonces su suceso complementario también pertenece \\(\\aleph\\);Si un suceso pertenece \\(\\aleph\\), entonces su suceso complementario también pertenece \\(\\aleph\\);Si \\(\\{A_i\\}\\) es un conjunto de sucesos en \\(\\aleph\\), entonces la unión \\(\\displaystyle\\underset{}\\bigcup A_i\\) y\nla intersección \\(\\displaystyle \\underset{}\\bigcap A_i\\) de los sucesos pertenecen \\(\\aleph\\).Si \\(\\{A_i\\}\\) es un conjunto de sucesos en \\(\\aleph\\), entonces la unión \\(\\displaystyle\\underset{}\\bigcup A_i\\) y\nla intersección \\(\\displaystyle \\underset{}\\bigcap A_i\\) de los sucesos pertenecen \\(\\aleph\\).— ISO 3534-1 2.69Medida de probabilidad \\(\\wp\\)Función negativa definida sobre la sigma álgebra de sucesos tal que\\(\\wp(\\Omega) = 1\\)donde \\(\\Omega\\) denota el espacio muestral\\(\\wp \\left (\\bigcup\\limits_{=1}^{\\infty} A_i \\right ) = \\sum\\limits_{=1}^{\\infty} \\wp(A_i)\\)donde \\(\\{A_i\\}\\) es una secuencia de pares de sucesos disjuntos— ISO 3534-1 2.70Espacio de probabilidad (o espacio probabilístico); \\((\\Omega, \\aleph, \\wp)\\)Espacio muestral, una sigma álgebra de sucesos asociada, y una medida de probabilidad.— ISO 3534-1 2.68","code":"\nncumple <- 30\ncposibles <- 365^ncumple\ncfavorables <- prod(365:(365 - ncumple + 1))\nprob_ninguno <- cfavorables/cposibles\nprob_alguno <- 1 - cfavorables/cposibles\nprob_alguno\n#> [1] 0.7063162"},{"path":"introp.html","id":"probabilidad-condicionada-y-sus-consecuencias","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4 Probabilidad condicionada y sus consecuencias","text":"","code":""},{"path":"introp.html","id":"probabilidad-condicionada","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4.1 Probabilidad condicionada","text":"El concepto de probabilidad condicionada es uno de los más importantes\nen teoría de la probabilidad. En ocasiones,\nla ocurrencia o de ciertos sucesos del espacio muestral puede estar afectada\npor otros sucesos del espacio muestral. Por ejemplo, desde el punto de vista\nde la definición de probabilidad de Laplace, en experimentos secuenciales\n\\(A_1, \\ldots, A_n\\),\nes posible que los resultados de los sucesivos experimentos influyan\nen los resultados de los siguientes, y entonces\nhablaremos, por ejemplo, de la probabilidad del suceso \\(A_2\\) condicionada\nque ha ocurrido el suceso \\(A_1\\), y la calcularemos enumerando\nlos casos favorables y los casos posibles bajo el supuesto de haber\nsucedido \\(A_1\\). Esta situación aparece, por ejemplo, en los problemas de urnas.\nDesde el\npunto de vista de la definición frecuentista de la probabilidad, podemos\nconsiderar un experimento en el que se observen un suceso \\(\\) en distintos\ngrupos o localizaciones, siendo \\(B\\) el suceso que indica la pertenencia \nese determinado grupo o característica.\nSe pueden considerar las frecuencias relativas del suceso \\(\\) sólo\npara aquellos experimentos en los que ha sucedido \\(B\\), y llamar estas\nfrecuencias54\nfrecuencias de \\(\\) condicionadas \\(B\\), \\(fr_{|B}\\).\nEstas frecuencias relativas las podemos calcular dividiendo el número de veces que\nocurren tanto \\(\\) como \\(B\\) \\((n_{AB})\\) entre el número total de veces que ocurre \\(B\\), \\((n_{B})\\):\\[fr_{| B}=\\frac{n_{AB}}{n_B}.\\]Ahora bien, como \\(fr_A= \\frac{n_A}{n}\\), \\(fr_B= \\frac{n_B}{n}\\) y \\(fr_{AB}= \\frac{n_{AB}}{n}\\),\nse tiene:\\[fr_{| B}=\\frac{n\\cdot fr_{AB}}{n\\cdot fr_B}=\\frac{fr_{AB}}{fr_B}.\\]Es decir, la frecuencia condicionada es igual la frecuencia conjunta dividido\npor la frecuencia marginal del suceso condicionante. Así pues, dado que para un número grande de realizaciones del experimento, las\nfrecuencias relativas equivalen la probabilidad, podemos definir la\nprobabilidad del suceso \\(\\) condicionada al suceso \\(B\\) como:\\[\\boxed{P(| B)=\\frac{P(\\cap B)}{P(B)}},\\]siempre y cuando \\(P(B) > 0\\). Se demuestra\nfácilmente55\nque esta definición de probabilidad condicionada cumple\nque dado un suceso \\(\\\\aleph\\), \\((\\Omega, \\aleph, \\wp(\\cdot|)\\) es un espacio de\nprobabilidad.La tabla 4.1 contiene las frecuencias con las que se\nhan observado los sucesos aprobar y suspender dos elementos evaluables\nde una asignatura: un examen y un trabajo.Designemos \\(AE\\) y \\(SE\\) los sucesos “aprobar el examen” y “suspender el examen”\nrespectivamente, y \\(\\) y \\(ST\\) los sucesos “aprobar el trabajo” y “suspender”\nel trabajo respectivamente. La probabilidad de aprobar el examen será:\\[P(AE)=\\frac{40}{100} = 0.4.\\]Si incluimos más información modo de condición, podemos calcular por ejemplo\nla probabilidad de aprobar el examen condicionado que se ha aprobado el trabajo:\\[P(AE | )=\\frac{P(AE \\cap )}{P()}=\\frac{30/100}{35/100} \\simeq 0.8571 .\\]Tabla 4.1: Datos ejemplo probabilidad condicionada","code":""},{"path":"introp.html","id":"probabilidad-de-la-intersección-de-sucesos","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4.2 Probabilidad de la intersección de sucesos","text":"La definición de probabilidad condicionada la que hemos llegado, nos\npermite calcular la probabilidad de la intersección de dos sucesos\ncualesquiera sin más que despejar de la fórmula. Además, tendremos dos formas\nde calcularla, según conozcamos \\(P(|B)\\) o \\(P(B|)\\):\\[\\boxed{P(\\cap B)=P(|B)\\cdot P(B)=P(B|)\\cdot P()}.\\]Recuerda que \\(\\cap B\\) significa y B, mientras que\n\\(|B\\) significa si ocurre B.La probabilidad condicionada aparece en los muestreos sin reemplazamiento. Se suele\nasociar los problemas de urnas, o también la extracción de cartas de una\nbaraja. Por ejemplo, podemos calcular la probabilidad de sacar dos figuras\nseguidas de una baraja de cartas francesa, con 52 cartas en total de las cuales\n12 son figuras (J, Q, K de cada uno de los cuatro palos). Entonces, si definimos\n\\(A_1\\) como “sacar figura en la primera extracción” y \\(A_2\\) como “sacar figura\nen la segunda extracción,” entonces lo que buscamos es la probabilidad de que\nocurran los dos sucesos, \\(P(A_1 \\cap A_2)\\):En nuestro ejemplo de los sujetos en estudio, ¿cuál es la probabilidad de que un cliente al azar sea mujer y además responda al tratamiento?partir de la probabilidad condicionada se llega la regla de la cadena\npara calcular la probabilidad de la intersección de una serie de sucesos. La regla\nconsiste en ir multiplicando cada vez la probabilidad del suceso \\(A_i\\) condicionada\nla intersección de todos los anteriores.\\[P\\left( \\bigcap\\limits_{=1}^{n} A_i \\right) = P(A_1)\\cdot P(A_2|A_1)\\cdot P(A_3|A_1 \\cap A_2)\\cdot\\ldots\\cdot P\\left(A_n | \\bigcap\\limits_{=1}^{n-1} S_i \\right). \\]Por ejemplo, en una urna hay 5 bolas rojas y 3 bolas blancas. Hacemos 3\nextracciones. Si en una extracción sale blanca, devolvemos la bola\nla urna y metemos 2 bolas blancas adicionales. ¿Qué probabilidad\nhay de sacar 3 blancas seguidas?Si definimos los sucesos \\(A_1\\), \\(A_2\\) y \\(A_3\\) como “sacar bola blanca en la primera,\nsegunda y tercera extracción respectivamente,” entonces estamos buscando:\\[P(A_1 \\cap A_2 \\cap A_3),\\]\nque utilizando la regla de la cadena calcularemos como:\\[P(A_1)\\cdot P(A_2|A_1) \\cdot P(A_3|A_1 \\cap A_2).\\]En la situación inicial hay 3 de ocho bolas blancas. En el segundo experimento,\nsi hemos sacado blanca, la devolvemos y añadimos dos más, es decir tenemos 5 de diez\nbolas blancas. si la segunda vuelve ser blanca, entonces en el tercer experimento\ntenemos 7 de 12 bolas blancas. Por lo tanto:","code":""},{"path":"introp.html","id":"independencia-de-sucesos","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4.3 Independencia de sucesos","text":"Si bien en muchas ocasiones el conocimiento de ciertos eventos afectan la\nprobabilidad de ocurrencia de otros, esto siempre tiene por qué ser así.\nEn estos casos, diremos que dos sucesos son independientes si el conocimiento\nde la ocurrencia de uno de ellos modifica la probabilidad de aparición del otro.\nPor tanto, en esos casos:\\[P(|B) = P()\\quad \\text{y}\\quad P(B|) = P(B).\\]Entonces, por la propia definición de la probabilidad condicionada, se tiene que\nsi dos sucesos son independientes, entonces:\\[\\boxed{P(\\cap B)=P()\\cdot P(B)}.\\]\nEsta fórmula, que es una definición en sí misma de independendencia de sucesos, nos\nproporciona también un método para comprobar si dos sucesos son independientes o \nconocidas las probabilidades de los mismos y la de la\nintersección56.Para más de dos sucesos, la regla de la cadena explicada más arriba se\nextiende inmediatamente de forma que la probabilidad de la intersección de \\(n\\) sucesos independientes es el producto de sus probabilidades:\\[P(A_1\\cap \\ldots \\cap A_n)=P(A_1) \\cdot \\ldots \\cdot P(A_n).\\]\nY en el caso particular de que los \\(n\\) sucesos sean equiprobables, tales que \\(P(A_i) = p \\;\\forall \\), entonces:\\[P(A_1\\cap \\ldots \\cap A_n)=p^n.\\]El lanzamiento sucesivo de una moneda o de un dado son claros ejemplos de sucesos independientes.En el lanzamiento de un dado dos veces seguidas (o lo que es lo mismo, en el lanzamiento\nde dos dados), el resultado del primero influye en el segundo. Por tanto, la\nprobabilidad de sacar dos seises en el lanzamiento de dos dados es:","code":""},{"path":"introp.html","id":"probabilidad-condicionada-e-independencia-en-iso-3534-1","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4.4 Probabilidad condicionada e independencia en ISO 3534-1","text":"La norma UNE-ISO 3534-1 recoge las definiciones de probabilidad condicionada\ne independencia de la siguiente forma:Probabilidad condicionada; \\(P(|B)\\)Probabilidad de la intersección de \\(\\) y \\(B\\) dividida por la probabilidad de \\(B\\).— ISO 3534-1 2.6Sucesos independientesPar de sucesos tal que la probabilidad de la intersección de los dos sucesos es el producto de las probabilidades individuales.— ISO 3534-1 2.4","code":""},{"path":"introp.html","id":"probabilidad-total-y-fórmula-de-bayes","chapter":"Capítulo 4 Introducción a la Probabilidad","heading":"4.4.5 Probabilidad total y fórmula de Bayes","text":"La probabilidad condicionada nos permite calcular probabilidades de sucesos\nde los que tenemos información parcial, en el sentido de que conocemos\nsu probabilidad condicionada algún otro suceso del espacio muestral,\npero queremos saber la probabilidad total del suceso, independientemente\nde aquellos sucesos. Las condiciones para que podamos calcular la probabilidad\ntotal de este suceso, llamémosle \\(B\\), son:Disponer de una partición de sucesos del espacio muestral \\(A_1, A_2, \\ldots, A_n\\)\ntales que \\(A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\) y \\(\\displaystyle \\underset{=1}{\\overset{n}\\bigcup A_i} = \\Omega\\).Disponer de una partición de sucesos del espacio muestral \\(A_1, A_2, \\ldots, A_n\\)\ntales que \\(A_i \\cap A_j = \\emptyset \\; \\forall \\neq j\\) y \\(\\displaystyle \\underset{=1}{\\overset{n}\\bigcup A_i} = \\Omega\\).Conocer las probabilidades de cada uno de esos sucesos que forman la partición, \\(P(A_i)\\).Conocer las probabilidades de cada uno de esos sucesos que forman la partición, \\(P(A_i)\\).Conocer las probabilidades del suceso de interés condicionadas cada uno de los sucesos\nque forman la partición del espacio muestral, es decir, \\(P(B|A_i)\\).Conocer las probabilidades del suceso de interés condicionadas cada uno de los sucesos\nque forman la partición del espacio muestral, es decir, \\(P(B|A_i)\\).Entonces, según el teorema de la probabilidad total, se verifica que:\\[\\boxed{P(B)=\\sum\\limits_{=1}^{n} P(B/A_i)\\cdot P(A_i)}.\\]\nEn efecto, podemos ver gráficamente en la figura 4.7 que cada sumando de la\nfórmula de la probabilidad total se corresponde con las intersecciones\ndel suceso de interés \\(B\\) con cada uno de los sucesos de la partición \\(A_i\\). Como\nestas intersecciones son sucesos disjuntos, la probabilidad de su unión es la\nsuma de sus probabilidades por las propiedades de la probabilidad.\nFigura 4.7: Representación del espacio muestral particionado más otro suceso\nEl desarrollo de la fórmula de la probabilidad condicionada partir de la situación\ndescrita para calcular la probabilidad total, nos permite darle la vuelta\nla condición y encontrar probabilidades de los sucesos de la partición \\(A_i\\) condicionados\nque se haya producido el suceso \\(B\\). Partimos de la propia definición de \\(P(A_i|B)\\):\\[P(A_i|B)=\\frac{P(A_i\\cap B)}{P(B)}.\\]\nPero su vez, la probabilidad del numerador la podemos escribir como \\(P(A_i \\cap B)=P(B|A_i)\\cdot P(A_i)\\), y la probabilidad del denominador, aplicando la fórmula de la probabilidad total, es \\(P(B)=\\sum\\limits_{=1}^{n} P(B/A_i)\\cdot P(A_i).\\) Lo que da lugar\nla fórmula de Bayes o Teorema de Bayes:\\[\\boxed{P(A_i|B)=\\frac{P(B|A_i)\\cdot P(A_i)}{\\sum\\limits_{=1}^{n} P(B/A_i)\\cdot P(A_i)}},\\]\nsiempre que \\(P(B>0)\\), que se puede expresar de forma simplificada como:\\[\\boxed{P(A_i|B)=\\frac{P(B|A_i)\\cdot P(A_i)}{P(B)}}\\]En una empresa que produce componentes electrónicos tomamos 5 lotes de producto,\ncada uno compuesto de 50 componentes. Hay dos tipos de lotes. Los del\ntipo 1 (\\(A_1\\)) tienen 48 componentes correctos y 2 defectuosos. Los del tipo 2 (\\(A_2\\))\ntienen 45 componentes correctos y 5 defectuosos. Tenemos 3 lotes tipo 1 y 2\nlotes tipo 2. Si se toma uno de los 5 lotes al azar y se saca de éste\nuna pieza, ¿qué probabilidad hay de que ese componente sea defectuoso?La figura 4.8 representa la partición del espacio muestral de\neste ejemplo.En este ejemplo se dan todos los elementos que habíamos descrito para\ncalcular la probabilidad total del suceso \\(B:\\) “el componente es defectuosos.”\nTenemos información parcial, en el sentido de que conocemos las probabilidades\nde ser defectuoso para cada uno de los tipos de lote, es decir \\(P(B|A_1)=\\frac{2}{50}=0.04\\)\ny \\(P(B|A_2)=\\frac{5}{50}=0.1\\). También conocemos las probabilidades de los\ndos sucesos que constituyen la partición, \\(P(A_1) = \\frac{3}{5}=0.6\\) y\n\\(P(A_2) = \\frac{2}{5}=0.4\\). Entonces, por el teorema de la probabilidad total:\\[P(B)=P(B|A_1)\\cdot P(A_1) + P(B|A_2)\\cdot P(A_2)= 0.04\\cdot 0.6 + 0.1\\cdot 0.4=0.064.\\]\nSupongamos ahora que se extrae del conjunto de todos los lotes un componente al azar,\ny resulta que es defectuoso. ¿Cuál es la probabilidad de que esa pieza provenga de un\nlote del tipo 1?Nótese que ahora lo que buscamos es \\(P(A_1|B)\\), como conocemos las \\(P(B|A_i)\\) y\n\\(P(A_i)\\), entonces podemos aplicar la fórmula de Bayes. Como ya hemos calculado\nantes la probabilidad total de \\(B\\), podemos usar la fórmula abreviada:\nFigura 4.8: Representación del espacio muestral del ejemplo de los componentes electrónicos\nEn nuestro ejemplo, conocíamos las probabilidades de que un sujeto responda al tratamiento\nsegún si es hombre o mujer. También conocemos la probabilidad\nde que el sujeto sea hombre o mujer. Entonces podemos calcular la probabilidad\nde que un sujeto (independientemente de si es hombre o mujer) responda al tratamiento como:\\[P(S)=P(S|M)\\cdot P(M) + P(S|H)\\cdot P(H)= \\frac{2}{6}\\cdot \\frac{1}{2} + \\frac{1}{6}\\cdot \\frac{1}{2} = 0.25.\\]Si un sujeto responde al tratamiento, la probabilidad de que sea mujer es:El problema de Monty HallMonty Hall es el nombre del presentador del concurso televisivo\nestadounidense Let’s make deal\nque se emitió entre 1963 y 1990. En alguna de las fases del programa,\nel concursante tiene que elegir una entre tres puertas, dos de las cuales\ntienen detrás una cabra, mientras que la otra tiene un coche.\nUna vez elegida la puerta, el presentador muestra el contenido de una\nde las otras dos puertas, que contiene una cabra. Entonces el concursante\ntiene la opción de cambiar su puerta por la otra que queda cerrada.\n¿Es más ventajoso cambiar de puerta o quedarse con la elegida inicialmente?\n¿O da lo mismo?La solución puede parecer contraintuitiva, aunque tanto desde el razonamiento\ntravés de las frecuencias como con un desarrollo matemático se llega \nla misma conclusión. Y la clave está en la probabilidad condicionada.El problema de Monty Hall dio lugar historias curiosas que se pueden consultar en\nCorbalán Sanz.57 Por ejemplo, el gran matemático Paul Erdös solo aceptó\ncomo buena la solución real tras comprobarla en una simulación por ordenador.\nInvito al lector que concurse en la aplicación que\nse muestra continuación58\ndurante un buen número de jugadas y\ncompruebe través de las frecuencias relativas qué estrategia ofrece mayor\nprobabilidad de ganar el coche.","code":""},{"path":"vauni.html","id":"vauni","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"Capítulo 5 Variable aleatoria univariante","text":"Trabajar con sucesos y todas sus combinaciones posibles puede resultar muy costoso,\no incluso imposible. Con las variables aleatorias pasamos del ámbito de los sucesos los números\nreales, de forma que podemos hacer cálculos numéricos.\nEl interés de las variables aleatorias es poder modelizar la incertidumbre mediante\nellas. Es importante tener en cuenta que las\npropiedades de las variables aleatorias son teóricas.\nMediante la inferencia estadística, podremos utilizar datos empíricos\nde muestras para obtener conclusiones sobre la variable aleatoria que caracteriza la población,\nrecuérdese la figura 4.1 al principio del capítulo 4.La figura 5.1 muestra la relación de las variables aleatorias con la\npoblación. En una muestra tenemos datos con los que calculamos estadísticos (media,\nvarianza, etc) de esos datos. Representamos las frecuencias mediante histogramas.\nPor su parte, la población sigue una distribución de probabilidad teórica, con unas\ncaracterísticas teóricas (media, varianza, etc.). Ambos “mundos” se relacionan mediante\nla inferencia estadística, que se trata en este texto.\nFigura 5.1: Variables aleatorias vs. datos empíricos\n","code":""},{"path":"vauni.html","id":"concepto-y-definición-de-variable-aleatoria","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.1 Concepto y definición de variable aleatoria","text":"Las variables aleatorias son variables numéricas cuyos valores vienen determinados por el azar.\nUtilizaremos letras mayúsculas \\(X, Y, \\ldots\\) para representar variables aleatorias, y letras minúsculas \\(x, y, \\ldots\\) para representar los valores que toman. En definitiva, asignamos un número cada posible resultado del experimento.\nMatemáticamente, una variable aleatoria es una función definida sobre el espacio muestral \\(\\Omega\\)\nperteneciente un espacio de probabilidad \\((\\Omega, \\aleph, \\wp)\\)\ny que toma valores en el conjunto de los números reales \\(\\mathbb{R}\\):\\[\n\\begin{aligned}\nX: \\quad & \\Omega \\longrightarrow \\mathbb{R}\\\\\n& \\omega \\longrightarrow X(\\omega)\n\\end{aligned}\n\\]La variable aleatoria así definida cumple las siguientes características:La imagen de cada elemento del espacio muestral, \\(X(\\omega)\\), es única .La inversa de la variable aleatoria aplicada cualquier intervalo de \\(\\mathbb{R}\\)\npertenece la sigma álgebra de sucesos \\(\\aleph\\).\\[M \\\\mathbb{R} \\implies X^{-1}(M) \\\\aleph.\\]La medida de probabilidad \\(\\wp\\) del espacio de probabilidad \\((\\Omega, \\aleph, \\wp)\\)\nse aplica entonces intervalos de los números reales en vez de sucesos de \\(\\aleph\\):\\[M \\\\mathbb{R} \\implies \\wp(M)=P[X \\M].\\]\nesta medida de probabilidad inducida por una variable aleatoria se le suele denominar modelo de distribución de probabilidad.Consideremos un experimento consistente en lanzar una moneda equilibrada\nal aire tres veces. El espacio\nmuestral de este experimento aleatorio es el siguiente:\\[\\Omega=\\{ (+,+,+), (c,+,+), (+,c,+), (+,+,c), (c,c,+),(c,+,c),  (+,c,c), (c,c,c) \\}\\]Definamos ahora la variable aleatoria “Número de caras” en el experimento\nanterior. La variable aleatoria quedaría definida como sigue:\\[\\begin{aligned}\nX: \\quad & \\Omega & \\longrightarrow & \\quad \\mathbb{R}\\\\\n& (+,+,+) & \\longrightarrow & \\quad 0\\\\\n& (c,+,+) & \\longrightarrow & \\quad 1\\\\\n& (+,c,+) & \\longrightarrow & \\quad 1\\\\\n& (+,+,c) & \\longrightarrow & \\quad 1\\\\\n& (c,c,+) & \\longrightarrow & \\quad 2\\\\\n& (c,+,c) & \\longrightarrow & \\quad 2\\\\\n& (+,c,c) & \\longrightarrow & \\quad 2\\\\\n& (c,c,c) & \\longrightarrow & \\quad 3\n\\end{aligned}\\]Por tanto, el campo de variación de la variable aleatoria \\(X\\) o imagen de \\(X\\) (\\(\\mathit{Im}(X)\\)) es:\\[\\mathit{Im}(X) = \\{0, 1, 2, 3\\}\\]Ahora, basándonos en el espacio de probabilidad \\((\\Omega, \\aleph, \\wp)\\), podemos\ncalcular probabilidades sobre cualquier subconjunto de \\(\\mathbb{R}\\), por ejemplo:","code":""},{"path":"vauni.html","id":"tipos-de-variables-aleatorias","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.1.1 Tipos de variables aleatorias","text":"Las variables aleatorias quedan definidas por su campo de variación y el\nconjunto de probabilidades que toman. El campo de variación es el recorrido\nde la variable aleatoria, es decir, los valores que puede tomar. El conjunto de probabilidades\nes el definido por la medida de probabilidad \\(\\wp\\).De acuerdo la naturaleza de su campo de variación, las variables aleatorias\npueden ser principalmente de dos tipos:Discretas: toman un conjunto de valores numerable (\\(x_i\\)).Continuas: toman un conjunto de valores numerable (\\(x\\)).También hay variables aleatorias mixtas, que se tratan en este texto.","code":""},{"path":"vauni.html","id":"operaciones-con-variables-aletorias","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.1.2 Operaciones con variables aletorias","text":"En general, una función de variables aleatorias es otra variable aleatoria.\nSobre una o varias variables aleatorias podemos definir funciones.\nEn particular, en los próximos apartados\nde este capítulo definiremos funciones para realizar cálculo de probabilidades\nsobre los valores que puede tomar la variable aleatoria, transformaciones\nde la variable aleatoria para calcular características de las mismas,\ny combinaciones de variables aleatorias y sus propiedades. Matemáticamente:\\[\n\\begin{aligned}\nX: \\quad & \\Omega  \\longrightarrow \\mathbb{R}\\\\\ng(X): \\quad & \\mathbb{R} \\longrightarrow \\mathbb{R}\\\\\ng(X,Y): \\quad & \\mathbb{R}^2 \\longrightarrow \\mathbb{R}\\\\\n\\end{aligned}\n\\]Los siguientes son ejemplos de funciones aplicadas variables aleatorias:\\[X^2; \\quad 1.5\\cdot X; \\quad aX + b; \\quad X\\cdot Y; \\quad \\ldots\\]","code":""},{"path":"vauni.html","id":"variables-aleatorias-y-conjuntos","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.1.3 Variables aleatorias y conjuntos","text":"El paso de sucesos variables aleatorias nos va permitir operar con probabilidades\nde la misma forma que hacíamos con los sucesos. Las mismas operaciones\nque hacíamos con sucesos, las vamos poder realizar con subconjuntos\nde los números reales, ya que, en definitiva,\n\\(\\mathbb{R}\\) es un conjunto. Así, el complementario de un suceso,\npasa ser el complementario de un intervalo o\nconjunto de intervalos de los números reales, la unión de dos sucesos pasa ser el conjunto de\nnúmeros que pertenecen alguno de los dos subconjuntos de números reales, y la\nintersección de sucesos pasa ser el conjunto de números que pertenecen los dos\nsubconjuntos de números reales. Algunos ejemplos:Complementario de un suceso: \\([X \\leq 1]^c\\)= \\(X>1\\).Unión de sucesos: \\([10, 20] \\cup (15, 30) = [10, 30)\\).Intersección de sucesos: \\([10, 20] \\cap (15, 30) = (15, 20]\\).","code":""},{"path":"vauni.html","id":"función-de-distribución","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.2 Función de distribución","text":"Definamos una función sobre una variable aleatoria que le asigne cada\nvalor de \\(X\\), \\(x\\), la probabilidad de que la variable aleatoria \\(X\\) tome\nvalores menores o iguales que dicho valor \\(x\\):\\[\\boxed{F(x) = P[X \\leq x]}.\\]\\(F\\) será por tanto una función cuyo dominio es \\(\\mathbb{R}\\) y\nrecorrido el intervalo \\([0, 1]\\):\\[\\begin{aligned}F: & \\mathbb{R}  \\longrightarrow & [0, 1]\\\\\n& x \\longrightarrow & F(x)\n\\end{aligned}\\]Propiedades de la función de distribución:Está acotada en el intervalo \\([0, 1]\\): \\(0 \\leq F(x) \\leq 1\\).Monóntona decreciente: \\(< b \\implies F() \\leq F(b) \\;\\;\\forall , b \\\\mathbb{R}\\).Continua por la derecha: \\(\\lim\\limits_{x \\x_0^+} F(x) = F(x_0)\\).\\(\\lim\\limits_{x \\\\infty} F(x) = 1\\).\\(\\lim\\limits_{x \\-\\infty} F(x) = 0\\).Una consecuencia de estas propiedades es la siguiente, que nos proporciona\nuna forma de calcular probabilidades para cualquier intervalo partir de la\nfunción de distribución:\\[\\boxed{P[< X \\leq b] = F(b)- F()}.\\]Podemos comprobarlo fácilmente pensando en los números reales como conjuntos y\naplicando las propiedades de la probabilidad. Efectivamente, partir de \\(F(b)\\),\nque se corresponde con la \\(P[X\\leq b]\\), y como \\(< b\\):\\[X \\leq b = (-\\infty, b] = (-\\infty , ] \\cup (, b]. \\]\nComo \\((-\\infty , ]\\) y \\((, b]\\) con conjuntos disjuntos (sucesos mutuamente excluyentes):\\[P[X \\(-\\infty, b]\\, ] = P[X \\(-\\infty , ]\\,] + P[X \\(, b]\\,]\\implies\\\\\n P[X \\(, b]\\,] = P[X \\(-\\infty, b]\\, ] - P[X \\(-\\infty , ]\\,] = F(b)- F().\\]Además, por las propiedades de la probabilidad59:\\[\\boxed{P[X>] = 1-F()},\\]\ndado que \\(P[X>] = P[(X\\leq )^c]=1 -P[X\\leq ]=1-F().\\)","code":""},{"path":"vauni.html","id":"sec:vadi","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.3 Variable aleatoria discreta","text":"Son variables aleatorias discretas aquellas que pueden tomar un conjunto de valores\nfinito o infinito numerable,\n\\(x_i\\), \\(=1, 2, \\ldots, n\\) o \\(=1, 2, \\ldots, \\infty\\).\nFormalmente, son aquellas cuya función de distribución es continua.\nEsta discontinuidad es de salto finito,\ny los saltos se producen en los valores que toma la variable, \\(x_i\\).\ncada posible valor \\(x_i\\) se le asigna una probabilidad \\(p(x_i) = P[X=x_i]\\).\nLos saltos son de longitud igual \\(p(x_i)\\).","code":""},{"path":"vauni.html","id":"función-de-probabilidad","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.3.1 Función de probabilidad","text":"Dado que la variable aleatoria \\(X\\) toma valores entre \\(x_{-1}\\), y \\(x_i\\), podemos\ndefinir la función de probabilidad de una variable aleatoria discreta como:\\[\\begin{aligned}\np: &\\mathbb{R} \\longrightarrow [0, 1]\\\\\n&X \\longrightarrow p(x_i)\n\\end{aligned}\\]\\[p(x_i) = P[X=x_i]=P[x_{-1}<X \\leq x_i] = F(x_i)-F(x_{-1}).\\]Nótese que la expresión anterior demuestra la magnitud de los saltos en las discontinuidades\nde la función de distribución de una variable aleatoria discreta. La función de probabilidad\nde una variable aleatoria discreta también se puede llamar función de cuantía o\nfunción de masa de probabilidad. Se puede encontrar también la notación abreviada \\(p_i\\)\npara referirse \\(p(x_i)\\).Para que una función \\(p(x_i)\\) sea función de probabilidad debe cumplir las siguientes\ncondiciones:\\(p(x_i) \\geq 0 \\; \\forall \\).\\(\\sum\\limits_{=1}^\\infty p(x_i) = 1\\).partir de la función de probabilidad podemos llegar la función de distribución\nde cualquier variable aleatoria discreta como sigue:\\(F(x_i) = \\sum\\limits_{j=1}^p(x_j),\\)esto es, acumulando la probabilidad de los valores iguales o inferiores \ncada valor \\(x_i\\).En el experimento descrito anteriormente de lanzar tres monedas, definíamos la\nvariable aleatoria:\\[X: \\text{Número total de caras}.\\]La función de probabilidad de esta variable aleatoria la podemos\ncalcular por la definición de Laplace contando los casos favorables de\n\\(\\Omega\\) para cada valor de la variable aleatoria \\(X\\), y sería la siguiente:\\[\\begin{aligned}\np: & \\quad \\mathbb{R} & \\longrightarrow & \\quad [0,1]\\\\\n& \\quad X & \\longrightarrow & \\quad p(x_i) = P[X = x_i]\\\\\n& \\quad 0 & \\longrightarrow & \\quad P[X = 0] = \\frac{1}{8}\\\\\n& \\quad 1 & \\longrightarrow & \\quad P[X = 1] = \\frac{3}{8}\\\\\n& \\quad 2 & \\longrightarrow & \\quad P[X = 2] = \\frac{3}{8}\\\\\n& \\quad 3 & \\longrightarrow & \\quad P[X = 3] = \\frac{1}{8}\n\\end{aligned}\\]La figura 5.2 representa gráficamente la función de\nprobabilidad60.partir de la función de probabilidad, aplicando que \\(F(x_i) = \\sum\\limits_{j=1}^p(x_j)\\), la función de distribución\nsería la siguiente:\\[\\begin{aligned}\nF: & \\quad \\mathbb{R} & \\longrightarrow & \\quad [0,1]\\\\\n& \\quad X & \\longrightarrow & \\quad F(x_i) = P[X \\leq x_i]\\\\\n& \\quad 0 & \\longrightarrow & \\quad P[X \\leq 0] = \\frac{1}{8}\\\\\n& \\quad 1 & \\longrightarrow & \\quad P[X \\leq 1] = \\frac{4}{8}=\\frac{1}{2}\\\\\n& \\quad 2 & \\longrightarrow & \\quad P[X \\leq 2] = \\frac{7}{8}\\\\\n& \\quad 3 & \\longrightarrow & \\quad P[X \\leq 3] = \\frac{8}{8}=1\n\\end{aligned}\\]La figura 5.3 representa gráficamente la función de distribución, que\nse puede expresar de la siguiente forma:\nFigura 5.2: Representación de la función de probabilidad para el experimento de las monedas\n\nFigura 5.3: Representación de la función de distribución para el ejemplo de las monedas\nOtros ejemplos de variables aleatorias discretas serían aquellos en los\nque realizamos recuentos u observamos características, por ejemplo:Número de defectos por m2 en una superficie.Indicador de pieza correcta/incorrecta.Puntuación en una escala de valoración (por ejemplo Likert).Número de clientes que llegan un banco cada hora.Número de unidades defectuosas en un lote de productos.Tabla 5.1: Funciones de probabilidad y distribución para la variable discreta del ejemplo ilustrativo\nFigura 5.4: Representación de la función de probabilidad de una variable aleatoria discreta\n\nFigura 5.5: Representación de la función de distribución de la variable aleatoria discreta del ejemplo ilustrativo\n","code":""},{"path":"vauni.html","id":"variable-aleatoria-continua","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.4 Variable aleatoria continua","text":"Son variables aleatorias continuas aquellas que pueden tomar un\nnúmero infinito numerable de valores.\nFormalmente, son aquellas cuya función de distribución, \\(F(x)\\), es continua y\nderivable en todo su dominio. Al ser la variable continua, puede tomar\ncualquier valor en un intervalo de su dominio. Podemos utilizar las propiedades\nde la función de distribución para calcular probabilidades en cualquier intervalo\nde la siguiente forma:\\[\\begin{equation}\n  P[<X\\leq b]=F(b)-F().\n  \\tag{5.1}\n\\end{equation}\\]Ahora bien, por ser \\(F\\) continua, entre \\(\\) y \\(b\\) siempre hay masa de probabilidad, y podemos obtener\nuna función de probabilidad como en el caso discreto. Esto es porque existe un valor\nanterior uno dado \\(x\\). Podemos acercar\nlos dos extremos del intervalo tanto como queramos, por ejemplo en la ecuación (5.1) para calcular \\(P[X=b]\\) podríamos buscar el valor anterior \\(b\\) aproximando \\(\\) \\(b\\), pero\nsiempre habría un valor más allá, y finalmente la\nprobabilidad para un valor concreto de la variable aleatoria es igual cero.\\[\\boxed{P[X=x]=0}\\; \\forall x \\\\mathbb{R}.\\]Es importante señalar que, en la práctica, el número de valores de una variable\naleatoria que podamos medir será finito, pero la variable aleatoria seguirá siendo\ncontinua conceptualmente, y la aplicación de sus propiedades nos permitirá resolver\naquellos problemas prácticos, aunque el aparato de medida utilizado \nnos permita ir más allá de cierta precisión.\nPiensa en tu marca de cerveza favorita (o cualquier otra bebida), por ejemplo en el formato de 33 cl (tercio). Cuando la pides en un bar, ¿cuál crees que es la probabilidad de que la botella tenga exactamente 33 cl?\n\nEn realidad, si medimos con una precisión, por ejemplo, de un decimal, podemos obtener mediciones de \\(33.0\\) cl. Pero las mediciones están sujetas un error, y en realidad lo que nos está diciendo esa medición es que el volumen está entre \\(32.95\\) y \\(33.05\\), intervalo del cual sí podemos calcular su probabilidad.\nEntonces nos surge la siguiente pregunta: si podemos calcular la probabilidad\nde los sucesos individuales, ¿cómo saber qué valores son más probables?\n¿dónde se concentra la probabilidad? Precisamente la continuidad de la\nfunción de distribución nos proporciona la herramienta matemática para resolver\nestas cuestiones. La figura 5.6\nmuestra la representación gráfica de la función de densidad \\(F(x)\\) de una determinada\nvariable aleatoria \\(X\\).\nFigura 5.6: Función de distribución de una variable continua y probabilidad de un intervalo\nComo podemos ver, la probabilidad en un intervalo cualquiera,\nes el cambio que se produce en la función de distribución entre los\nextremos del intervalo. Si acercamos los extremos del intervalo, es decir, hacemos que\n\\(b\\) tienda \\(\\), obtenemos la tasa instantánea de cambio en un\npunto, que representa la masa de probabilidad en ese punto, y que es la derivada\nde la función de distribución en ese punto:\\[\\lim\\limits_{b \\}\\frac{F(b)- F()}{b-}.\\]","code":""},{"path":"vauni.html","id":"función-de-densidad","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.4.1 Función de densidad","text":"Si pensamos en la probabilidad como la tasa de cambio en la función de distribución, entonces podemos definir la densidad de probabilidad como la derivada de la función de distribución, y calcular así probabilidades con esa función, llamada función de densidad que se\nrepresenta por \\(f(x)\\):\\[f(x)= \\frac{d F(x)}{dx}.\\]Además, por el teorema fundamental del cálculo, podemos obtener la función de distribución \npartir de la función de densidad mediante la integral:\\[F(x)=\\int_{-\\infty}^x f(t) dt =P[X\\leq x].\\]\nLa figura 5.7 representa la relación entre la función de densidad y\nla función de distribución. Como se puede apreciar, el área debajo de la curva\nde la función de densidad \\(f(t)\\) se corresponde con las distintas probabilidades\nde que la variable aleatoria tome valores en los intervalos que encierran dicha\nárea.\nFigura 5.7: Relación entre las funciones de densidad y de probabilidad\nPara que una función \\(f(x)\\) sea función de densidad, tiene\nque cumplir las siguientes condiciones:\\(f(x)\\geq 0\\).\\(f(x)\\geq 0\\).\\(\\int_{-\\infty}^\\infty f(x)dx = 1\\).\\(\\int_{-\\infty}^\\infty f(x)dx = 1\\).La primera condición impone la condición evidente de que la masa de probabilidad\nsea positiva. La segunda condición la impone el segundo axioma de la probabilidad\ny las propiedades de la función de distribución, ya que:\\[\\int_{-\\infty}^\\infty f(x)dx = F(\\infty)=P[X\\leq \\infty] = P(\\Omega)=1.\\]\nEsto implica que cualquier función \\(g(x)\\) definida positiva en un determinado intervalo, se\npuede convertir en una función de densidad multiplicándola por una constante\n\\(k\\) calculada como:\\[k=\\frac{1}{\\int_{-\\infty}^\\infty g(x)dx}.\\]Supóngase que la empresa de servicios de nuestro ejemplo quiere hacer una campaña para aplicar entre un 5% y un 25% de descuento sus clientes de forma aleatoria y lineal de forma que haya más descuentos bajos que altos. Entonces la función de densidad para la variable aleatoria \\(X=\\) Descuento aplicado un cliente se puede modelizar mediante una recta con esta forma:\\[\nf(x) = \n\\begin{cases}\nk(25-x) & \\text{si } 5 \\leq x \\leq 25\\\\\n0 & \\text{resto}\n\\end{cases}\n\\]que, para que sea función de densidad, debe cumplir que:\\[\\int_{-\\infty}^\\infty f(x) dx=1,\\]y por tanto:\n\\[k=\\frac{1}{\\int_{5}^{25} (25-x) dx} = 0.005.\\]La figura 5.8 representa esta función de densidad.\nFigura 5.8: Función de densidad del ejemplo de los descuentos\nAunque las integrales que se presentan en este texto son inmediatas, en la práctica\nel uso del software para resolverlas es más rápido y productivo. Se aconseja al lector\nrealizar las comprobaciones por sí mismo.MAXIMAMaxima resuelve integrales de forma simbólica. En el caso de una integral definida\nobtenemos directamente el área bajo la curva de una función. En el ejemplo:1 / integrate(25-x, x, 5, 25);RR puede calcular integrales definidas metiante métodos numéricos. El código continuación\nmuestra la expresión para calcular la integral buscada y el valor de \\(k\\).Sea la variable aleatoria \\(X\\) con función de densidad:\\[f(x)=\n\\begin{cases}\n\\frac{1}{8}x \\quad \\text{si} \\quad 0<x<4\\\\\n0 \\quad \\quad \\text{resto}\n\\end{cases}\\]Comprobar que es función de densidad, y obtener la función de distribución.Para comprobar si es función de densidad, verificamos las dos condiciones:\\(f(x)\\geq 0\\) según está definida (véase la figura 5.9)\\(f(x)\\geq 0\\) según está definida (véase la figura 5.9)Integral en todo \\(\\mathbb{R}\\):Integral en todo \\(\\mathbb{R}\\):\\[\\int_{-\\infty}^\\infty f(x)dx = \\int_{0}^4 \\frac{1}{8}x dx = \\left [ \\frac{x^2}{16} \\right]_{0}^4=1.\\]Para calcular la función de distribución, tenemos en cuenta que:\\[F(x)=\\int_{-\\infty}^x f(t) dt =P[X\\leq x].\\]Como lo que queremos obtener es una función, y un número, el límite\nsuperior de la integral definida es variable (x). la función \\(f\\) la ponemos\nen función de \\(t\\) simplemente para utilizar el mismo símbolo \\(x\\) y evitar\nconfusiones.Si tenemos una función de densidad definida por trozos, tendremos que ir acumulando trozos.\nRecorremos de menor mayor los intervalos de \\(\\mathbb{R}\\) realizando la integral\ndefinida completa para los intervalos anteriores al que\nestamos considerando. Entonces, para nuestra función:Si \\(x\\leq 0\\):\\[F(x)=\\int_{-\\infty}^x 0 dt =0.\\]Si \\(0 <x < 4\\):\n\\[F(x)=\\int_{-\\infty}^0 0 dt + \\int_{0}^x \\frac{1}{8}t dt = \\left [ \\frac{t^2}{16} \\right]_{0}^x = \\frac{x^2}{16}.\\]Si \\(0 <x < 4\\):\\[F(x)=\\int_{-\\infty}^0 0 dt + \\int_{0}^x \\frac{1}{8}t dt = \\left [ \\frac{t^2}{16} \\right]_{0}^x = \\frac{x^2}{16}.\\]Si \\(x>4\\):Si \\(x>4\\):\\[F(x)=\\int_{-\\infty}^0 0 dt + \\int_{0}^4 \\frac{1}{8}t dt + \\int_{4}^x 0 dt= \\left [ \\frac{t^2}{16} \\right]_{0}^4 = 1.\\]Expresamos por tanto la función de distribución, cuya representación aparece en la figura 5.10, de la siguiente forma para todos sus intervalos:MAXIMALa integral definida para comprobar que vale uno sería:integrate((1/8)*x, x, 0, 4);Podríamos obtener la expresión de la función de distribución en el intervalo en que está definida\ncon la siguiente expresión:integrate((1/8)*t, t, 0, x);REl código continuación realiza la comprobación de que la integral vale 1. R puede\nhacer cálculo simbólico para obtener una expresión de la función de distribución. obstante,\nse puede crear una función que obtenga valores de la función de distribución para utilizar\nposteriormente, o representarla gráficamente.\nFigura 5.9: Representación de la función de densidad del ejemplo\n\nFigura 5.10: Representación de la función de distribución del ejemplo\nNótese que la función de densidad es una probabilidad, y, por tanto,\npodría tomar valores mayores que 1. Por otra parte, la función de densidad\npuede ser discontinua.Es fácil comprobar que:\\[P[<X\\leq b]=\\int_a^b f(x)dx.\\]Lo que nos proporciona una forma de calcular probabilidades de una variable\naleatoria continua mediante la función de densidad (aunque conozcamos la\nfunción de distribución). Las probabilidades son, por lo tanto, equivalentes\nal área bajo la curva de la función de densidad, que, esta vez\nsí, tiene que ser menor o igual que 1. Utilizando las propiedades\nde la probabilidad, podemos calcular probabilidades de cualquier intervalo\nutilizando tanto la función de densidad como la función de distribución, tal\ny como se resume en la figura 5.11.\nFigura 5.11: Cálculo de probabilidades de una variable continua\nPara mejorar la comprensión de la función de densidad, cuya importancia\nes vital en el cálculo de probabilidades, vamos relacionarla con otros conceptos\nya conocidos por el lector. En primer lugar, en el tránsito de variables discretas\ncontinuas, hemos pasado del sencillo cálculo del sumatorio \\((\\sum)\\) al\nintimidante cálculo con integrales \\((\\int)\\). Sin embargo, una integral es\nen realidad una suma infinita de áreas bajo la curva cuando tomamos intervalos\ncada vez más pequeños.\nEn segundo lugar, recordemos la definición de\nprobabilidad como frecuencia relativa en el límite. Entonces decíamos,\nque si pudiéramos repetir un experimento un número grande de veces,\nla frecuencia relativa de ocurrencia de un suceso tendía la probabilidad de\nese suceso. En el marco de las variables aleatorias, tendríamos un número\ngrande de realizaciones de la variable aleatoria, es decir, de números reales.\nComo sabemos por la estadística descriptiva, estos valores los podemos\nagrupar en intervalos y contar las frecuencias de los valores dentro de cada\nintervalo, representándolos en un histograma. Pues bien, si tenemos\nmuchos números, y hacemos la amplitud\nde los intervalos muy pequeños, entonces el histograma de los datos\nse parece cada vez más la función de densidad de la variable aleatoria\nque describe el experimento62, recuérdese la Figura 5.1. Además, el área de las barras del histograma\nrepresenta también las probabilidades de los intervalos que podamos formar.\nLa figura 5.12 muestra esta relación entre frecuencias\ny función de densidad en un determinado experimento.\nFigura 5.12: Frecuencias, histograma y función de densidad\nUna última consideración en cuanto las variables aleatorias continuas es\nla irrelevancia práctica de incluir o el símbolo igual en las desigualdades.\nSi bien en las variables aleatorias discretas sí habrá una diferencia numérica\nque puede ser importante en aplicaciones prácticas, en las variables aleatorias\ncontinuas la utilización del símbolo \\(\\leq\\) o el símbolo \\(<\\), o sus contrarios\n\\(\\geq\\) y \\(>\\) es irrelevante para el cálculo. Efectivamente, como la probabilidad en un punto, \\(P[X=x]=0\\),\nentonces se cumple para variables continuas que:\\[P[X\\leq x] = P[X<x]; \\quad P[X\\geq x] = P[X>x].\\]Pero mucho cuidado porque esto pasa con las variables aleatorias discretas.\nAdemás, siempre es preferible utilizar los símbolos de forma adecuada aunque\ntenga consecuencias prácticas.Sea la variable aleatoria del ejemplo anterior, con las siguientes\nfunciones de densidad y de distribución:\\[f(x)=\n\\begin{cases}\n\\frac{1}{8}x \\quad \\text{si} \\quad 0<x<4\\\\\n0 \\quad \\quad \\text{resto}\n\\end{cases}\\;;\nF(x)=\n\\begin{cases}\n0 \\;\\; \\quad \\text{si} \\quad x \\leq 0\\\\\n\\frac{x^2}{16} \\quad \\text{si} \\quad 0 <x <4\\\\\n1 \\;\\; \\quad \\text{si} \\quad x \\geq 4\\\\\n\\end{cases}\n\\]Calcular:\\[P[1<X<2].\\]Lo podemos hacer través de la función de densidad:\\[P[1<X<2]=\\int_{1}^2 f(x)dx = \\int_{1}^2 \\frac{1}{8}x dx = \\left [ \\frac{x^2}{16} \\right]_{1}^2=\\frac{2}{8}-\\frac{1}{16} = \\frac{3}{16},\\]y también través de la función de distribución:MAXIMALa probabilidad pedida se calcularía simplemente:integrate((1/8)*x, x, 1, 2);REl código continuación calcula la probabilidad pedida.El tiempo de duración (en minutos) de la visita de un potencial usuario de un servicio tras seguir el link de una oferta es una variable aleatoria \\(X\\) que sigue una distribución de probabilidad según la siguiente función de densidad:\\[f(x) = \n\\begin{cases}\n2e^{-2x} & \\text{si } x > 0\\\\\n0 & \\text{si } x\\leq 0\n\\end{cases}\\]La representación gráfica de esta función aparece en la figura 5.13.\nPodemos comprobar que es una función de densidad verificando que cumple los dos\nrequisitos. Es una función exponencial multiplicada por un número positivo, por\ntanto es siempre positiva. Comprobemos el área debajo de la curva para todo su\ndominio:\\[\\int_{-\\infty}^\\infty f(x) dx = 1 \\iff \\int_0^\\infty 2e^{-2x} dx=\\left[-e^{-2x}\\right ]_0^\\infty=1\\]La función de distribución de esta variable aleatoria será:\\[F(x) = \\int_{-\\infty}^x f(x) dx = \\int_0^x 2e^{-2t} dt=\\left[-e^{-2t}\\right ]_0^x=1-e^{-2x},\\]y su representación gráfica es la que se muestra en la figura 5.14.¿Qué porcentaje de visitantes abandonarán probablemente la página antes de 10 segundos?\n(nótese que 10 segundos = 10/60 minutos).Dado que tenemos la función de distribución,\nes más sencillo obtenerlo través de esta que resolviendo la integral:\\[P[X<10/60] = F(10/60) = 1-e^{-2\\cdot 10/60}= 0.2835.\\]Como la pregunta se hace en términos de porcentaje, la respuesta sería\naproximadamente un 28.35% de los visitantes.MAXIMALas siguientes expresiones obtienen en Maxima los resultados del ejemplo.integrate(2*exp(-2*x), x, 0, inf);integrate(2*exp(-2*t),t, 0, x);integrate(2*exp(-2*x), x, 0, 10/60);REn el siguiente código de R se realizan los cálculos explicados en el ejemplo.\nFigura 5.13: Representación de la función de densidad del ejemplo ilustrativo\n\nFigura 5.14: Representación de la función de distribución del ejemplo ilustrativo\n","code":"\nintegral <- integrate(f = function(x) { 25 - x }, \n                      lower = 5, \n                      upper = 25)\nintegral\n#> 200 with absolute error < 2.2e-12\nk <- 1/integral$value\nk\n#> [1] 0.005\nintegrate(f = function(x) { (1/8)*x }, \n                      lower = 0, \n                      upper = 4)\n#> 1 with absolute error < 1.1e-14\nFx <-  function(x) {\n  integrate(f = function(t) { (1/8)*t }, \n                      lower = 0, \n                      upper = x)\n}\nFx(2)\n#> 0.25 with absolute error < 2.8e-15\nintegrate(f = function(x) { (1/8)*x }, \n                      lower = 1, \n                      upper = 2)\n#> 0.1875 with absolute error < 2.1e-15\nintegrate(function(x) 2*exp(-2*x), 0, Inf)\n#> 1 with absolute error < 5e-07\nintegrate(function(x) 2*exp(-2*x), 0, 10/60)\n#> 0.2834687 with absolute error < 3.1e-15"},{"path":"vauni.html","id":"características-de-una-variable-aleatoria","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5 Características de una variable aleatoria","text":"Al igual que con los datos concretos de una muestra podemos calcular estadísticos que resumen la información, las variables\naleatorias constan de parámetros de centralización, posición y forma que caracterizan la variable aleatoria través de su distribución de probabilidad.\ntravés de los posibles valores de una variable aleatoria y sus probabilidades podemos\ndefinir estas características. Las más importantes son la esperanza (media) y la varianza.\nUna vez más, téngase en cuenta que estos parámetros de la variable aleatoria\nson valores teóricos de la variable aleatoria, generalmente referidas una\npoblación de la cual tenemos sólo información parcial través de una muestra (recuérdese la figura 5.1).","code":""},{"path":"vauni.html","id":"esperanza-matemática","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.1 Esperanza Matemática","text":"La Esperanza matemática se define sobre una función \\(g(x)\\) de una variable aleatoria \\(X\\) como:\\[E[g(X)]=\\int_{\\mathbb {R}} g(x) dF(x),\\]que en el caso discreto resulta en:\\[E[g(X)]=\\sum\\limits_i g(x_i)\\cdot p(x_i),\\]y en el caso continuo:\\[E[g(x)] = \\int_{-\\infty}^\\infty g(x) \\cdot f(x)dx.\\]Así, la esperanza va ser un número, ya sea calculado como suma de términos\nen el caso discreto, o como suma infinita través de la integral definida.\nEl uso de integrales debe intimidar, ya que se trata más que de áreas debajo\nde una curva, cuyo cálculo con el software apropiado es muy sencillo.\nSe puede ver como la suma de los valores de la variable aleatoria (o una función\nde ella) multiplicado por sus probabilidades. El resultado va ser el valor esperado,\nque se corresponde con la media de la distribución, su valor central.\nEl uso de la palabra esperanza en este ámbito tiene su origen, cómo , en los juegos de azar. Así, se hablaba de la esperanza de ganar en el juego (y la ganancia que se esperaba tener era el resultado), y también del temor, cuando la esperanza era negativa.\nLa esperanza se define como hemos visto sobre una función cualquiera de la variable\naleatoria \\(g(x)\\). Si \\(g(x)=x\\), entonces tendremos la\nesperanza de la propia variable aleatoria. Se cumplen las\nsiguientes propiedades para la esperanza matemática:La esperanza de una constante es esa misma constante:\\[c \\;\\;\\text{constante} \\implies E[c] = c.\\]Sea una variable aleatoria que es suma de \\(n\\) variables aleatorias. Entonces su esperanza es la suma de las esperanzas de dichas variables aleatorias:\\[E\\left [ \\sum\\limits_{=1}^n X_i\\right ] = \\sum\\limits_{=1}^n E[X_i].\\]Sea una variable aleatoria que es producto de \\(n\\) variables aleatorias. Entonces su esperanza es el producto de las esperanzas de dichas variables aleatorias si y solo si dichas variables aleatorias son independientes:\\[E\\left [ \\prod\\limits_{=1}^n X_i\\right ] = \\prod\\limits_{=1}^n E[X_i] \\iff X_i \\;\\; \\text{independientes}.\\]La esperanza de una variable aleatoria es su valor central:\\[E[X] = \\mu \\implies E[X-\\mu]=0.\\]la variable aleatoria transformada \\(X-\\mu\\) se le denomina variable aleatoria centrada, y su media es cero.Sea una variable aleatoria que es una transformación lineal de otra variable aleatoria. Entonces su esperanza es la misma transformación lineal de la esperanza de la variable original:\\[\\begin{equation}\n, b \\;\\;\\text{constantes} \\implies E[+ bX] = + [X].\n\\tag{5.2}\n\\end{equation}\\]Si la integral existe, la variable aleatoria tiene esperanza.\nEsto puede pasar cuando existe la integral que la define63.","code":""},{"path":"vauni.html","id":"momentos-de-variables-aleatorias-unidimensionales","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.2 Momentos de variables aleatorias unidimensionales","text":"Hemos visto que la esperanza se define sobre una determinada función \\(g(x)\\) de la variable aleatoria.\nLos momentos se definen sobre unas funciones muy específicas y que nos van permitir\ncaracterizar las variables aleatorias.\nSe define el momento de orden \\(r\\) respecto al origen de una variable aleatoria \\(X\\), \\(\\alpha_r\\), como:\\[\\alpha_r = E[X^r].\\]El momento de orden 1 respecto del origen es la media de la variable aleatoria, \\(\\mu\\):\\[\\alpha_1=\\boxed{E[X]=\\mu}.\\]El momento de orden \\(r\\) respecto de la media \\(\\mu\\) de una variable aleatoria, \\(\\mu_r\\) se define como:\\[\\mu_r = E\\left [(X - \\mu)^r\\right ].\\]\nNótese que, en realidad, \\(X-\\mu\\) es una variable aleatoria centrada cuya esperanza\nes igual cero por las propiedades de la esperanza enumeradas anteriormente, y por tanto:\\[X-\\mu \\implies \\mu_1=E\\left [X - \\mu\\right ] = 0.\\]En el caso discreto, estos momentos se calcularán respectivamente como:\\[\\alpha_r = \\sum\\limits_{} x_i^r p(x_i),\\]y\\[\\mu_r = \\sum\\limits_{} (x_i- \\mu)^r p(x_i).\\]En el caso continuo, se calcularán respectivamente como:\\[\\alpha_r = \\int_{-\\infty}^\\infty x^r f(x) dx,\\]y\\[\\mu_r = \\int_{-\\infty}^\\infty (x-\\mu)^r f(x) dx.\\]Se verifica la siguiente relación entre los momentos respecto del origen y los momentos\nrespecto de la media que nos ayudarán, como veremos posteriormente, simplificar\nlos cálculos:\\[\\begin{equation}\n\\mu_r = \\alpha_r - \\binom{r}{1}\\alpha_1 \\alpha_{r-1} + \\binom{r}{2}\\alpha_1^2 \\alpha_{r-2} + \\cdots + (-1)^r\\alpha_1^r = \\\\\n=\\sum\\limits_{k=0}^r (-1)^k\\binom{r}{k}\\mu^k \\alpha_{r-k}.\n\\tag{5.3}\n\\end{equation}\\]También se verifica que:Si existe \\(\\alpha_r\\), entonces existen también todos los \\(\\alpha_s\\) tales que \\(s<r\\).Si existe \\(\\mu_r\\), entonces existen también todos los \\(\\mu_s\\) tales que \\(s<r\\).En resumen, podemos calcular momentos respecto de la media (que requieren cálculos más costosos) través de momentos respecto del origen (cuyos cálculos son más sencillos).\nY si existe un momento, todos los de orden inferior también existen.","code":""},{"path":"vauni.html","id":"medidas-de-centralización-de-una-variable-aleatoria","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.3 Medidas de centralización de una variable aleatoria","text":"Ya hemos visto que la esperanza matemática de una variable aleatoria se corresponde con su valor central, al que denominaremos media, y es el parámetro de centralización de la variable aleatoria.\nEs importante confundir este valor medio o esperado de la variable aleatoria, que es teórico, referido una población, con la media de unos datos concretos, que es empírica, calculada para una muestra.\\[\\mu = \\alpha_1= E[X],\\]cuyo cálculo para variables discretas es el siguiente:\\[\\mu = \\boxed{E[X] =  \\sum\\limits_{} x_i p(x_i)},\\]y para variables continuas:\\[\\mu = \\boxed{E[X] = \\int_{-\\infty}^\\infty x f(x) dx},\\]La mediana es otra medida de centralización, y es el valor de la variable aleatoria que divide la probabilidad del espacio muestral en dos mitades. Por tanto, será el primer\nvalor de la variable aleatoria para el cual la función de distribución vale \\(0.5\\):\\[\\mathit{}=\\inf x : F(x)\\leq 0.5.\\]En variables discretas menudo la mediana se puede obtener simplemente de\nla tabla de valores de \\(F(x)\\). Un método más general consiste en obtener la\nfunción inversa de la función de distribución, \\(F^{-1}(x)\\), que estará en función de la probabilidad\nacumulada, y sustituir la probabilidad por \\(0.5\\):\\[F(x) = p \\iff x = F^{-1}(p) \\implies = F^{-1}(0.5).\\]\nCuando es posible despejar la \\(x\\) hay que recurrir métodos\nnuméricos para obtener la inversa de la función de distribución.\nLa figura 5.15 muestra gráficamente la mediana\ncomo inversa de la función de distribución en \\(F(x) = 0.5\\).\nFigura 5.15: La mediana portir de la inversa de la función de distribución\nPor último, la moda de una variable aleatoria es el valor donde la función de probabilidad o la función de densidad tienen su máximo. La moda puede ser única, y en particular para variables continuas se suele hablar de distribuciones bimodales o\nmultimodales cuando tienen más de un máximo local (aunque solo uno de ellos sea\nel máximo absoluto). En la figura 5.16 se representan las tres medidas\nde una determinada variable aleatoria con una determinada función de densidad. Nótese que en una distribución de probabilidad\nasimétrica, como es la que se representa, la media se desplaza hacia la cola\nmás larga.\nFigura 5.16: Medidas de centralización de una variable aleatoria\nLa media de la variable aleatoria número de caras del experimento\ndescrito más arriba y consistente en lanzar una moneda tres veces, es la siguiente:\\[\\mu=E[X]=0\\cdot \\frac{1}{8}+1\\cdot \\frac{3}{8}+2\\cdot \\frac{3}{8}+3\\cdot \\frac{1}{8}=1.5.\\]Para obtener la mediana, miramos en la función de distribución\nel primer valor para el que \\(F(x)\\ge 0.5\\), y entonces la mediana es 1.\nLa moda es el valor más frecuente, mirando en la función de probabilidad vemos\nque los valores 1 y 2 tienen la frecuencia más alta. Como vemos, la moda puede\nser única (sí lo son siempre la media y la mediana.)HOJA DE CÁLCULODisponemos los posibles valores de la variable \\(x_i\\) en la primera columna,\nlas probabilidades en la segunda columna. En la tercera columna calculamos \\(x_i\\cdot p_i\\),\ny sumamos los valores.RLa media de la variable aleatoria definida por la función de densidad:\\[f(x)=\n\\begin{cases}\n\\frac{1}{8}x \\quad \\text{si} \\quad 0<x<4\\\\\n0 \\quad \\quad \\text{resto}\n\\end{cases}\\]Se calcula de la siguiente forma:\\[\\mu=E[X]=\\int_{-\\infty}^\\infty xf(x)dx = \\int_{0}^4 \\frac{1}{8}x^2 dx = \\left [ \\frac{x^3}{24} \\right]_{0}^4=\\frac{64}{24}=\\frac{8}{3}\\simeq 2.67.\\]Para obtener la mediana, tendríamos que obtener la inversa de la función\nde distribución, \\(F^{-1}(p)\\), y sustituir \\(p\\) por \\(0.5\\). En este caso es sencillo,\nbasta con despejar \\(x\\) de la función de densidad (nos centramos solo en el tramo\ndonde la densidad es mayor de cero):\\[F(x) = p = \\frac{x^2}{16} \\iff x = F^{-1}(p) = +\\sqrt{16p}\\]\nTomamos solo la raíz positiva puesto que sabemos que la variable está entre 0 y 4.\nEntonces la mediana de esta variable aleatoria es:\\[F^{-1}(0.5) = +\\sqrt{16\\cdot 0.5} = 2\\sqrt{2} \\approx 2.8284.\\]En cuanto la moda sería 4, ya que es el valor donde la función de densidad\nes máximo, al ser una recta de pendiente positiva entre 0 y 4 (véase la figura 5.9).MAXIMALa siguiente expresión devuelve el valor la integral definida con el resultado de la esperanza:integrate(x*(1/8)*x, x, 0, 4);REl código continuación obtiene la esperanza de la variable aleatoria.\nVamos calcular las medias de las variables aleatorias de los ejemplos de sujetos en estudio. Para la variable aleatoria discreta:\n\n\\(X:\\) Número de mensajes remitidas por correo electrónico en un año los sujetos,\n\nla media sería:\n\n\\[\\mu=E[X]=\\sum\\limits_{=1}^3 x_i p_i = 20\\cdot \\frac{36}{52} + 36 \\cdot \\frac{12}{52} + 60\\cdot\\frac{4}{52}\\simeq26.7692.\\] Para la variable aleatoria continua:\n\n\\(X:\\) Tiempo de duración de la visita la web de un sujeto,\n\nla media sería:\n\n\\[\\mu=E[X]=\\int_{-\\infty}^\\infty x f(x) dx = \\int_{0}^\\infty x \\cdot 2 e^{-2x}dx = 0.5,\\]\n\nresolviendo la integral por partes y aplicando la regla de Barrow.\nHOJA DE CÁLCULO (variable discreta)Disponemos los posibles valores de la variable \\(x_i\\) en la primera columna,\nlas probabilidades en la segunda columna. En la tercera columna calculamos \\(x_i\\cdot p_i\\),\ny sumamos los valores.MAXIMALa esperanza de la variable continua la podemos obtener con la siguiente\nexpresión:integrate(x*2*exp(-2*x), x, 0, inf);R","code":"\nx_i <- 0:3\np_i <- c(1/8, 3/8, 3/8, 1/8)\nEx <- sum(x_i*p_i)\nEx\n#> [1] 1.5\nintegrate(function(x) x*(1/8)*x, 0, 4)\n#> 2.666667 with absolute error < 3e-14\nx_i <- c(20, 36, 60)\np_i <- c(36/52, 12/52, 4/52)\nEx <- sum(x_i*p_i)\nEx\n#> [1] 26.76923\nintegrate(function(x) x*2*exp(-2*x), 0, Inf)\n#> 0.5 with absolute error < 8.6e-06"},{"path":"vauni.html","id":"medidas-de-dispersión-de-una-variable-aleatoria","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.4 Medidas de dispersión de una variable aleatoria","text":"La varianza es el parámetro de dispersión de la variable aleatoria. Se define como el momento de orden 2 respecto de la media, y se representa por \\(\\sigma^2\\).\\[V[X] = \\sigma^2 = \\mu_2 = E\\left [(X- \\mu)^2 \\right ],\\]que para variables discretas se calcula como:\\[\\sigma^2 = V[X] =  \\sum\\limits_{} (x_i- \\mu)^2 p(x_i),\\]y para variables continuas como:\\[\\sigma^2 = V[X] = \\int_{-\\infty}^\\infty (x-\\mu)^2 f(x) dx.\\]Aplicando la relación entre los momentos respecto del origen y los momentos respecto de la media\nde la ecuación (5.3) resulta que:\\[\\mu_2 = \\alpha_2-\\alpha_1^2,\\]y podemos calcular la varianza con la siguiente expresión abreviada:\\[\\boxed{\\sigma^2= E[X^2] - E[X]^2},\\]\ndonde\\[\\alpha_2=E[X^2]= \\sum\\limits_{} x_i^2 p(x_i)\\]\npara variables discretas y:\\[\\alpha_2=E[X^2]= \\int_{-\\infty}^\\infty x^2 f(x) dx\\]\npara variables continuas.La varianza de una variable aleatoria cumple además las siguientes propiedades:La varianza de una constante es nula:\\[V[c] = 0.\\]La varianza de una variable aleatoria es siempre positiva:\\[V[X] \\geq 0.\\]Sea una variable aleatoria que es una transformación lineal de otra variable aleatoria. Entonces su varianza es:\\[, b \\;\\;\\text{constantes} \\implies \\boxed{V[+ bX] = b^2 V[X]}.\\]Nótese la diferencia con la esperanza de la transformación lineal vista en la ecuación (5.2).La desviación típica de la variable aleatoria es la raíz cuadrada positiva de la varianza. La desviación típica viene expresada en las mismas unidades que la variable aleatoria, mientras que la varianza está expresada en las unidades de la variable aleatoria al cuadrado.\\[\\sigma = +\\sqrt{V[X]}.\\]Una caracteristica adimensional de la variabilidad es el coeficiente de variación, que es el cociente entre la desviación típica y la media de la variable aleatoria. Si la media fuera negativa, se suele expresar en valor absoluto:\\[\\mathit{CV}= \\frac{\\sigma}{\\mu}.\\]Para calcular la varianza de la variable aleatoria número de caras del experimento consistente en lanzar una moneda tres veces, primero calculamos el momento de orden 2 respecto del origen:\\[\\alpha_2=E[X^2] = 0^2\\cdot \\frac{1}{8}+1^2\\cdot \\frac{3}{8}+2^2\\cdot \\frac{3}{8}+3^2\\cdot \\frac{1}{8}=3.\\]Como ya sabíamos que la media era \\(\\mu=1.5\\), entonces la varianza es:\\[\\sigma^2=\\alpha_2 -  \\mu^2=3-\\left(\\frac{3}{2}\\right )^2=\\frac{3}{4}=0.75.\\]La desviación típica y el coeficiente de variación serán:\n\\[\\sigma = \\sqrt{3/4} \\simeq 0.8660;\\; CV = \\frac{\\sigma}{\\mu} \\simeq 0.5774 \\]Para calcular la media de la variable aleatoria definida por la función de densidad:\\[f(x)=\n\\begin{cases}\n\\frac{1}{8}x \\quad \\text{si} \\quad 0<x<4\\\\\n0 \\quad \\quad \\text{resto}\n\\end{cases}\\]calculamos también en primer lugar el momento de orden 2 respecto del origen,\nen este caso través de la integral:\\[\\alpha_2=E[X^2]=\\int_{-\\infty}^\\infty x^2f(x)dx = \\int_{0}^4 \\frac{1}{8}x^3 dx = \\left [ \\frac{x^4}{32} \\right]_{0}^4=\\frac{256}{32}=8.\\]Como la media era \\(\\mu=\\frac{8}{3}\\), la varianza es:\\[\\sigma^2=\\alpha_2 -  \\mu^2=8-\\left(\\frac{8}{3}\\right )^2=\\frac{8}{9}\\simeq 0.8889.\\]HOJA DE CÁLCULO (variable discreta)Si tenemos dispuestos los valores y probabilidades como se indicó más arriba,\npodemos añadir dos columnas con el cálculo de \\(x_i^2\\) y \\(x_i^2 \\cdot p_i\\) en cada fila, sumar esta última para obtener \\(\\alpha_2\\) y continuación restarle\nla media calculada anteriormente elevada al cuadrado, para obtener la varianza.MAXIMAPara la variable continua obtenemos \\(\\alpha_2\\) con la siguiente expresión,\ny después podemos hacer operaciones para calcular todas las características:integrate(x^2*(1/8)*x, x, 0, 4);R\nVamos calcular las varianzas de las variables aleatorias de los ejemplos de sujetos en estudio. Para la variable aleatoria discreta:\n\n\\(X:\\) Número de mensajes remitidos por correo electrónico en un año los sujetos,\n\nel momento de orden dos con respecto al origen sería:\n\n\\[\\alpha_2=E[X^2]=\\sum\\limits_{=1}^3 x_i^2 p_i = 20^2\\cdot \\frac{36}{52} + 36^2 \\cdot \\frac{12}{52} + 60^2\\cdot\\frac{4}{52}\\simeq 852.9231.\\]\n\nY entonces, la varianza es:\n\n\\[\\sigma^2=\\alpha_2 -  \\mu^2=852.9231-(26.7692)^2=136.333.\\]\n\nPara la variable aleatoria continua:\n\n\\(X:\\) Tiempo de duración de la visita la web de un sujeto,\n\nel momento de orden dos sería:\n\n\\[\\alpha_2=E[X^2]=\\int_{-\\infty}^\\infty x^2 f(x) dx = \\int_{0}^\\infty x^2 \\cdot 2 e^{-2x}dx = 0.5,\\]\n\ny entonces la varianza es:\n\n\\[\\sigma^2=\\alpha_2 -  \\mu^2=0.5-(0.5)^2=0.25.\\]\nHOJA DE CÁLCULO (discreta)Procederíamos igual que en el ejemplo anterior, calculando en columnas,\nsumando totales y finalmente aplicando la fórmula.MAXIMALa siguiente expresión calcularía \\(\\alpha_2\\), y partir de ahí se\naplican las fórmulas para obtener los distintos parámetros.integrate(x^2*2*exp(-2*x), x, 0, inf);RA partir de la variable aleatoria anterior:\\(X:\\) Tiempo de duración de la visita la web de un sujeto,supongamos que esta visita se produce siempre después de haber visto un\nanuncio de 10 segundos, y queremos estudiar la variable:\\(Y:\\) Tiempo total de conexión con el servidor en segundos.Esta nueva variable aleatoria se puede expresar como:\\[Y = 10 + 60 \\cdot X\\]y tendrá una determinada distribución de\nprobabilidad cuya determinación se trata en este texto. En cualquier caso,\ntravés de las propiedades de la esperanza y la varianza, sí podemos\ncalcular el valor de estas características para la nueva distribución.\nAsí:\\[E[Y] = 10 + 60 \\cdot E[X] = 10 + 60 \\cdot 0.5 = 40,\\]\n\\[V[Y] =60^2 \\cdot V[X] = 60^2 \\cdot 0.5 = 1800.\\]","code":"\nalpha_2 <- integrate(function(x) x^2*(1/8)*x, 0, 4)$value\nalpha_1 <- integrate(function(x) x*(1/8)*x, 0, 4)$value\nvarianza <- alpha_2 - alpha_1^2; varianza\n#> [1] 0.8888889\ndesv.tip <- sqrt(varianza); desv.tip\n#> [1] 0.942809\ncv <- desv.tip/alpha_1; cv\n#> [1] 0.3535534\nalpha_2 <- integrate(function(x) x^2*2*exp(-2*x), 0, Inf)$value\nalpha_1 <- integrate(function(x) x*2*exp(-2*x), 0, Inf)$value\nvarianza <- alpha_2 - alpha_1^2; varianza\n#> [1] 0.25"},{"path":"vauni.html","id":"variable-aleatoria-estandarizada","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.5 Variable aleatoria estandarizada","text":"La última de las propiedades de la varianza enumeradas anteriormente, es decir:\\[, b \\;\\;\\text{constantes}, Y=+bX \\implies V[Y] = b^2 V[X],\\]nos va permitir escalar cualquier variable aleatoria transformándola en otra que tenga desviación típica igual uno. Efectivamente, si en la transformación lineal anterior hacemos \\(b=\\frac{1}{\\sigma}\\):\\[V[Y]=\\left(\\frac{1}{\\sigma}\\right)^2 \\cdot\\sigma^2=1.\\]Si aplicamos esta transformación una variable aleatoria centrada \\(X-\\mu\\), entonces tenemos una variable aleatoria estandarizada con media cero y desviación típica 1 y que normalmente se denota por \\(Z\\):\\[Z=\\frac{X-\\mu}{\\sigma}\\implies \\mu_Z=0;\\; \\sigma_Z=1.\\]Utilizaremos esta transformación para realizar cálculo de probabilidades del modelo de distribución normal en el capítulo 7. Además, tiene mucho interés en Estadística inferencial y en técnicas multivariantes que se tratan en este texto.","code":""},{"path":"vauni.html","id":"otros-parámetros","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.6 Otros parámetros","text":"Al igual que se definió la mediana, podemos definir cualquier cuantil \\(X_p\\) para una\nprobabilidad dada:\\[X_{p}= \\inf x: F(x)\\leq p\\]Por ejemplo, los cuantiles \\(0.25\\) y \\(0.75\\) serían los valores\n\\(X_{0.25}\\) y \\(X_{0.75}\\) que dejan por debajo una probabilidad de \\(0.25\\) y \\(0.75\\) respectivamente64.\nEl método más general para calcular cualquier cuantil consiste en obtener\nla inversa de la función de distribución \\(x = F^{-1}(p)\\) y dar valores \n\\(p\\) (véase el ejemplo de la mediana más arriba).\nLa figura 5.17 muestra la representación del cuantil 0.75\nen relación con una determinada función de densidad.También se pueden calcular partir de los momentos otros parámetros como los coeficientes de asimetría y de curtosis de una variable aleatoria, que se tratan en este texto.\nFigura 5.17: Cuantiles de una variable aleatoria\n","code":""},{"path":"vauni.html","id":"desigualdad-de-chebyshev","chapter":"Capítulo 5 Variable aleatoria univariante","heading":"5.5.7 Desigualdad de Chebyshev","text":"En ocasiones, es posible que conozcamos la media, \\(\\mu\\), y la varianza, \\(\\sigma^2\\), de una variable aleatoria, pero conozcamos nada sobre su distribución de probabilidad. En estos casos, podemos calcular la probabilidad en un intervalo, pero podemos acotar la probabilidad entre dos valores entorno la media conocida. La fórmula general para\nesta acotación es la siguiente:\\[P[|X-\\mu|\\geq k\\sigma] \\leq \\frac{1}{k^2},\\]\nconocida como desigualdad de Chebyshev65 y que nos permite acotar la probabilidad de una variable aleatoria de dos formas:La probabilidad de que la variable aleatoria tome valores más extremos de \\(k\\) desviaciones típicas desde la media es, como mucho, \\(\\frac{1}{k^2}\\), véase la figura 5.18:\\[P[\\mu-k\\sigma \\geq X \\geq \\mu+k\\sigma] \\leq \\frac{1}{k^2}.\\]La probabilidad de que la variable aleatoria tome valores dentro de \\(k\\) desviaciones típicas desde la media es, como poco, \\(1-\\frac{1}{k^2}\\), véase la figura 5.19:\\[P[\\mu-k\\sigma < X < \\mu+k\\sigma] \\geq 1- \\frac{1}{k^2}.\\]\nSi lo que queremos es acotar la probabilidad para un valor concreto \\(x\\), entonces podemos encontrar primero \\(k\\) despejando de \\(\\mu+k\\sigma=x\\) y después aplicar las propiedades de la probabilidad para encontrar una cota.De la desigualdad de Chebyshev se deduce que, por ejemplo, para cualquier variable aleatoria la probabilidad de que esa variable aleatoria tome valores entre su media y dos desviaciones típicas es de, al menos, \\(0.75\\):\\[P[\\mu-2\\sigma < X < \\mu+2\\sigma] \\geq 1- \\frac{1}{2^2}=0.75.\\]\nTambién podemos determinar mediante esta desigualdad, entre qué valores estará, al menos, una determinada probabilidad.\nFigura 5.18: Cota superior externa Desigualdad de Chebyshev\n\nFigura 5.19: Cota inferior interna Desigualdad de Chebyshev\nSe sabe que la media de una variable aleatoria es 9 y su varianza 4. ¿Entre qué dos valores tendremos, al menos, una probabilidad de 0.75?De la propiedad que acabamos de ver, esto se cumple para \\(k=2\\), y por tanto esos valores serán \\(\\mu\\pm 2\\sigma=9\\pm 2 \\cdot \\sqrt{4} = [5, 13].\\)¿Entre qué valores tendremos una probabilidad de, al menos, 0.84?Para contestar esta pregunta, calculamos primero \\(k\\) teniendo en cuenta:\\[P[\\mu-k\\sigma < X < \\mu+k\\sigma] \\geq 1- \\frac{1}{k^2} \\implies 1- \\frac{1}{k^2}=0.84 \\implies k=2.5,\\]\ny calculamos los valores como:\\[\\mu \\pm k\\sigma = 9 \\pm 2.5\\cdot\\sqrt{4} = [4,\\; 14].\\]\nEntre \\(4\\) y \\(14\\) tendremos, al menos, una probabilidad de \\(0.84\\). Y\nla inversa, más allá de estos valores tendremos, como mucho, una\nprobabilidad de \\(0.16\\).¿Cuál sería la probabilidad de que esta variable aleatoria tome valores mayores de 15?podemos contestar exactamente esta pregunta puesto que disponemos de la distribución de probabilidad. Pero sí podemos dar una cota de dicha probabilidad. En este caso tenemos que obtener \\(k\\) sabiendo cuánto vale\n\\(\\mu + k\\sigma\\):\\[\\mu + k\\sigma = 15 \\iff k = \\frac{15-9}{2}=3\\]Entonces:\\[P[9-3\\cdot 2 > X > 9+3\\cdot 2] \\le \\frac{1}{k^2} \\iff P[3 > X > 15] \\le \\frac{1}{9} \\approx 0.1111\\]Nótese que esto significa que:\\[P[X \\le 3] + P[X \\ge 15] \\le 0.1111,\\]","code":""},{"path":"vabi.html","id":"vabi","chapter":"Capítulo 6 Variable aleatoria bivariante","heading":"Capítulo 6 Variable aleatoria bivariante","text":"En preparación.Distribución conjuntaCorrelación y regresión","code":""},{"path":"modelos.html","id":"modelos","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"Capítulo 7 Modelos de distribución de probabilidad","text":"","code":""},{"path":"modelos.html","id":"introducción","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.1 Introducción","text":"En el capítulo 5 vimos que una variable aleatoria unidimensional se puede modelizar por cualquier función de distribución de probabilidad que cumpla los requisitos básicos de la probabilidad así, tenemos infinitas funciones de probabilidad para variables aleatorias discretas, o de densidad para variables aleatorias continuas. Sin embargo, la mayoría de los fenómenos de interés estudiados mediante la probabilidad se ajustan un reducido conjunto de modelos de distribución de probabilidad o familias de distribuciones para los que se han determinado sus características principales, facilitando así el trabajo con variables aleatorias. En este capítulo revisaremos los más importantes para variables aleatorias discretas.El primer paso para identificar el modelo de distribución de probabilidad más adecuado, es describir claramente la variable aleatoria \\(X\\), y de ahí deducir cuál es el modelo adecuado. Para cada modelo, se conoce su función de probabilidad o de densidad que contiene un número muy reducido de parámetros. partir de esta función de probabilidad o de densidad, se deducen sus características, por ejemplo la media y la varianza, que quedan expresadas en función de dichos parámetros. Una vez\nidentificado el modelo de distribución de probabilidad, hay que establecer los parámetros concretos que\ncaracterizan la variable aleatoria concreta de interés. En este libro se asumen como\nconocidos (o deducibles fácilmente de la descripción del problema), aunque\nen aplicaciones reales se deberán estimar partir de muestras representativas de\nla población con técnicas de inferencia estadística, que se tratan en este texto. Una vez\ndeterminados los parámetros, podemos calcular fácilmente las características de la variable\naleatoria con las fórmulas dadas, así como realizar cálculo de probabilidades utilizando\ntodo lo aprendido hasta ahora.Para indicar que una variable aleatoria \\(X\\) sigue una determinada distribución de probabilidad, utilizamos\nla siguiente notación:\\[X \\sim \\mathcal{D}\\mathit{istr}(\\boldsymbol{\\theta}),\\]donde \\(\\mathcal{D}\\mathit{istr}\\) identifica el modelo de distribución de probabilidad, y \\(\\boldsymbol{\\theta}\\) es el vector\nde parámetros con los que queda totalmente definida la distribución de probabilidad de la variable\naleatoria \\(X\\) según ese modelo de distribución. Tanto para los modelos de distribución de probababilidad discretos\nde este capítulo, como en los continuos del siguiente, se proporciona\nla función de probabilidad o de densidad de los mismos, así como la esperanza y la varianza que se\ndeduce de las mismas (aunque se incluye dicha deducción). El resto\nde características de cada modelo se puede obtener igualmente partir de su distribución de probabilidad.\nTampoco se incluyen las demostraciones de que, obviamente, las funciones de densidad y de probabilidad\nde cada distribución cumplen las propiedades para ser una Ley de probabilidad.","code":""},{"path":"modelos.html","id":"modelosdisc","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2 Modelos de distribución de probabilidad discretos","text":"","code":""},{"path":"modelos.html","id":"distribución-de-bernoulli","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2.1 Distribución de Bernoulli","text":"Las distribuciones de probabilidad discretas se basan de una forma u otra en procesos de Bernoulli.\nUn proceso de Bernoulli consiste en realizar un experimento que tiene dos resultados posibles.\nuno le llamamos éxito y al otro le llamamos fracaso, y conocemos la probabilidad\ndel suceso éxito, la que llamamos \\(p\\).Dado un proceso de Bernoulli aislado, podemos definir la variable\naleatoria \\(X\\) que toma el valor 1 si el experimento es un éxito, y\n0 si el experimento es un fracaso.\\[X=\n\\begin{cases}\n1 & \\text{ si éxito con probabilidad } p\\\\\n0 & \\text{ si fracaso}\n\\end{cases}\\]\nEntonces las probabilidades para los dos posibles valores de la variable serán:\\[P[X=1]=p;\\quad P[X=0]=1-p,\\]y diremos que \\(X\\) sigue una distribución de Bernoulli de parámetro \\(p\\):\\[X \\sim \\mathit{Ber}(p);\\; 0<p<1.\\]Algunas veces se utiliza la notación \\(q=1-p\\). Una expresión general para la función de probabilidad es la siguiente:\\[P[X = x] = p^x (1-p)^{(1-x)};\\; x =0, 1.\\]Las características de posición y dispersión de esta variable aleatoria se deducen fácilmente:Media: \\(\\mu=E[X] = p\\).Media: \\(\\mu=E[X] = p\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = p \\cdot (1-p)\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = p \\cdot (1-p)\\).La distribución de Bernoulli aparece en los procesos de clasificación de\nobservaciones (individuos, empresas, etc.) en una de dos categorías.En el ejemplo de los potenciales usuarios de nuestro servicio,\ndedujimos en el capítulo 4 que la probabilidad\nde que un cliente tomado al azar contrate el servicio era \\(0.25\\).\nEntonces la variable aleatoria:\\[X: \\begin{cases}0\\quad \\text{ el cliente contrata}\\\\1\\quad \\text{ el cliente contrata}\\end{cases}\\]sigue una distribución de probabilidad de Bernoulli de parámetro \\(p=0.25\\),\nsu media es \\(\\mu=0.25\\), su varianza \\(\\sigma^2=0.1875\\) y su función de\nprobabilidad:\\[P[X=x]=0.25^x\\times 0.75^{1-x}\\]El interés de la distribución de Bernoulli también está en las distribuciones\nde probabilidad derivadas de ella cuando repetimos el proceso bajo\ndistintas condiciones. En los siguientes apartados veremos algunas\nde estas distribuciones que se extienden partir de la de Bernoulli.","code":""},{"path":"modelos.html","id":"distribución-binomial","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2.2 Distribución binomial","text":"Partiendo de un proceso de Bernoulli, consideremos la repetición del experimento\n\\(n\\) veces, y que el resultado de cada experimento es independiente de los\ndemás. Entonces, la variable aleatoria \\(X\\): Número de éxitos en \\(n\\) pruebas independientes de Bernoulli con probabilidad de éxito \\(p\\) cada una de ellas sigue una distribución de probabilidad binomial de parámetros \\(n\\) y \\(p\\):\\[X \\sim \\mathit{Bin}(n;\\;p);\\; n> 0,\\;0<p<1. \\]Nótese que la distribución de Bernoulli es un caso particular de la binomial cuando \\(n=1\\).\\[\\mathit{Ber}(p) = \\mathit{Bin}(1;\\;p).\\]su vez, la\ndistribución binomial es la suma de \\(n\\) variables aleatorias independientes de\nBernoulli:\\[ \\implies \\mathit{Bin}(n;\\;p) = \\sum\\limits_{=1}^n X_i :\\; X_i \\sim \\mathit{Ber}(p)\\; \\forall\\, ,\\]de donde llegamos la siguiente expresión de la función de probabilidad:\\[\\boxed{P[X = x] = \\binom{n}{x}\\cdot p^x \\cdot (1-p)^{(n-x)};\\; x = 0, 1, \\ldots, n},\\]donde:\\[\\binom{n}{x}=\\frac{n!}{x!(n-x)!},\\]conocido como número combinatorio o coeficiente binomial. En el apéndice\nC.2 se encuentran algunas propiedades de este coeficiente, que\nse puede calcular fácilmente en las calculadoras científicas con la tecla nCr.Nótese que en la fórmula de la función de probabilidad de la distribución binomial aparecen\nmuchos conceptos de probabilidad aprendidos hasta ahora. Como son sucesos independientes,\n\\(p^x\\) es la probabilidad de la intersección de \\(x\\) éxitos, y \\((1-p)^{n-x}\\), la\nprobabilidad de la intersección de \\({n-x}\\) fracasos. Entonces \\(p^x \\cdot (1-p)^{(n-x)}\\)\nes la probabilidad de una de las ordenaciones posibles. Como el orden de éxitos y fracasos\nnos da igual, la probabilidad que nos interesa es la probabilidad de la unión de todas\nlas ordenaciones posibles que, como son sucesos disjuntos, se corresponde con la\nsuma de probabilidades. Estas probabilidades son todas iguales, y el número de\nordenaciones posibles es \\(\\binom{n}{x}\\), por eso multiplicamos.La figura 7.1 muestra gráficamente la distribución\nde probabilidad para varios valores de \\(n\\) y \\(p\\).\nFigura 7.1: Representación de la función de probabilidad del modelo binomial\nLas caracterísiticas principales de la distribución binomial se deducen fácilmente\naplicando las fórmulas de la esperanza matemática vistas en el\ncapítulo 5, y son:Media: \\(\\mu=E[X] = n\\cdot p.\\)Media: \\(\\mu=E[X] = n\\cdot p.\\)Varianza: \\(\\sigma^2=\\mathit{V}[X] = n\\cdot p\\cdot (1-p).\\)Varianza: \\(\\sigma^2=\\mathit{V}[X] = n\\cdot p\\cdot (1-p).\\)La distribución binomial, además, cumple la propiedad aditiva, es decir,\nla suma de \\(m\\) variables aleatorias binomiales con idéntico parámetro \\(p\\) y, posiblemente,\ndistintos parámetros \\(n_j, \\, j=1, \\ldots, m\\), es una distribución binomial de\nmodo que:\\[Y=\\sum\\limits_{j=1}^m {X_j},\\, X_j \\sim \\mathit{Bin}(n_j;\\;p) \\implies Y \\sim \\mathit{Bin}\\left ( \\sum\\limits_{j=1}^m n_j;\\; p \\right ).\\]\nEsta propiedad, que iremos viendo en casi todos los modelos, es muy importante\nporque nos permite resolver problemas de probabilidad en los que que se repiten\nlas realizaciones de las variables aleatorias, lo que\nnos interesa es el total. hay que confundir la suma de variables\naleatorias con la mezcla de poblaciones en los que hay que\naplicar los teoremas de la probabilidad total y de Bayes.Supongamos que la probabilidad de que un estudiante acabe un grado en Ciencias\nes de \\(0.4\\). Tomamos al azar un grupo de 5 estudiantes. ¿Cuál es la probabilidad\nde que ninguno obtenga el grado? ¿Y la probabilidad de que al menos dos lo\nobtengan?Si definimos la variable aleatoria \\(X:\\) Número de estudiantes que obtienen el grado de un grupo de 5,\nentonces \\(X\\) sigue la distribución:\\[X\\sim \\mathit{Bin}(5;\\; 0.4);\\; x = 0, 1, 2, 3, 4, 5\\]y por tanto las probabilidades pedidas son, respectivamente:\\[P[X=0]=\\binom{n}{x}\\cdot p^x \\cdot (1-p)^{(n-x)}=\\binom{5}{0}\\cdot 0.4^0 \\cdot (0.6)^5\\simeq0.0776.\\]HOJA DE CÁLCULOEn las aplicaciones de hoja de cálculo, tenemos funciones que devuelven\nla densidad (probabilidad en modelos discretos) y la probabilidad acumulada\n(función de distribución) de los modelos de distribución de probabilidad más\nutilizados. Puede diferir el nombre de la función entre diferentes programas.\nEn Hojas de Cálculo de Google y LibreOffice se obtendrían las probabilidades\ndel ejemplo así:=BINOM.DIST(0;5;0,4;0)=1-BINOM.DIST(1;5;0,4;1)Mientras que en EXCEL la función se llama DISTR.BINOM.N:=DISTR.BINOM.N(0;5;0,4;)=1-DISTR.BINOM.N(1;5;0,4;VERDADERO)RSelecciono 10 potenciales sujetos del estudio al azar. ¿Cuál es la probabilidad de que al menos uno responda al tratamiento?En términos de variable aleatoria:\\(X\\): Número de éxitos en 10 experimentos independientes de Bernoulli con probabilidad de éxito 0.25\\(X \\sim \\mathit{Bin(10;\\; 0.25)}\\)\\(P[X \\geq 1] = 1- P[X < 1] = 1-P[X=0] \\simeq 1- 0.0563 \\simeq 0.9437\\)HOJA DE CÁLCULO[LibreOffice] =1-BINOM.DIST(0;10;0,25;1)[EXCEL] =1-DISTR.BINOM.N(0;10;0,25;VERDADERO)RHay tres consideraciones muy importantes la hora de resolver ejercicios en variables\ndiscretas:Es muy importante tener claro cuáles son los posibles valores de la variable aleatoria,\ny así saber qué probabilidades hay que calcular.Es muy importante tener claro cuáles son los posibles valores de la variable aleatoria,\ny así saber qué probabilidades hay que calcular.Es posible llegar al resultado de varias formas posibles, y hay que pararse pensar\ncuál será la más rápida, usando las propiedades de la probabilidad (principalmente: probabilidad\ndel suceso complementario y probabilidad de la unión de sucesos disjuntos).Es posible llegar al resultado de varias formas posibles, y hay que pararse pensar\ncuál será la más rápida, usando las propiedades de la probabilidad (principalmente: probabilidad\ndel suceso complementario y probabilidad de la unión de sucesos disjuntos).Al cambiar de una probabilidad la del suceso contrario, es muy importante tener en\ncuenta si las desigualdades incluyen el símbolo igual.Al cambiar de una probabilidad la del suceso contrario, es muy importante tener en\ncuenta si las desigualdades incluyen el símbolo igual.","code":"#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ndbinom(x = 0, size = 5, prob = 0.4)\n#> [1] 0.07776\n1 - pbinom(q = 1, size = 5, prob = 0.4)\n#> [1] 0.66304\npbinom(q = 1, size = 5, prob = 0.4, lower.tail = FALSE)\n#> [1] 0.66304\nsum(dbinom(x = 2:5, size = 5, prob = 0.4))\n#> [1] 0.66304\npbinom(q = 0, size = 10, prob = 0.25, lower.tail = FALSE)\n#> [1] 0.9436865"},{"path":"modelos.html","id":"distribución-de-poisson","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2.3 Distribución de Poisson","text":"La distribución de Poisson surge inicialmente como distribución límite de la binomial cuando \\(n\\) tiende infinito y \\(p\\) se mantiene estable. Posteriormente se vio que describe muy bien los procesos donde se cuentan el número de ocurrencias de un evento por unidad (de tiempo, espacio, …). La probabilidad de ocurrencia en un instante concreto es muy baja, pero en un intervalo determinado es muy probable que suceda varias veces. Bajo estas circunstancias, la variable aleatoria:\\[X: \\text{ Número de eventos por unidad}\\]\nsigue una distribución de Poisson:\\[X \\sim \\mathit{Poiss}(\\lambda);\\; \\lambda >0, \\]donde el único parámetros \\(\\lambda\\) es la media y la varianza de la variable aleatoria. Es decir, se producen,\nde media, \\(\\lambda\\) eventos por unidad de tiempo, superficie, etc. La\ndistribución de Poisson tiene la siguiente función de probabilidad:\\[\\boxed{P[X = x] = \\frac{e^{-\\lambda}\\lambda^x}{x!};\\; x = 0, 1, \\ldots\\ \\infty}.\\]La figura 7.1 muestra gráficamente la distribución\nde probabilidad para varios valores de \\(n\\) y \\(p\\). Se representan valores\ndesde \\(x=0\\) hasta \\(x= \\mu + 4\\sigma\\). Aunque teóricamente los\nposibles valores son hasta infinito, partir de ese valor\nla probabilidad es prácticamente cero. Para valores de \\(\\lambda\\) grandes,\nesto también sucede en los valores de \\(x\\) bajos.\nFigura 7.2: Representación de la función de probabilidad del modelo de Poisson\nLas características principales de la distribución de Poisson son las siguientes:Media: \\(\\mu=E[X] = \\lambda\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\lambda\\).Como la binomial, también cumple la propiedad aditiva de modo que, para \\(m\\)\nvariables aleatorias independientes de Poisson:\\[Y=\\sum\\limits_{j=1}^m {X_j},\\; X_j \\sim \\mathit{Poiss}(\\lambda_j) \\implies Y \\sim \\mathit{Poiss}\\left ( \\sum\\limits_{j=1}^m \\lambda_j \\right ).\\]En una parada de autobús llegan de media cuatro autobuses cada hora. Cuál es la\nprobabilidad de llevar una hora y que haya pasado ninguno todavía?Si \\(X:\\) número de autobuses que pasan en una hora, entonces:\\[X \\sim \\mathit{Poiss}(4),\\]\ny entonces lo que queremos saber es:HOJA DE CÁLCULOEn este caso la función si es la misma en Excel y en las hojas de cálculo libres.=POISSON.DIST(0;4;0)RLa tasa media semanal de visitas de un cliente la página web de ofertas es igual 8. Calcular la probabilidad de que un posible cliente acceda menos de 3 veces en una semana. En términos de variable aleatoria, tenemos que:\\(X\\): Número de visitas por semana la web de oferta\\(X\\): Número de visitas por semana la web de oferta\\(X \\sim \\mathit{Poiss}(8)\\)\\(X \\sim \\mathit{Poiss}(8)\\)\\(P[X < 3] = P[X \\leq 2] = \\sum\\limits_{x = 0}^2 P[X = x]=P[X=0] + P[X=1]+P[X=2]\\)\n\\(\\simeq 0.0003 + 0.0027 + 0.0107=0.0138\\)\\(P[X < 3] = P[X \\leq 2] = \\sum\\limits_{x = 0}^2 P[X = x]=P[X=0] + P[X=1]+P[X=2]\\)\n\\(\\simeq 0.0003 + 0.0027 + 0.0107=0.0138\\)Supongamos que estamos interesados en las visitas que un cliente hace la página web\ndurante cuatro semanas. Y queremos saber la probabilidad de que acceda 30 veces.\nEntonces aplicamos la propiedad aditiva de la distribución de Poisson, y definimos:\\(Y: \\text{ Número de visitas en cuatro semanas } = X_1 + X_2 + X_3 + X_4,\\)donde\\(X_i: \\text{ Número de visitas en el día } , \\, = 1, 2, 3, 4 \\sim \\mathit{Poiss}(8)\\)Entonces:\\[Y \\sim \\mathit{Poiss}(32),\\]y la probabilidad buscada es:\\[P[Y = 30] = \\frac{e^{-32}\\cdot 32^{30}}{30!} \\simeq 0.0681.\\]HOJA DE CÁLCULO=POISSON.DIST(2;8;1)=POISSON.DIST(30;32;0)RLa distribución de Poisson se puede utilizar como aproximación de la distribución\nbinomial bajo ciertas condiciones. En la práctica, para \\(n\\geq 100\\) y \\(p \\leq 0.05\\),\nse puede utilizar la aproximación:\\[X\\sim \\mathit{Bin}(n;\\;p) \\leadsto \\mathit{Poiss}(\\lambda=np),\\]\nsiempre y cuando \\(np\\) tenga sentido como parámetro \\(\\lambda\\), es decir, excesivamente grande\nni excesivamente pequeño. La figura 7.3 muestra la función\nde distribución de una variable aleatoria binomial con parámetros \\(n = 100, \\, p = 0.05\\)\ny su aproximación por una Poisson de parámetro \\(\\lambda = 5\\).\nFigura 7.3: Aproximación binomial por la Poisson\nSupongamos que tenemos en la página web del estudio un formulario de contacto, y\nque sabemos por históricos que el 1% de los sujetos\nde nuestro servicio que entran al formulario, terminan enviando una reclamación.Tomamos al azar 100 potenciales usuarios. ¿Cuál es la probabilidad de que\nmenos de 3 hayan puesto una reclamación?La variable aleatoria con la que podemos modelizar este problema es:\\(X: \\text{ Número de clientes de una muestra de 100 que pone una reclamación},\\)que sigue una distribución binomial de parámetros \\(n=100\\), \\(p = 0.01\\). Como\nse dan los requisitos, podemos hacer la aproximación la distribución de\nPoisson, y entonces:\\[X \\leadsto \\mathit{Poiss}(\\lambda=1),\\]y la probabilidad pedida la podemos aproximar como:R","code":"\ndpois(x = 0, lambda = 4)\n#> [1] 0.01831564\nppois(q = 2, lambda = 8)\n#> [1] 0.01375397\ndpois(x = 30, lambda = 32)\n#> [1] 0.06814215\npbinom(2, 100, 0.01)\n#> [1] 0.9206268\nppois(2, 1)\n#> [1] 0.9196986"},{"path":"modelos.html","id":"distribución-binomial-negativa","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2.4 Distribución binomial negativa","text":"La distribución binomial negativa describe procesos en los que realizamos\nsucesivos experimentos independientes de Bernoulli, con probabilidad de éxito \\(p\\).\nPero sabemos cuántos vamos realizar, porque lo que nos interesa es\nel número de fracasos \\(x\\) hasta que se\nproduzcan \\(c\\) éxitos. También se puede expresar como el número total de\npruebas necesarias \\(x+c\\) hasta obtener \\(c\\) éxitos. Así, definimos la variable aleatoria:\\[X: \\text {Número de fracasos hasta  } c \\text{ éxitos }\\]que sigue el modelo de distribución de probabilidad binomial negativa con parámetros\n\\(c\\) y \\(p\\):\\[X \\sim \\mathit{BN}(c;\\; p); \\; c>0;\\; 0<p<1.\\]Nótese que \\(X\\) puede tomar, teóricamente, cualquier valor mayor o igual que \\(0\\) (tiene límite). Su función de probabilidad es:\\[\\boxed{P[X = x] =\\binom{x+c-1}{x}\\cdot p^c \\cdot (1-p)^{x};\\; x = 0, 1, 2, \\ldots, \\infty },\\]donde:\\[\\binom{x+c-1}{x}=\\frac{(c+x-1)!}{x!(c-1)!}.\\]Si nos fijamos detenidamente en la función de probabilidad, podemos hacer\nel mismo análisis que en la binomial, multiplicando las probabilidades\nde cada experimento independiente de Bernoulli para una ordenación posible,\ny sumando las probabilidades\nde cada ordenación. La diferencia está en que el último experimento es\nsiempre un éxito (habremos llegado al éxito número \\(c\\), y paramos).\nSi se da \\(X=x\\), entonces habremos realizado un total de \\(x+c\\) pruebas de Bernoulli.El término negativa viene de la siguiente forma alternativa de escribir su función de probabilidad:\\[P[X = x] =  \\binom{-c}{x}\\cdot p^c \\cdot (1-p)^{x}.\\]La figura 7.4 muestra gráficamente la distribución\nde probabilidad para varios valores de \\(c\\) y \\(p\\). Se representan valores\ndesde \\(x=0\\) hasta \\(x= 20\\). Aunque teóricamente los\nposibles valores son hasta infinito, partir de cierto valor\n(dependiendo de los parámetros)\nla probabilidad es prácticamente cero. Para valores de \\(p\\) pequeños,\nesto también sucede en los valores de \\(x\\) bajos.\nFigura 7.4: Representación de la función de probabilidad del modelo binomial negativo\nLa media y la varianza de una variable aleatoria que sigue un modelo binomial negativo son:Media: \\(\\mu=E[X] = \\frac{c\\cdot (1-p)}{p}\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\frac{c\\cdot (1-p)}{p^2}\\).Se cumple la propiedad aditiva de forma similar como lo hacía en la distribución binomial.\nEs decir, la suma de \\(m\\) variables aleatorias binomiales negativas con el mismo parámetro \\(p\\)\ny parámetros \\(c_j\\) que pueden ser diferentes, es una variable aleatoria que sigue también\nuna distribución binomial negativa con el mismo parámetro \\(p\\):\\[Y=\\sum\\limits_{j=1}^m {X_j},\\; X_j \\sim \\mathit{BN}(c_j;\\; p) \\implies Y \\sim \\mathit{BN}\\left ( \\sum\\limits_{j=1}^m c_j;\\; p \\right ).\\]Dos equipos de balonmano y B se disputan la final de liga al mejor de\n7 partidos. El factor campo influye y el equipo tiene una probabilidad\nde ganar un partido de \\(0.6\\). ¿Cuál es la probabilidad de que el equipo gane\nla liga en 5 partidos?Para plantear el problema en términos de variable aleatoria, tenemos que pensar\nqué llamamos éxito y qué llamamos fracaso, definir la variable aleatoria, y decidir\ncuál es el valor del que queremos calcular la probabilidad. Como la pregunta se\nplantea para el equipo , que tiene una probabilidad de ganar un partido de \\(0.6\\),\ncada partido es un experimento independiente de Bernoulli con probabilidad de\néxito \\(p=0.6\\), que vamos realizando\nuno tras otro. Si la liga se disputa al mejor de 7, quiere decir que la ganará\nel primero que gane 4. Por tanto, repetiremos el experimento de Bernoulli que hemos\ndefinido hasta tener 4 éxitos (\\(c=4\\)). Como el suceso que nos interesa\nes que el equipo gane la partida en \\(x+c=5\\) partidos, esto significará que habrá\nperdido \\(5-4=1\\) partido (un fracaso). Si definimos la variable aleatoria\\(X:\\) Número de partidos que pierde antes de\nganar el cuarto,entonces\\(X\\sim \\mathit{BN}(c=4;\\;p=0.6),\\)y por tanto buscamos la\nprobabilidad de que pierda solo uno es la probabilidad de que la variable aleatoria\nsea igual uno:HOJA DE CÁLCULOEn hojas de cálculo de Google hay que quitar el último argumento de la\nfórmula.=NEGBINOM.DIST(1;4;0,6;0)RLa siguiente expresión obtiene la probabilidad pedida.En nuestro ejemplo ilustrativo, se seleccionan sujetos al azar y de forma independiente. ¿Cuál es la probabilidad de que se necesiten más de 10 extracciones para que haya 4 mujeres?El experimento de Bernoulli consiste en observar si un sujeto es mujer (éxito)\nu hombre (fracaso). Y se repite hasta qu hayamos observado \\(c=4\\) mujeres. Entonces\\(X\\): Número de fracasos en pruebas independientes de Bernoulli con probabilidad de éxito 1/2 hasta el cuarto éxito\\(X\\): Número de fracasos en pruebas independientes de Bernoulli con probabilidad de éxito 1/2 hasta el cuarto éxito\\(X \\sim \\mathit{BN(4;\\; 1/2)}\\)\\(X \\sim \\mathit{BN(4;\\; 1/2)}\\)Nótese que aquí se está planteando la pregunta en términos de número total de\nexperimentos, es decir, \\(x+c > 10\\), y entonces buscamos \\(x > 10-4\\):\\[P[X > 6] = 1- P[X \\leq 6]= 1- \\sum\\limits_{x=0}^6 P[X=x] = \\]\n\\[ =1-(0.0625 + 0.125 + 0.1563 + 0.1562 + 0.1367 + 0.1094 + 0.082) =0.1719\\]HOJA DE CÁLCULO=1-NEGBINOM.DIST(6;4;0,5;1)En hojas de cálculo de Google está el argumento para calcular acumulado,\npor lo que habría que calcular primero las probabilidades (desde cero hasta 6),\nsumar y restarlo de 1.RUn caso particular de la distribución binomial negativa cuando\n\\(c=1\\), es la distribución geométrica. Es decir, nos interesan el número de fracasos hasta obtener el primer éxito y entonces:\\(X\\): Número de fracasos hasta obtener el primer éxito en una serie de pruebas independientes de Bernoulli con probabilidad de éxito \\(p\\):\\[X \\sim \\mathit{Ge}(p); \\; 0<p<1,\\]\ncuya función de probabilidad se simplifica bastante, ya que solo hay una ordenación posible de los éxitos y fracasos:\\[\\boxed{P[X = x] = p \\cdot (1-p)^{x};\\; x = 0, 1, \\ldots, \\infty }.\\]En la figura 7.4 la primera columna se corresponde con\ndistribuciones geométricas. La media y varianza de una distribución geométrica son:Media: \\(\\mu=E[X] = \\frac{1-p}{p}.\\)Media: \\(\\mu=E[X] = \\frac{1-p}{p}.\\)Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\frac{1-p}{p^2}.\\)Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\frac{1-p}{p^2}.\\)Observamos los sujetos que inician sesión en la página web del estudio, y nos\ninteresa si es un investigador o . ¿Cuál es la probabilidad de que se lleguen menos de 5 sujetos\nhasta que llega el primer investigador? ¿Cuál sería el número esperado de investigadores hasta que llegue\nel primer investigador?La probabilidad de éxito es \\(p=4/52\\), y el suceso que nos interesa se corresponde con \\(x+1<5\\).\nEntonces:\\(X\\): Número de fracasos en pruebas independientes de Bernoulli con probabilidad de éxito 4/52 hasta el primer éxito\\(X \\sim \\mathit{Ge(4/52)}\\)\\(P[X < 4] = P[X \\leq 3] \\simeq 0.0769 + 0.071 + 0.0655 + 0.0605 \\simeq 0.2739\\)la segunda pregunta damos respuesta calculando la media:\\[\\mu = \\frac{1-p}{p}= \\frac{1-(4/52)}{4/52}=12,\\]HOJA DE CÁLCULONo hay una fórmula específica para la distribución geométrica, pero podemos usar\nla de la binomial negativa con parámetro \\(c=1\\).=NEGBINOM.DIST(3;1;4/52;1)R","code":"\ndnbinom(x = 1, size = 4, prob = 0.6)\n#> [1] 0.20736\npnbinom(q = 6, size = 4, prob = 0.5, lower.tail = FALSE)\n#> [1] 0.171875\npnbinom(q = 6, size = 4, prob = 1/2, lower.tail = FALSE)\n#> [1] 0.171875\nqnbinom(p = 0.95, size = 4, prob = 1/2)\n#> [1] 9\npgeom(q = 3, prob = 4/52)\n#> [1] 0.273975"},{"path":"modelos.html","id":"distribución-hipergeométrica","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.2.5 Distribución hipergeométrica","text":"La distribución hipergeométrica es el equivalente la binomial cuando las pruebas de Bernoulli son independientes. Se asemeja los problemas de urnas con bolas blancas y negras, o aquellos en los que realizamos muestreos sin reposición. La variable aleatoria\nse define en los siguientes términos: tenemos una conjunto de \\(N\\) elementos (por ejemplo bolas) de los cuales \\(M\\) son de una determinada\nclase \\(\\) (por ejemplo blancas). Por tanto, \\(N-M\\) son de la clase \\(\\) (por ejemplo negras). Extraemos \\(n\\) elementos sin reposición de este conjunto, y lo que nos interesa es el número de elementos de la muestra que cumplen la característica. Entonces podemos definir\nla variable aleatoria:\\(X\\): Número de elementos de la clase \\(\\) obtenidos en un muestreo sin reemplazo de tamaño \\(n\\) de un conjunto con \\(N\\) elementos totales de los que \\(M\\) son de dicha categoría \\(\\).Que sigue una distribución geométrica de parámetros \\(N\\), \\(M\\) y \\(n\\).\\[X \\sim \\mathit{HG}(N;\\; M;\\; n);\\;N>M;\\;N\\geq n.\\]La distribución hipergeométrica tiene la siguiente función de probabilidad:\\[\\boxed{P[X = x] = \\frac{\\binom{N-M}{n-x}\\cdot \\binom{M}{x}}{\\binom{N}{n}};\\; \\max{(0, n+M-N)} \\leq x \\leq \\min{(M,n)}}.\\]La figura 7.1 muestra gráficamente la distribución\nde probabilidad para varios valores de \\(M\\) y \\(n\\) y \\(N=20\\).\nFigura 7.5: Representación de la función de probabilidad del modelo hipergeométrico\nLa media y la varianza de la distribución hipergeométrica son las siguientes:Media: \\(\\mu=E[X] = M\\cdot \\frac{n}{N}\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\frac{M\\cdot(N-M)\\cdot n\\cdot (N-n)}{N^2\\cdot(N-1)}\\).Nótese que la distribución hipergeométrica asume la independencia de los\nsucesivos experimentos. obstante, es asintótica la distribución\nbinomial \\(\\mathit{Bin}\\left (n;\\; p = \\frac{M}{N}\\right)\\) si \\(p\\) se mantiene estable. Se suele\nconsiderar apropiada la aproximación si \\(\\frac{n}{N}<0.1\\).En una comunidad de vecinos con 50 propietarios, 30 están de acuerdo\nen instalar un ascensor, y el resto . En el descanso, cinco vecinos (al azar) se\nsalen fumar la puerta. ¿Cuál es la probabilidad de que de esos cinco solo uno\nesté de acuerdo en instalar el ascensor?Definimos la variable aleatoria:\\(X\\): Número de vecinos de esos cinco que están de acuerdo en instalar el ascensor. Entonces:\\[X\\sim \\mathit{HG}(N=50;\\,M=30;\\,n=5),\\]y la probabilidad que buscamos es:HOJA DE CÁLCULO[EXCEL] =DISTR.HIPERGEOM.N(1;5;30;50;0)[LibreOffice] =HYPGEOM.DIST(1;5;30;50;0)[Hojas de Cálculo de Google] =HYPGEOM.DIST(1;5;30;50)RLa parametrización en R es ligeramente distinta, aunque obviamente equivalente,\nla que hemos usado aquí, que se corresponde con la que aparece\nen la definición 2.48 de la norma ISO 3534-1. Además, utiliza los términos utilizados\nen problemas de urnas, de forma que los argumentos de la función son:x: El valor (quantile) para el cual hay que calcular la probabilidad.m: Número de bolas blancas (white balls), que se corresponde con nuestro parámetro \\(M\\).n: Número de bolas negras (black balls), que se corresponde con \\(N-M\\) según nuestra parametrización.k: Número de bolas extraídas, que se corresponde con nuestro parámetro \\(n\\).La siguiente expresión calcula la probabilidad del ejemplo.Se asignan 10 premios potenciales usuarios del servicio, pero se pueden repetir ganadores. ¿Cuál es la probabilidad de que exactamente un directivo sea premiado?Recordemos que teníamos 52 potenciales usuarios, de los cuales 4 eran directivos. Conocemos\nla composición exacta del conjunto, y es un muestreo sin reemplazamiento, por tanto la distribución\nadecuada es la hipergeométrica. Además, podríamos usar la aproximación de la binomial, ya que\n\\(10/52 \\nleq 0.1\\).En términos de variable aleatoria, definimos:\\(X\\): Número de directivos en una muestra sin reemplazamiento de tamaño 10 realizada sobre un conjunto de 52 personas de las que 4 son directivos.Entonces:\\(X \\sim \\mathit{HG}(N=52;\\; M=4;\\; n=10)\\)\\(P[X = 1]\\simeq 0.4240\\)","code":"\ndhyper(x = 1, m = 4, n = 52-4, k = 10)\n#> [1] 0.4240465"},{"path":"modelos.html","id":"modelos-de-distribución-de-probabilidad-continuos","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3 Modelos de distribución de probabilidad continuos","text":"","code":""},{"path":"modelos.html","id":"introducción-1","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3.1 Introducción","text":"En este apartado vamos revisar algunas distribuciones de probabilidad\ncontinuas que tienen interés en ciencias e ingeniería. Al igual que en los modelos de\ndistribución de probabilidad discretos, un conjunto de parámetros determinan\ncompletamente la distribución de probabilidad.\nEntonces tendremos la función de densidad, o la función de distribución, o ambas,\nen función de la variable \\(x\\) y también del conjunto de parámetros \\(\\boldsymbol{\\theta}\\).\nEntonces, para valores concretos de los parámetros, podremos calcular probabilidades\no determinar las características de la variable aleatoria en estudio.\nPara algunas distribuciones de probabilidad se han tabulado los valores de la\nfunción de distribución o su complementario, y tradicionalmente se han utilizado\nestas tablas para resolver problemas de probabilidad. Actualmente se pueden\nrealizar los cálculos con el uso de software. Por tanto, seguiremos utilizando\nla notación vista en el apartado 7.2 para indicar\nla distribución de probabilida continua que sigue la variable aleatoria \\(X\\):\\[X \\sim \\mathcal{D}\\mathit{istr}(\\boldsymbol{\\theta}),\\]donde \\(\\mathcal{D}\\mathit{istr}\\) identifica el modelo de distribución de probabilidad, y \\(\\boldsymbol{\\theta}\\) es el vector de parámetros. Entonces las expresiones de\nla función de densidad y de distribución contendrán los parámetros: \\(f(x|\\boldsymbol{\\theta})\\),\n\\(F(x|\\boldsymbol{\\theta})\\).En este capítulo veremos con detalle las distribuciones uniforme, exponencial y normal.\nExisten otros modelos de distribución de probabilidad continuos univariantes\ny multivariantes que se referencian al final del capítulo.","code":""},{"path":"modelos.html","id":"distribución-uniforme","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3.2 Distribución uniforme","text":"La distribución uniforme se caracteriza por tener una densidad constante en un intervalo \\([, b]\\).\nSi una variable aleatoria \\(X\\) sigue una distribución uniforme en el intervalo entre \\(\\) y \\(b\\) lo\nexpresamos así:\\[X \\sim \\mathit{U}(;\\; b);\\; < b;\\; , b \\\\mathbb{R}.\\]\nLa función de densidad de una variable aleatoria continua que sigue un modelo uniforme tiene\nla siguiente función de densidad:\\[f(x) = \n\\begin{cases}\n\\frac{1}{b-} & \\text{si } \\leq x \\leq b\\\\\n0 & \\text{resto}\n\\end{cases}\\]y la función de distribución se obtiene fácilmente partir de esta:\\[F(x)=\\int_a^x \\frac{1}{b-}dt=\\left [ \\frac{t}{b-}\\right ]_a^x = \\frac{x}{b-}- \\frac{}{b-}=\\frac{x-}{b-},\\]quedando en su forma completa como:\\[F(x) = \n\\begin{cases}\n0 & \\text{si } x < \\\\\n\\frac{x-}{b-} & \\text{si } \\leq x < b\\\\\n1 & \\text{si } x \\geq b\n\\end{cases}\\]La media y la varianza de una variable aleatoria uniforme se deducen\nfácilmente partir de su función de densidad:Media: \\(\\mu=E[X] = \\frac{+b}{2}\\).Varianza: \\(\\sigma^2=\\mathit{V}[X] = \\frac{(b-)^2}{12}.\\)El modelo de distribución uniforme es muy útil para simular probabilidades y variables aleatorias través\nde la \\(U(0; 1)\\). También se suele utilizar cuando conocemos el rango de valores pero tenemos información\nsobre cuáles de esos valores son más probables. La figura 7.6 muestra\nla representación de las funciones de densidad y distribución de una variable\naleatoria que sigue una distribución continua uniforme.\nFigura 7.6: Representación gráfica de las funciones de densidad y distribución de una variable aleatoria uniforme\nEl volumen anual de ventas de un almacén se distribuye uniformemente entre 380 y 1200 miles de euros.\n¿Cuál es la probabilidad de que las ventas sean superiores 1000 miles de euros? ¿Cuáles son las ventas esperadas en un año?Definimos la variable aleatoria:\\(X\\): ventas del almacén un año \\(X\\sim U(380;\\,1200)\\)Entonces la función de densidad es:\\[f(x)=\\frac{1}{1200-380},\\; 380<x<1200,\\]la probabilidad pedida:\\[P[X>1000]= \\int_{1000}^{1200}\\frac{1}{820}dx = \\frac{200}{820}\\simeq 0.2439.\\]Pero también podemos calcularla más fácilmente utilizando la función de distribución, que conocemos:\\[P[X>1000]= 1- P[X\\leq 1000 ] = 1- F(1000) =\\\\= 1 - \\frac{1000 - 380}{1200-380} \\simeq 1 - 0.7561 \\simeq 0.2439.\\]y las ventas esperadas son la media de la variable aleatoria:\nFigura 7.7: Ejemplo distribución uniforme\nHOJA DE CÁLCULONo hay funciones específicas para obtener la probabilidad de una variable\naleatoria uniforme, aunque se puede insertar una fórmula con la función\nde distribución y partir de ahí calcular probabilidades, por ejemplo, si\nen la celda A1 tenemos el valor 1000, en la celda A2 el parámetro\n= 380 y en la celda A3 el parámetro b = 1200, entonces\nen otra celda podemos calcular la\nprobabilidad del ejemplo como:= 1 - (A1 - A2)/(A3 - A2)RSi la proporción de video visualizado por un sujeto que sigue el mensaje se distribuye de forma uniforme, ¿cuál es la probabilidad de que un visitante de la web del estudio vea más del 90% del vídeo?En términos de variable aleatoria:\\(X\\): Proporción de video visualizado, \\(X \\sim U(0;\\; 1)\\).Entonces:\\[P[X > 0.9]=\\int_{0.9}^{1}dx = 0.1.\\]O bien:\\[P[X > 0.9]=1-F(0.9)=1-\\frac{0.9 - 0}{1-0} = 0.1.\\]R","code":"#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\npunif(q = 1000, min = 380, max = 1200, lower.tail = FALSE)\n#> [1] 0.2439024\npunif(q = 0.9, min = 0, max = 1, lower.tail = FALSE)\n#> [1] 0.1"},{"path":"modelos.html","id":"distribución-exponencial","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3.3 Distribución exponencial","text":"Cuando en un proceso de Poisson observamos el tiempo que transcurre entre un evento y otro, aparece la distribución exponencial. También modeliza bien tiempos de vida, por ejemplo de componentes electrónicos.\nLa distribución exponencial solo tiene un parámetro:\\[X \\sim \\mathit{Exp}(\\beta),\\; \\beta>0.\\]El parámetro \\(\\beta\\) del modelo de distribución exponencial representa, al igual\nque en la distribución de Poisson, la tasa media de eventos por unidad de tiempo. Una variable aleatoria que sigue un modelo de distribución exponencial tiene la siguiente función de densidad:\\[f(x) = \n\\begin{cases}\n\\beta e^{-\\beta x} & \\text{si } x > 0\\\\\n0 & \\text{si } x\\leq 0\n\\end{cases}\\]La función de distribución se obtiene fácilmente partir de la función\nde densidad:\\[F(x)=\\int_{-\\infty}^xf(t)dt=1-e^{-\\beta x}, \\; x > 0.\\]La figura 7.8 muestra\nla representación de las funciones de densidad y distribución de una variable\naleatoria que sigue una distribución continua exponencial.\nFigura 7.8: Representación gráfica de las funciones de densidad y distribución de una variable aleatoria exponencial\nLa media y la varianza de una variable aleatoria que sigue el modelo exponencial son:Media: \\(\\mu=E[X] = \\frac{1}{\\beta}\\).Varianza: \\(\\mathit{V}[X] = \\frac{1}{\\beta^2}\\)Se dice que la exponencial es una variable aleatoria sin memoria, en el sentido de\nque el tiempo que haya tardado en ocurrir un evento, es independiente de cuándo\nsucedió el anterior:\\[(P[X > t_2 + t_1 | X > t_1] = P[X > t_2]).\\]La distribución exponencial es un caso particular de la distribución gamma, que \nvemos en este texto. La distribución gamma\nmodeliza el tiempo transcurrido hasta ocurrir un número determinado de eventos.El tiempo en horas que se tarda en arreglar una máquina sigue una distribución\nexponencial de parámetro \\(\\beta=4\\). ¿Cuál es la probabilidad de que una avería\ntarde más de una hora en ser reparada?\\[X\\sim \\mathit{Exp}(4),\\]\\[P[X>1]=1-\\int_0^14 e^{-4x}dx=1-\\left[-e^{-4x}\\right]_0^1=1-(-e^{-4}-(-e^0))=e^{-4}\\simeq 0.0183.\\]Es más sencillo si lo resolvemos con la función de distribución:\\[P[X>1]=1-F(1)=1-(1-e^{-4\\cdot 1})=\\simeq 0.0183.\\]HOJA DE CÁLCULO[EXCEL] =1-DISTR.EXP.N(1; 4; 1)[LibreOffice] =1-EXPON.DIST(1;4;1)R\nFigura 7.9: Representación de la función de densidad del modelo exponencial del ejemplo\nEn ocasiones nos interesa calcular la inversa de la función de distribución.\nEs decir, encontrar un valor de la variable aleatoria para el cual\nse cumple alguna condición de probabilidad, como en el siguiente ejemplo.El tiempo que permanece un visitante en la web del estudio sigue una distribución exponencial.\nLa tasa media de abandonos es de 2 cada minuto. ¿Cuánto tiempo permanece como máximo el 95% de los usuarios antes de abandonar?En términos de variable aleatoria:\\(X\\): Tiempo hasta abandonar la web tras hacer clic en el mensaje, \\(X\\sim \\mathit{Exp}(2)\\).En este caso, lo que nos interesa es obtener el cuantil 0.95, es decir, el valor \\(x\\) de la variable aleatoria para el cual \\(P[X > x] = 0.05\\), o lo que es lo mismo, \\(P[X \\leq x]=0.95\\). como tenemos la expresión de la\nfunción de distribución, hay más que despejar y tenemos:\\[F(x) = 0.95 \\iff 1-e^{-2x}=0.95 \\iff x = 1.498 \\text{ minutos}.\\]También nos podemos preguntar cuánto tiempo permanece un visitante, en promedio, en la web.\nEntonce calculamos la experanza:\\[\\mu = \\frac{1}{\\beta} = 0.5\\]R","code":"\npexp(q = 1, rate = 4,lower.tail = FALSE)\n#> [1] 0.01831564\nqexp(p = 0.95, rate = 2)\n#> [1] 1.497866"},{"path":"modelos.html","id":"distribución-normal","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3.4 Distribución normal","text":"Sin duda, la distribución normal (o gaussiana) es el modelo de distribución de probabilidad continuo más importante de todos.\nGracias al teorema central del límite que veremos en el capítulo 7.5, muchas situaciones se aproximan la distribución normal66. El modelo de distribución\nnormal queda determinado por dos parámetros, que son su media \\(\\mu\\) y su desviación típica \\(\\sigma\\):\\[X \\sim \\mathit{N}(\\mu;\\; \\sigma);\\; \\mu \\\\mathbb{R}, \\sigma > 0.\\]La función de densidad de una variable aleatoria que sigue el modelo de distribución normal\ntiene la siguiente función de densidad:\\[f(x) = \n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\;-\\infty < x < \\infty.\\]La figura 7.10 muestra la función de densidad y la función de distribución\npara unos valores determinados\nde \\(\\sigma\\) y \\(\\mu\\). La función de distribución se ha obtenido por métodos numéricos,\nya que es posible obtener una expresión analítica de \\(F(x)\\) al existir una\nprimitiva de \\(f(x)\\).\nFigura 7.10: Representación gráfica de las funciones de densidad y distribución de una variable aleatoria normal\nLa distribución normal es simétrica respecto\nde la media, siendo la mediana y la moda igual ella. Esta importante propiedad implica que \\(P[X \\leq \\mu] = 0.5\\). Cuanto más cerca de la media estén los valores, más probables son, y medida que nos alejamos de la media, cada vez son más improbables, de hecho como vemos en la figura 7.11 entre la media y dos desviaciones típicas tenemos más del 95% de la probabilidad, y la probabilidad de que la variable aleatoria tome valores más allá de tres desviaciones típicas desde la media es de solo 0.0027. La función de densidad presenta puntos de inflexión en \\(\\mu \\pm \\sigma\\).\nFigura 7.11: Función de densidad de la distribución normal\nEl modelo de distribución normal cumple la propiedad aditiva, de modo que si\ntenemos las variables aleatorias:\\[X_j \\sim N(\\mu_j; \\sigma_j) \\; \\forall\\; j=1, \\ldots, n,\\]entonces la variable aleatoria:\\[Y=+\\sum\\limits_{j=1}^n b_j X_j,\\]\nsiendo todos los \\(b_j\\) nulos, se distribuye también como una distribución normal,\ny por tanto por las propiedadades de la esperanza y la varianza que vimos en el\ncapítulo 5:\\[Y \\sim N\\left(+\\sum\\limits_{j=1}^n b_j \\mu_j; \\sqrt{\\sum\\limits_{j=1}^n b_j^2 \\sigma_j^2} \\right ).\\]Un caso particular del modelo de distribución normal, es la\ndistribución normal estándar, cuyos parámetros serán \\(\\mu=0\\) y\n\\(\\sigma=1\\), y que vamos representar en este texto como \\(Z\\)67:\\[Z \\sim N(0;1).\\]\nLa función de densidad en este caso quedaría:\\[f(x) = \n\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}},\\;-\\infty < x < \\infty.\\]Nótese que, al ser la distribución normal simétrica, se cumple que \\(P[Z \\leq 0] = 0.5\\).Trabajar con variables aleatorias estandarizadas es conveniente en muchas situaciones.\nEn particular, se ha utilizado tradicionalmente para obtener probabilidades\npor medio de tablas estadísticas que contienen probabilidades de la distribución\nnormal estandarizada, bien la función de distribución \\(F(z)=P[Z \\leq z]\\) o su complementario\n\\(P[Z>z]\\). través de estas tablas podemos hacer cálculo de probabilidades para\ncualquier variable aleatoria normal, con cualesquiera \\(\\mu\\) y \\(\\sigma\\), ya que se cumple,\nsegún la aditividad y las propiedades de la esperanza y la varianza:\\[X \\sim N(\\mu;\\; \\sigma) \\implies Z = \\frac{X-\\mu}{\\sigma} \\sim N(0;\\;1).\\]\nYa vimos en el capítulo 5 que podemos estandarizar cualquier\nvariable aleatoria. Si estandarizamos una distribución\nnormal con cualesquiera parámetros \\(\\mu\\) y \\(\\sigma\\), entonces tendremos\nvariables aleatorias estandarizadas.la hora de calcular probabilidades de la distribución normal, nos encontramos\nque la función de densidad es integrable, es decir, podemos\nencontrar una primitiva. Entonces, en vez de utilizar integrales se utilizan\nmétodos numéricos o tablas como se ha descrito anteriormente.El procedimiento para calcular probabilidades\nde variables aleatorias que siguen el modelo de distribución normal es el siguiente:Determinar los parámetros de la distribución \\(\\mu\\) y \\(\\sigma\\) (para el alcance de este capítulo, vendrán dados).Determinar los parámetros de la distribución \\(\\mu\\) y \\(\\sigma\\) (para el alcance de este capítulo, vendrán dados).Tipificar el/los valores de la variable \\(X\\) para los que se quiere calcular la probabilidad (\\(X \\Z\\)).Tipificar el/los valores de la variable \\(X\\) para los que se quiere calcular la probabilidad (\\(X \\Z\\)).Utilizando las propiedades de la probabilidad, transformar la expresión de la probabilidad que se quiere calcular en expresiones compatibles con la tabla utilizar, por ejemplo \\(P[Z\\leq z]\\)Utilizando las propiedades de la probabilidad, transformar la expresión de la probabilidad que se quiere calcular en expresiones compatibles con la tabla utilizar, por ejemplo \\(P[Z\\leq z]\\)Buscar dentro de la tabla las probabilidades que se necesiten para los valores \\(z\\) y hacer los cálculos.Buscar dentro de la tabla las probabilidades que se necesiten para los valores \\(z\\) y hacer los cálculos.Para la operación inversa del cálculo de cuantiles partir de una probabilidad, procedemos de la siguiente forma:Tipificar la variable aleatoria, obteniendo una expresión \\(z=\\frac{x-\\mu}{\\sigma}\\),\ndonde \\(x\\) es el valor que queremos encontrar.Tipificar la variable aleatoria, obteniendo una expresión \\(z=\\frac{x-\\mu}{\\sigma}\\),\ndonde \\(x\\) es el valor que queremos encontrar.Expresar la probabilidad en forma compatible con la tabla utilizar,\npor ejemplo \\(P\\left [Z\\leq\\frac{x-\\mu}{\\sigma}\\right ]=p\\).Expresar la probabilidad en forma compatible con la tabla utilizar,\npor ejemplo \\(P\\left [Z\\leq\\frac{x-\\mu}{\\sigma}\\right ]=p\\).Buscar dentro la tabla la probabilidad deseada \\(p\\).Buscar dentro la tabla la probabilidad deseada \\(p\\).Encontrar el valor de \\(z\\) que se corresponde con dicha probabilidad, y despejar\n\\(x\\) de la expresión \\(z=\\frac{x-\\mu}{\\sigma}\\).Encontrar el valor de \\(z\\) que se corresponde con dicha probabilidad, y despejar\n\\(x\\) de la expresión \\(z=\\frac{x-\\mu}{\\sigma}\\).En lo que sigue, utilizaremos la tabla de la cola inferior de la distribución\nnormal estandarizada, disponible en el apéndice B. En esta tabla tenemos, para\nvalores de \\(z>0\\), \\(P[Z \\leq z]\\). Con estos valores, seremos capaces de calcular\ncualquier probabilidad utilizando las siguientes propiedades y\ngracias la simetría de la distribución. Dados \\(< b\\) positivos, debemos\nexpresar cualquier probabilidad de forma que podamos buscarla en la tabla:En la tabla tenemos \\(P[Z \\leq b]\\) o \\(P[Z \\leq ]\\).En la tabla tenemos \\(P[Z \\leq b]\\) o \\(P[Z \\leq ]\\).\\(P[Z > ] = P[Z \\leq -] = 1- P[Z \\leq ]\\).\\(P[Z > ] = P[Z \\leq -] = 1- P[Z \\leq ]\\).\\(P[Z > -] = P[Z \\leq ]\\).\\(P[Z > -] = P[Z \\leq ]\\).\\(P[-b \\leq Z\\leq -]\\) = \\(P[\\leq Z \\leq b]= P[Z \\leq b] - P[Z \\leq ]\\).\\(P[-b \\leq Z\\leq -]\\) = \\(P[\\leq Z \\leq b]= P[Z \\leq b] - P[Z \\leq ]\\).\\(P[-\\leq Z\\leq b]\\) = \\(P[Z \\leq b] + P[Z \\leq ] - 1\\).\\(P[-\\leq Z\\leq b]\\) = \\(P[Z \\leq b] + P[Z \\leq ] - 1\\).La figura 7.12 resume estos cálculos. Ayudará al lector pensar en la probabilidad en términos de área bajo la curva\nde la función de densidad, teniendo en cuenta que el área total debe ser igual \nla unidad, y que el área por encima y por debajo de cero es \\(0.5\\).\nLa misma lógica se aplicaría en el caso de utilizar una tabla con la cola superior\nque podamos encontrar en alguna otra bibliografía.\nFigura 7.12: Cálculo de probabilidades de la distribución \\(N(0; 1)\\)\nEn un curso de reciclaje dirigido teleoperadores las puntuaciones obtenidas en el\ntest final se distribuyen siguiendo un modelo normal de media 5 y desviación típica 2.\nCon menos de tres puntos un teleoperador promociona. ¿Cuál es la probabilidad\nde que un teleoperador promocione? ¿Cuál es la puntuación mínima que han obtenido el 3% de los\nteleoperadores mejor preparados?La variable aleatoria es:\\(X:\\) Calificación obtenida por el teleoperador, \\(\\sim N(5; 2)\\),y lo que buscamos es la probabilidad de obtener menos de tres puntos:\\[P[X<3]=P\\left[\\frac{X-5}{2}<\\frac{3-5}{2} \\right]=P[Z < -1]=1-P[Z\\leq 1]=\\boxed{0.1587}.\\]la segunda pregunta contestamos de manera inversa. Tenemos una probabilidad \\(p=0.03\\), y\nbuscamos el valor \\(x\\) de la variable que cumple:\\[P[X\\leq x] = 1- 0.03,\\]\no lo que es lo mismo:\\[P\\left [Z\\leq\\frac{x-\\mu}{\\sigma}\\right] = 0.97,\\]Buscamos\nesta probabilidad en el interior de la tabla68,\nen este caso el valor más próximo redondeando dos decimales es \\(0.9699\\), que se corresponde\ncon un valor \\(z=1.88\\). Entonces tenemos:\\[z=\\frac{x-\\mu}{\\sigma} \\iff 1.88=\\frac{x-5}{2} \\iff x = 1.88\\cdot 2 + 5=\\boxed{8.76},\\]Es decir,\\[P[X>8.76]\\simeq 0.03.\\]\nFigura 7.13: Ejemplo de cálculo de probabilidad y cuantil de la normal\nAl utilizar software, es necesario estandarizar. Le pasaremos directamente\nlos parámetros de la distribución normal la función correspondiente.HOJA DE CÁLCULO[LibreOffice] =NORM.DIST(3;5;2;1)[EXCEL] =DISTR.NORM.N(3;5;2;1)Para obtener el cuantil, tenemos que pasar como argumento de probabilidad 1-0.03,\nya que siempre da la cola inferior.[LibreOffice] =NORM.INV(0,97;5;2)[EXCEL] =INV.NORM(0,97;5;2)RCon la función pnorm calculamos la probabilidad, y con la función qnorm,\nel cuantil.El peso de los paquetes que contienen los pedidos que recibe un laboratorio\nse distribuye según una distribución normal de media \\(1.8\\) y desviación típica \\(0.5\\) kg. ¿Cuál es la\nprobabilidad de que un paquete esté entre 1 y 2 kilos?Definimos la variable aleatoria:\\(X:\\) Peso de los paquetes, \\(X\\sim N(1.8, 0.5)\\).Entonces:\\[P[1 \\leq X \\leq 2] = P \\left [\\frac{1-1.8}{0.5} \\leq \\frac{X-\\mu}{\\sigma} \\leq \\frac{2-1.8}{0.5} \\right ]  = \\]\n\\[=P[-1.6 \\leq Z \\leq 0.4] =P[Z\\leq 4]-P[Z\\leq -1.6]=\\]\n\\[P[Z\\leq4]-(1-P[Z\\leq 1.6])=0.6554+0.9452-1=\\boxed{0.6006}.\\]¿Por debajo de qué peso estarán probablemente\nal menos el 95% de los paquetes?Buscamos el valor de \\(x\\) que cumpla:\\[P[X<x] = 0.95\\]Buscamos\nesta probabilidad en el interior de la tabla, y hay dos valores que nos servirían\nsi redondeamos dos decimales: \\(0.9495\\), correspondiente \\(z=1.64\\) y \\(0.9505\\),\ncorrespondiente \\(z=1.65\\). Vamos tomar este último para asegurarnos la probabilidad\nde \\(0.95\\)69.\nSolo nos queda igualar este valor la \\(x\\) estandarizada y depejar:\\[z=\\frac{x-\\mu}{\\sigma} \\iff 1.65=\\frac{x-1.8}{0.5} \\iff x = 1.65\\cdot 0.5 + 1.8=2.625.\\]Resolvemos de forma análoga al ejemplo anterior. Nótese cómo ahora calculamos\nel cuantil exacto para la probabilidad de 0.95. Como las funciones\nnos dan la función de distribución, aplicamos que \\(P[\\leq x < b] = F(b)- F()\\).HOJA DE CÁLCULO[LibreOffice] =NORM.DIST(2;1,8;0,5;1) - NORM.DIST(1;1,8;0,5;1)[LibreOffice] =NORM.INV(0,95;1,8;0,5)[EXCEL] =DISTR.NORM.N(2;1,8;0,5;1) - DISTR.NORM.N(1;1,8;0,5;1)[EXCEL] =INV.NORM(0,95;1,8;0,5)","code":"\npnorm(q = 3, mean = 5, sd = 2)\n#> [1] 0.1586553\nqnorm(p = 0.03, mean = 5, sd = 2, lower.tail = FALSE)\n#> [1] 8.761587\npnorm(2, 1.8, 0.5) - pnorm(1, 1.8, 0.5)\n#> [1] 0.6006224\nqnorm(p = 0.95, 1.8, 0.5)\n#> [1] 2.622427"},{"path":"modelos.html","id":"mezcla-de-poblaciones-y-adición-de-variables-aleatorias","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.3.5 Mezcla de poblaciones y adición de variables aleatorias","text":"Vamos ilustrar con un ejemplo más completo la propiedad de la\naditividad de variables aleatorias normales. Es importante confundir\nla aditividad con la mezcla de poblaciones. En ambos casos el\nproblema al que nos enfrentamos puede estar referido una característica que\nse observa en dos grupos, y veces es difícil distinguir si tenemos que\nresolverlo mediante la probabilidad total y el teorema de Bayes, o mediante\nla suma de variables aleatorias. Para diferenciarlo, debemos entender bien\nel planteamiento del problema. Algunos indicios que nos ayudarán son:Mezcla de poblaciones: Hay dos o más grupos en los que se observan elementos\ntomados al azar. La característica tiene distinta distribución de probabilidad\nen cada grupo, pero la probabilidad de interés se refiere las poblaciones\nmezcladas (probabilidad total) o la probabilidad de pertenecer uno de los\ngrupos, condicionado que se ha producido algún evento de interés.Mezcla de poblaciones: Hay dos o más grupos en los que se observan elementos\ntomados al azar. La característica tiene distinta distribución de probabilidad\nen cada grupo, pero la probabilidad de interés se refiere las poblaciones\nmezcladas (probabilidad total) o la probabilidad de pertenecer uno de los\ngrupos, condicionado que se ha producido algún evento de interés.Suma de variables aleatorias: Hay dos o más variables aleatorias (que se pueden\nreferir grupos distintos, y de ahí la posible confusión con la mezcla de\npoblaciones). Pero lo que nos interesa es estudiar la variable aleatoria que\nresulta de hacer operaciones con esas variables aleatorias (por ejemplo, sumarlas).Suma de variables aleatorias: Hay dos o más variables aleatorias (que se pueden\nreferir grupos distintos, y de ahí la posible confusión con la mezcla de\npoblaciones). Pero lo que nos interesa es estudiar la variable aleatoria que\nresulta de hacer operaciones con esas variables aleatorias (por ejemplo, sumarlas).En el siguiente ejemplo se plantean preguntas que abordan los dos problemas.Una empresa de comercio minorista tiene tres tiendas (, B y C) en una determinada\nciudad. El tiempo que se tarda en atender un cliente\nse distribuye según una distribución exponencial de media 2 minutos, 4 minutos y\n5 minutos en las tiendas , B y C respectivamente. La tienda C atiende tantos\nclientes como y B juntas (que atienden al mismo número de clientes).\nSi llamamos \\(T_A\\), \\(T_B\\), \\(T_C\\)\nlas variables aleatorias “tiempo en ser atendido en la tienda , B, o C” respectivamente,\nentonces:\\[T_A \\sim \\mathit{Exp}(0.5),\\]\n\\[T_B \\sim \\mathit{Exp}(0.25),\\]\n\\[T_C \\sim \\mathit{Exp}(0.2).\\]Se considera que un cliente estará insatisfecho si se tarda más de 8 minutos en\natenderle.Por otra parte, las ventas\ndiarias de cada tienda, \\(V_A\\), \\(V_B\\) y \\(V_C\\),\nson independientes, y se distribuyen según una\ndistribución normal con los siguientes\nparámetros en miles de unidades monetarias (u.m.):\\[V_A \\sim N(\\mu = 100; \\sigma = 10),\\]\n\\[V_B \\sim N(\\mu = 150; \\sigma = 20),\\]\n\\[V_C \\sim N(\\mu = 140; \\sigma = 40).\\]Cuestion 1:¿Cuál es la probabilidad de que un cliente de la empresa esté satisfecho con el tiempo de servicio?Recibimos una queja de un cliente insatisfecho con el tiempo de servicio. ¿Cuál es la probabilidad de que sea un cliente de la tienda ?Cuestión 2:Las tiendas y B son propiedad 100% de la empresa. Pero de la tienda C la empresa\nrealmente solo recauda el 50%, ya que el otro 50% es de otro socio.\nPor otra parte, la empresa recibe unos ingresos fijos de 25.000 u.m. diarios\nde una tienda franquiciada en otra ciudad.¿Qué distribución de probabilidad siguen las ventas totales de la empresa, teniendo\nen cuenta su participación en las tiendas?¿Qué distribución de probabilidad siguen las ventas totales de la empresa, teniendo\nen cuenta su participación en las tiendas?¿Cuál es la probabilidad de que un día cualquiera esas ventas totales\nsea de menos de 300.000 u.m.?¿Cuál es la probabilidad de que un día cualquiera esas ventas totales\nsea de menos de 300.000 u.m.?Para resolver cada cuestión, tenemos que pensar si estamos anta una mezcla\nde poblaciones, o una suma de variables. Al estar los dos problemas planteados,\nes fácil de ver. Pero si solamente nos estuvieran preguntando por una de las dos\ncosas, pueden surgir dudas.La primera cuestión es un típico problema de probabilidad total y\nTeorema de Bayes en el que tenemos una partición del espacio muestral en tres\ntiendas, y conocemos las probabilidades priori. En cuanto al suceso de\ninterés (cliente insatisfecho), conocemos las distribuciones de\nprobabilidad de cada tienda, y tendremos\nque calcular las probabilidades condicionadas cada tienda.En la segunda cuestión lo que tenemos es una combinación lineal de variables\naleatorias, porque las ventas totales serán la suma de las ventas de las tiendas.\nAdemás, una de las variables estará multiplicada por un coeficiente, y tenemos\ntambién una constante que sumar.Pasemos entonces resolver cada cuestión.Cuestión 1.)Consideremos el suceso \\(D\\): el cliente está insatisfecho (espera más de 8 minutos).\nEntonces buscamo \\(P(D)\\). En la definición del problema tenemos las distribuciones\nde probabilidad del tiempo de espera, entonces podemos calcular:\\[P(D|) = P[T_A > 8] = 1 - F_{T_A}(8) = 1 - (1 - e^{-0.5\\cdot 8}) \\simeq 0.0183,\\]\n\\[P(D|B) = P[T_B > 8] = 1 - F_{T_B}(8) = 1 - (1 - e^{-0.25\\cdot 8}) \\simeq 0.1353,\\]\n\\[P(D|C) = P[T_C > 8] = 1 - F_{T_C}(8) = 1 - (1 - e^{-0.2\\cdot 8}) \\simeq 0.2019.\\]Del enunciado también podemos deducir la probabilidad de que un cliente tomado\nal azar sea cliente de cada una de las tiendas. Las únicas proporciones que suman\n1 y cumplen que la tercera es la suma de las otras dos, que son iguales, es la\nsiguiente:\\[P() = P(B) = 0.25; \\; P(C) = 0.5.\\]Entonces ya tenemos todos los datos para calcular la probabilidad del suceso \\(D\\):\\[P(D) = P(D|)P()+P(D|B)P(B)+P(D|C)P(C) =\\\\\n= 0.0183\\cdot 0.25 + 0.1353 \\cdot 0.25 + 0.2019 \\cdot 0.5 \\simeq \\boxed{0.1393}.\\]Cuestión 1.b)En este caso la probabilidad pedida es \\(P(|D)\\), que calculamos con la\nfórmula de Bayes, donde el denominador ya lo hemos calculado:\\[P(|D) = \\frac{P(D|)P()}{P(d)}= \\frac{0.0183\\cdot 0.25}{0.1393} \\simeq \\boxed{0.0329}. \\]Cuestión 2.)Ahora estamos mezclando poblaciones, sino sumando variables aleatorias. En\nconcreto, las ventas totales recaudadas por la empresa será una variable aleatoria\nque resulta de operar con las variables aleatorias \\(V_A\\), \\(V_B\\) y $V_C):\\[Y = 25 + V_A + V_B + 0.5\\cdot V_C.\\]Por la propiedad aditiva de la distribución normal, al ser variables independientes,\nesta variable sigue una distribución normal de parámetros:\\[\\mu_Y = 25 + 100 + 150 + 0.5\\cdot 140= 345,\\]\n\\[\\sigma_Y = \\sqrt{10^2 + 20^2 + 0.5^2\\cdot 40^2} = 30,\\]Y por tanto:\\[\\boxed{Y \\sim N(345, 30)},\\]Cuestión 2.b)Una vez tenemos la distribución de probabilidad, obtenemos\nla probabilidad de la manera habitual:\\[P[Y < 300] = P \\left[ \\frac{Y-\\mu_Y}{\\sigma_Y} < \\frac{300 -345}{30}\\right] = \\\\\nP[Z < -1.5] = P[Z > 1.5] \\simeq \\boxed{0.0668}.\\]Las probabilidades de este ejemplo se resuelven de forma análoga\nlos anteriores. Se deja como ejercicio para el lector comprobar\npor sí mismo los resultados ofrecidos través del programa\nde su elección.","code":""},{"path":"modelos.html","id":"otros-modelos-de-distribución-de-probabilidad","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.4 Otros modelos de distribución de probabilidad","text":"Los modelos vistos en este capítulo y el anterior\ncubren la mayoría de los problemas cotidianos\nde modelización. Existen otros modelos de distribución que se\naplican problemas específicos.\nPara finalizar este capítulo, se proporciona una breve descripción\nde las que aparecen en la norma ISO 3534-1.Distribución multinomial. Es el equivalente multivariante la\ndistribución binomial, donde solamente hay dos resultados posibles\nsino más de dos. Entonces tenemos un vector aleatorio con tantas\ncomponentes como clases posibles (resultados del experimento). Cada componente\ndel vector aleatorio sigue una distribución binomial.Distribución multinomial. Es el equivalente multivariante la\ndistribución binomial, donde solamente hay dos resultados posibles\nsino más de dos. Entonces tenemos un vector aleatorio con tantas\ncomponentes como clases posibles (resultados del experimento). Cada componente\ndel vector aleatorio sigue una distribución binomial.Distribución lognormal. Una variable lognormal, al transformarla\nmediante el logaritmo será una normal.Distribución lognormal. Una variable lognormal, al transformarla\nmediante el logaritmo será una normal.La distribución Gamma. Ya se ha comentado que es una generalización de la\ndistribución exponencial, y modeliza el tiempo hasta \\(k\\) eventosLa distribución Gamma. Ya se ha comentado que es una generalización de la\ndistribución exponencial, y modeliza el tiempo hasta \\(k\\) eventosLa distribución Beta. Es muy útil para modelizar proporciones y probabilidades.La distribución Beta. Es muy útil para modelizar proporciones y probabilidades.La distribución de Weibull. También se utiliza para modelizar tiempos\nde vida, y es muy flexible describir formas muy diferentes de la distribución\nmediante el ajuste de sus parámetros. Es también una distribución de valores\nextremos (tipo III). La norma incluye otras dos distribuciones de valores extremos:\nTipo (Gumbel) y Tipo II (Fréchet).La distribución de Weibull. También se utiliza para modelizar tiempos\nde vida, y es muy flexible describir formas muy diferentes de la distribución\nmediante el ajuste de sus parámetros. Es también una distribución de valores\nextremos (tipo III). La norma incluye otras dos distribuciones de valores extremos:\nTipo (Gumbel) y Tipo II (Fréchet).La distribución normal multivariante. Se aplica vectores aleatorios\ndonde todas sus componentes son variables aleatorias normales.La distribución normal multivariante. Se aplica vectores aleatorios\ndonde todas sus componentes son variables aleatorias normales.La distribución multinomial. Se aplica características cualitativas multiclase.La distribución multinomial. Se aplica características cualitativas multiclase.En el apéndice B se puede encontrar un resumen de todas\nlas distribuciones de probabilidad y sus principales\ncaracterísticas.","code":""},{"path":"modelos.html","id":"convergencia","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.5 Convergencia de variables aleatorias","text":"","code":""},{"path":"modelos.html","id":"distribuciones-relacionadas-con-la-normal","chapter":"Capítulo 7 Modelos de distribución de probabilidad","heading":"7.6 Distribuciones relacionadas con la normal","text":"","code":""},{"path":"muestreo.html","id":"muestreo","chapter":"Capítulo 8 Muestreo y estimación","heading":"Capítulo 8 Muestreo y estimación","text":"En preparación.Muestreo estadísticoEstimación y contrastesEstadísticosEstimadores puntuales (medias, proporciones, varianzas)Estimación por intervalosEstimación paramétricaInferencia Bayesiana*","code":""},{"path":"comparacion2.html","id":"comparacion2","chapter":"Capítulo 9 Comparación de dos grupos","heading":"Capítulo 9 Comparación de dos grupos","text":"En preparación.Comparación de atributosComparación de dos gruposComparación de más de dos grupos : remitir ANOVA","code":""},{"path":"anova.html","id":"anova","chapter":"Capítulo 10 Análisis de la Varianza","heading":"Capítulo 10 Análisis de la Varianza","text":"","code":""},{"path":"anova.html","id":"introducción-2","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.1 Introducción","text":"El análisis de la varianza es una técnica estadística de análisis de dependencias,\ndonde se busca explicar una o varias variables cuantitativas partir de una\no varias variables cualitativas o factores. Es decir, buscamos un modelo\ndel tipo:\\[\\begin{equation}\n\\pmb{Y}=f(\\pmb{X}) + \\varepsilon\n\\tag{10.1}\n\\end{equation}\\]donde \\(\\pmb{Y}\\) es un vector de variables respuesta numéricas que queremos\nexplicar o predecir, y \\(\\pmb{X}\\) es un vector de variables predictivas\ncualitativas con las que pretendemos explicar las variables respuesta. Como\ntodo modelo estadístico, está sujeto un error \\(\\varepsilon\\).En realidad el análisis de la varianza incluye un conjunto amplio de\ntécnicas cuyo análisis varía ligeramente según la naturaleza y el número de variables\nen los vectores aleatorios \\(\\pmb{Y}\\) y \\(\\pmb{X}\\). En los apartados siguientes\nse irán detallando los distintos modelos desde el más sencillo al más complejo.\nmedida que se avance en los modelos, se presentan más ejemplos y menos teoría,\nya que el fundamento es muy similar y se puede consultar en la\nbibliografía citada.La técnica del análisis de la varianza se puede abordar desde dos perspectivas:\nexplicativa y predictiva. Desde una perspectiva explicativa, se puede aplicar\nla técnica para realizar estudios observacionales datos ya existentes.\nEstos estudios observacionales confirmarán la relación entre las variables.\nDesde el punto de vista predictivo, se pueden diseñar experimentos antes\nde la recogida de datos. Estos estudios predictivos permiten confirmar la\nrelación de causa-efecto entre las variables.","code":""},{"path":"anova.html","id":"sec:anova1","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.2 Análisis de la varianza de un factor","text":"El caso más sencillo que podemos aplicar al modelo general (10.1),\nes aquel en el que tenemos una única variable continua en el vector aleatorio\n\\(\\pmb{Y}\\) y una sola variable cualitativa (factor) en el vector aleatorio \\(\\pmb{X}\\)\ncon \\(k>2\\) niveles.\nCuando la variable cualitativa tiene solamente dos posibles niveles, podemos\nutilizar simplemente contrastes de hipótesis para la comparación de poblaciones\nmediante test paramétricos como el de la \\(t\\) de Student, o paramétricos como\nel test de Wilcoxon, o el test de Wilcoxon-Mann-Whitney. Una granja experimental quiere estudiar el efecto que tiene\nel tipo de fertilizante utilizado en el cultivo de una determinada variedad de plantas y su peso en su punto óptimo de recolección. Para ello diseña un experimento en el que selecciona doce semillas aleatoriamente de un determinado lote.\nSe asigna aleatoriamente cada semilla una maceta. Y cada maceta, se le asigna también aleatoriamente un tipo de fertilizante entre tres varieaddes: , B y C. El peso de cada planta en gramos se recoge en la tabla 10.1, que están guardados en el data frame\ndanova70.\nVamos utilizar este ejemplo lo largo de este capítulo.El primer paso en toda técnica\nestadística es hacer un análisis exploratorio. Como son muy pocos puntos por cada\ngrupo, vamos obtener un resumen numérico y representarlos todos con un gráfico de puntos (figura 10.1).la vista de las medias parece que el peso medio con el fertilizante C es menor.\nPor otra parte parece que hay menos variabilidad\ncon también con el fertilizante C. Una vez ajustado el modelo lo comprobaremos numéricamente.Tabla 10.1: Peso de la planta su recogida\nFigura 10.1: Gráfico de puntos del experimento en plantas\n","code":"\nlibrary(tidyverse)\ndanova |> \n  group_by(Fertilizante) |> \n  summarise(Peso_medio = mean(Peso),\n            Desv.Tipica = sd(Peso)) \n#> # A tibble: 3 × 3\n#>   Fertilizante Peso_medio Desv.Tipica\n#>   <fct>             <dbl>       <dbl>\n#> 1 A                  143.       28.6 \n#> 2 B                  125.       20.6 \n#> 3 C                  110.        8.77\ndanova |> \n  ggplot(aes(x = Fertilizante, \n             y = Peso)) +\n  geom_point(alpha = 0.5,\n             col = \"orangered\")"},{"path":"anova.html","id":"modelo","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.2.1 Modelo","text":"Tenemos una variable \\(Y\\) que toma valores reales y una variable cualitativa o factor\n\\(X\\) con niveles \\(1, 2, \\ldots, , \\ldots, k\\). La variable \\(Y\\) toma valores \\(y_{ij}\\), \\(j = 1, \\ldots, n_i\\)\nen el nivel \\(\\) del factor \\(X\\), siendo \\(n_i\\) el número de observaciones en el nivel \\(\\) del factor \\(X\\).\nCuando todos los niveles tienen el mismo número de observaciones, \\(n_i=n_{^\\prime}\\; \\forall ,^\\prime\\), decimos\nque el diseño está balanceado o equilibrado.\nEl modelo puede escribirse de dos formas:\\[\\begin{equation}\ny_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij},\n\\tag{10.2}\n\\end{equation}\\]\\[\\begin{equation}\ny_{ij} = \\mu_i + \\varepsilon_{ij},\n\\tag{10.3}\n\\end{equation}\\]En la ecuación (10.2), \\(\\mu\\) es la media de la variable \\(Y\\),\nmientras que \\(\\alpha_i\\) es el efecto en la media del nivel \\(\\), es decir, cuánto\naumenta o disminuye la media de \\(Y\\) por pertenecer la categoría \\(\\). En la ecuación\n(10.3), \\(\\mu_i\\) es la media de la variable \\(Y\\) para el nivel \\(\\)\ndel factor \\(X\\), de donde tenemos que el efecto es:\\[\\alpha_i = \\mu_i - \\mu,\\]y el término de error, que representa toda la variabilidad que explica el modelo, es:\\[\\varepsilon_{ij}=y_{ij}-\\mu_i.\\]Se cumple que:\\[\\sum_i \\alpha_i = 0;\\; \\varepsilon_{ij} \\sim N(0, \\sigma^2)\\]Podríamos representar matemáticamente nuestro ejemplo como:\n\\[\\mathit{Peso} = \\mu + \\alpha_{\\mathit{Fertillizante}} + \\varepsilon,\\]\no bien como:\n\\[\\mathit{Peso} = \\mu_{\\mathit{Fertilizante}} + \\varepsilon.\\]El modelo ANOVA se ajusta en R con la función aov. El siguiente código ajusta\nel modelo de nuestro ejemplo, pero de momento solo lo guardamos, veremos\nen los siguientes apartados cómo extraer información e interpretarla.","code":"\nmodelo.aov <- aov(Peso ~ Fertilizante, danova)"},{"path":"anova.html","id":"estimación-de-los-parámetros","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.2.2 Estimación de los parámetros","text":"Si tenemos un total\nde \\(n\\) datos de los cuales hay \\(n_i\\) de cada nivel \\(\\), de forma que \\(\\sum_i n_i = n\\),\nlos estimadores obtenidos tanto por el método de mínimos cuadradados como por\nel método de máxima verosimilitud (véase por ejemplo John Lawson71) son los siguientes:\\[\\hat{\\mu_i} = \\overline{y}_{\\cdot}= \\frac{\\sum\\limits_{j=1}^{n_i}{y_{ij}}}{n_i},\\]\n\\[\\hat{\\mu} = \\overline{y}_{\\cdot\\cdot}= \\frac{\\sum\\limits_{=1}^{k}{y_{\\cdot}}}{k}=\\frac{\\sum\\limits_{=1}^k\\sum\\limits_{j=1}^{n_i}{y_{ij}}}{n},\\]\\[\\hat{\\alpha}_i=\\hat{\\mu}_i-\\hat{\\mu},\\]es decir, las medias dentro de cada nivel del factor, y la media total. Con estos estimadores de los parámetros, la\nestimación de valores de \\(Y\\) vendrá dada por:\\[\\hat{y}_{ij}=\\hat{\\mu_i} = \\hat{\\mu}+ \\hat{\\alpha_i},\\]\ny por tanto los residuos del modelo son:\\[e_{ij}=y_{ij} - \\hat{y}_{ij}\\]Nótese que, si tenemos \\(k\\) niveles solo tenemos que estimar \\(k-1\\), ya que:\\[\\sum_i \\alpha_i = 0.\\]Esta última restricción hace que se puedan estimar los efectos con la\nrepresentación matricial completa:\\[\\pmb{y} = \\pmb{X}\\pmb{\\beta}+\\pmb{\\varepsilon}\n\\tag{10.4}\\]\\[\\left( \\begin{array}{c} \ny_{11} \\\\ \n\\vdots\\\\\ny_{1 n_1}\\\\\ny_{21}\\\\\n\\vdots\\\\\ny_{2n_2}\\\\\n\\vdots\\\\\ny_{k1}\\\\\n\\vdots\\\\\ny_{kn_k}\n\\end{array} \\right )  =\n\\left (\n\\begin{array}{ccccc}\n1 & 1 & 0 &\\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n1 & 1 & 0 & \\cdots & 0\\\\\n1 & 0 & 1 &\\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & 0 \\\\\n1 & 0 & 1 &\\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & 0 & 0 &\\cdots & 1\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & 0 & 0 &\\cdots & 1\n\\end{array}\n\\right )\n\\left(\n\\begin{array}{c}\n\\mu\\\\\n\\alpha_1\\\\\n\\alpha_2\\\\\n\\vdots\\\\\n\\alpha_k\n\\end{array}\n\\right ) +\n\\left(\n\\begin{array}{c}\n\\varepsilon_{11} \\\\ \n\\vdots\\\\\n\\varepsilon_{1 n_1}\\\\\n\\varepsilon_{21}\\\\\n\\vdots\\\\\n\\varepsilon_{2n_2}\\\\\n\\vdots\\\\\n\\varepsilon_{k1}\\\\\n\\vdots\\\\\n\\varepsilon_{kn_k}\n\\end{array}\n\\right )\\]Debido la singularidad de la\nmatriz \\(\\pmb{X}^T \\pmb{X}\\) (véase por ejemplo Lawson72),\nlo que se hace es fijar\nuno de los niveles del factor como nivel “base,” y estimar\nla media del nivel base y los efectos de los otros niveles con respecto la media del nivel\nbase, es decir:\\[\\hat\\beta = \n\\left(\n\\begin{array}{c}\n\\hat\\mu + \\hat\\alpha_1\\\\\n\\hat\\alpha_2 - \\hat\\alpha_1\\\\\n\\hat\\alpha_3 -  \\hat\\alpha_1\n\\end{array}\n\\right)\\]El nivel base que se toma en R de un factor ordenado es el primero en orden alfabético,\ny se puede cambiar con la función relevel.Sobre el objeto modelo.aov que guardamos antes, podemos aplicar funciones genéricas\nque devuelvan ciertos resultados. Por ejemplo, la función coef nos devuelve los estimadores de los\ncoeficientes, teniendo en cuenta que toma como nivel de referencia del factor Fertilizante.\nPodemos comprobar cómo se corresponden los coeficientes con los efectos estimados.\nLa función confint nos\ndevuelve un intervalo de confianza para los parámetros. Vemos que, con respecto al nivel\nbase , el Peso medio de la planta se reduce en casi 18 gramos cuando el fertilizante es el B\ny más de 33 cuando es el C.\nSe pueden visualizar los efectos con el paquete effects, como se muestra en\nla figura 10.2.\nFigura 10.2: Visualización de los efectos\n","code":"\n## El primer nivel es el de referencia\nlevels(danova$Fertilizante)\n#> [1] \"A\" \"B\" \"C\"\n## Estimación de los coeficientes\ncoef(modelo.aov)\n#>   (Intercept) FertilizanteB FertilizanteC \n#>      143.1667      -17.9500      -33.5500\n## Efecto nivel 1\na1 <- coef(modelo.aov)[1] - mean(danova$Peso); a1\n#> (Intercept) \n#>    17.16667\n## Efecto nivel 2\na2 <- coef(modelo.aov)[2] + a1; a2\n#> FertilizanteB \n#>    -0.7833333\n## Efecto nivel 3\na3 <- coef(modelo.aov)[3] + a1; a3\n#> FertilizanteC \n#>     -16.38333\n## Comprobación\ndanova |> \n  group_by(Fertilizante) |> \n  summarise(Medias = mean(Peso)) |> \n  mutate(Efectos = Medias - mean(danova$Peso))\n#> # A tibble: 3 × 3\n#>   Fertilizante Medias Efectos\n#>   <fct>         <dbl>   <dbl>\n#> 1 A              143.  17.2  \n#> 2 B              125.  -0.783\n#> 3 C              110. -16.4\n# aggregate(Peso ~ Fertilizante, danova, mean)$Peso - mean(danova$Peso)\n## Intervalo de confianza\nconfint(modelo.aov, alpha = 0.99)\n#>                   2.5 %     97.5 %\n#> (Intercept)   124.89704 161.436297\n#> FertilizanteB -43.78716   7.887158\n#> FertilizanteC -59.38716  -7.712842\nplot(effects::effect(\"Fertilizante\", modelo.aov))"},{"path":"anova.html","id":"contrastes","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.2.3 Contrastes","text":"En el análisis de la varianza de un factor, lo que nos interesa demostrar es\nque hay diferencias entre las medias de \\(Y\\) para distintos niveles del factor \\(X\\) (la \\(X\\) explica la \\(Y\\)). Si \nhubiera diferencias entre los niveles, entonces las medias \\(\\mu_i\\) serían iguales,\no lo que es lo mismo, los efectos \\(\\alpha_i\\) serían nulos. Por tanto,\nla hipótesis nula del modelo ANOVA de un factor es:\\[H_0: \\mu_1 = \\mu_2 = \\cdots=\\mu_k,\\]\no equivalentemente:\\[H_0: \\alpha_i = 0 \\quad \\forall .\\]Nótese que la hipótes alternativa es que hay diferencia entre los niveles,\npero eso significa que todos los niveles sean diferentes. Es decir, si\nrechazamos la hipótesis nula, al menos dos grupos serán distintos. Y por tanto tenemos evidencia para aceptar la\nalternativa:\\[H_1: \\alpha_i \\neq 0 \\text{ para algún } ,\\]Para contrastar la hipótesis nula, dividimos la variabilidad total de los datos\nentre la variabilidad que existe “dentro” de los grupos y la variabilidad\nque existe “entre” los grupos. Esta variabilidad la representamos por las\nsumas de cuadrados, de manera que la suma de cuadrados total (\\(SCT\\))\nla podemos descomponer en la suma de cuadrados entre grupos (\\(SCE\\)) más la suma de cuadrados\ndentro de grupos (\\(SCD\\)):\\[\\sum\\limits_{ij} (y_{ij} - \\overline{y}_{\\cdot\\cdot})^2 = \\sum\\limits_{}n_i(\\overline{y}_{\\cdot}-\\overline{y}_{\\cdot\\cdot})^2 + \\sum\\limits_{ij}(y_{ij}-\\bar y_{\\cdot})^2,\\]\n\\[SCT = SCE + SCD.\\]Los grados de libertad de la suma de datos total es \\(n-1\\), que se descomponen también en \\(k-1\\) grados de libertad\npara la suma de cuadrados entre grupos y \\(n-k\\) grados de libertad para la suma\nde cuadrados dentro de los grupos, de forma que podemos calcular los cuadrados medios totales,\nentre grupos y dentro de grupos:\\[CMT=\\frac{\\sum\\limits_{ij} (y_{ij} - \\overline{y}_{\\cdot\\cdot})^2 }{n-1},\\]\n\\[CME=\\frac{\\sum\\limits_{}n_i(\\overline{y}_{\\cdot}-\\overline{y}_{\\cdot\\cdot})^2 }{k-1},\\]\n\\[CMD=\\frac{\\sum\\limits_{ij}(y_{ij}-\\bar y_{\\cdot})^2 }{n-k},\\]Entonces, si se cumplen las condiciones para aplicar el modelo y la hipótesis nula es cierta,\nel estadístico:\\[F = \\frac{CME}{CMD}\\]\nsigue una distribución \\(F\\) con \\(k-1\\) y \\(n-k\\) grados de libertad, y podemos bien realizar\nel contraste de hipótesis para un nivel de confianza determinado, bien interpretar el\np-valor, para llegar una conclusión o decisión. En Análisis de la Varianza aquí descrito\nse resume normalmente en la llamada tabla ANOVA (ver tabla 10.2), que incluye\nlas sumas de cuadrados, cuadrados medios, estadístico F y el p-valor.Tabla 10.2: Contenido de la tabla ANOVALa función genérica summary aplicada un modelo aov devuelve precisamente la tabla ANOVA.\nEn nuestro ejemplo, el p-valor es pequeño, menor de 0.05 (aunque por poco). Luego para\nun nivel de significación del 5% podemos rechazar la hipótesis nula y aceptamos\nque hay diferencias en las medias73.Como se ha indicado anteriormente, al rechazar la hipótesis nula aceptamos que\ntodos los niveles del factor producen una misma respuesta en la variable\n\\(Y\\). Una vez rechazada la hipótesis nula, tendremos que comprobar qué niveles son realmente distintos para, en última\ninstancia, sacar conclusiones o tomar decisiones. Un enfoque erróneo sería\nrealizar comparaciones con el test de la t de Student para cada par de\nniveles del factor. En su lugar, utilizamos el método HSD74 de Tukey,\nque utiliza el estadístico de los rangos estudentizados, \\(R/\\hat{\\sigma}\\) para\nrealizar todos los contrastes siguientes:\\[H_0: \\mu_i = \\mu_j \\quad \\forall \\neq j.\\]La función TukeyHSD realiza los contrastes dos dos de un modelo ANOVA, como en el siguiente\nejemplo. La salida proporciona las diferencias entre cada par de niveles, un intervalo de confianza\ny el p-valor del contraste. Vemos que hay diferencias significativas entre los fertilizantes y C, pero en el resto de comparaciones. Estas diferencias se pueden visualizar utilizando\nla función plot, que produce la figura 10.3.\nFigura 10.3: Visualización de las diferencias por pares\nRecordemos que en un estudio observacional si se rechaza la hipótesis nula estamos confirmando\nque existe relación entre el factor y la variable. Para confirmar la relación causa-efecto del factor sobre la variable,\nel ANOVA de un factor debe realizarse partir de un experimento diseñado correctamente,\nvéase ??.","code":"\nsummary(modelo.aov)\n#>              Df Sum Sq Mean Sq F value Pr(>F)  \n#> Fertilizante  2   3382  1691.2   3.836 0.0451 *\n#> Residuals    15   6612   440.8                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(modelo.aov)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = Peso ~ Fertilizante, data = danova)\n#> \n#> $Fertilizante\n#>       diff      lwr       upr     p adj\n#> B-A -17.95 -49.4362 13.536201 0.3274991\n#> C-A -33.55 -65.0362 -2.063799 0.0361280\n#> C-B -15.60 -47.0862 15.886201 0.4236517\nplot(TukeyHSD(modelo.aov))"},{"path":"anova.html","id":"validación-del-modelo-y-alternativas","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.2.4 Validación del modelo y alternativas","text":"Las hipótesis del test de la \\(F\\) y de los tests HSD de Tukey son la igualdad\nde varianzas entre grupos, y la normalidad de los residuos. La igualdad de varianzas\nse pueden comprobar fácilmente con el test de Bartlett75. La normalidad de los residuos\nse puede verificar con alguno de los múltiples tests de normalidad existentes,\ncomo por ejemplo el de Kolmogorov-Smirnoff, el de Shapiro-Wilk o el de\nAnderson-Darling. La función genérica plot de R sobre un modelo ANOVA guardado\nproduce una serie de gráficos de diagnóstico que nos dan una idea veces suficiente\ndel cumplimiento de las hipótesis. La función autoplot() del paquete {ggfortify}76 los realiza con {ggplot2}.La función genérica residuals() sobre el objeto modelo.aov\ndevuelve los residuos del modelo. Podemos entonces hacer un contraste\nde normalidad. El p-valor es grande, mucho mayor de \\(0.05\\), por lo\nque podemos rechazar que los residuos sean normales.\nLa función bartlett.test() contrasta la hipótesis de\nhomogeneidad de varianzas.\nEl p-valor de este contraste es grande, mayor de 0.05 (aunque mucho mayor). Podemos aceptar\nla igualdad de varianzas y por tanto las conclusiones de las pruebas de Fisher y de Tukey son\nválidas.La figura 10.4 muestra los gráficos de diagnóstico generados con la\nfunción ggfortify::autoplot(). El gráfico superior izquierdo deberia mostrar una línea recta\nsi el modelo lineal se ajusta, y también se aprecia la homogeneidad de varianzas.\nmenudo cuando se cumple esta hipótesis el gráfico muestra forma de embudo.\nEl gráfico inferior izquierdo muestra la misma información pero otra escala,\ndonde tampoco se aprecian varianzas distintas.\nEl gráfico superior derecho es un gráfico cuantil-cuantil para comprobar\nla normalidad de los residuos, en este caso se ajusta bastante bien.\nEl gráfico inferior derecho sirve para detectar observaciones con gran influencia\nen el modelo. En este caso se etiqueta ninguna.\nFigura 10.4: Gráficos de diagnóstico modelo ANOVA\nEn caso de cumplimiento de las hipótesis, podemos realizar contrastes\nparamétricos. Para el contraste de hipótesis de igualdad entre los niveles del factor,\nutilizamos el contraste de Kruskal-Wallis, que también dispone de un método\npara realizar comparaciones múltiples, como veremos en los ejemplos. Es\nimportante tener en cuenta que la función kruskal.test() de R requiere\nque la variable cualitativa sea de tipo factor y carácter.Otra alternativa es transformar los datos originales para conseguir normalidad\ny/o homogeneidad de varianzas\ny realizar el análisis con los datos transformados.En el conjunto de datos hay una variable que hemos usado. Es el\nfactor Tierra, que tiene tres niveles (tabla 10.3.\nSe deja al lector como ejercicio realizar un análisis exploratorio de los\ndatos.\nAjustamos el modelo ANOVA\npara estudiar el efecto de este factor en el Peso de la planta. Toma los valores Z1, Z2, Z3, según la zona de origen de la tierra utilizada en la maceta. En este caso,\nel contraste de hipótesis permite rechazar la igualdad de medias.\nobstante, al validar las hipótesis del modelo77 vemos que se cumple\nla hipótesis de normalidad, por lo que hay que hacer el contraste de Kruskal-Wallis\npara confirmar que la Tierra explica el Peso. Obtenemos\nun p-valor muy grande, por lo que podemos rechazar que los datos\nvengan de la misma población, y por tanto llegamos la conclusión\nde que hay diferencias. Aunque en este caso sería necesario seguir,\npor completitud del ejemplo vamos realizar las comparaciones por pares\nutilizando la función kruskalmc() del paquete {pgirmess}.78 Obtenemos en\neste caso las diferencias observadas y críticas (para un determinado nivel de\nsignificación) y un indicador (TRUE/FALSE) de diferencia significativa.\nComo era de esperar, se encuentra ningún par de niveles con\ndiferencias significativas.Tabla 10.3: Peso de las plantas y origen de la tierra","code":"\nshapiro.test(residuals(modelo.aov))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(modelo.aov)\n#> W = 0.92539, p-value = 0.161\nbartlett.test(Peso ~ Fertilizante, data = danova)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  Peso by Fertilizante\n#> Bartlett's K-squared = 5.3317, df = 2, p-value =\n#> 0.06954\nlibrary(ggfortify)\nautoplot(modelo.aov)\nmodelo.aov2 <- aov(Peso ~ Tierra, danova)\nsummary(modelo.aov2)\n#>             Df Sum Sq Mean Sq F value Pr(>F)\n#> Tierra       2     43    21.6   0.033  0.968\n#> Residuals   15   9951   663.4\nshapiro.test(residuals(modelo.aov2))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(modelo.aov2)\n#> W = 0.84513, p-value = 0.007103\nbartlett.test(Peso ~ Tierra, danova)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  Peso by Tierra\n#> Bartlett's K-squared = 3.4019, df = 2, p-value =\n#> 0.1825\nkruskal.test(Peso ~ Tierra, danova)\n#> \n#>  Kruskal-Wallis rank sum test\n#> \n#> data:  Peso by Tierra\n#> Kruskal-Wallis chi-squared = 0.5653, df = 2, p-value\n#> = 0.7538\npgirmess::kruskalmc(Peso ~ Tierra, data = danova)\n#> Multiple comparison test after Kruskal-Wallis \n#> p.value: 0.05 \n#> Comparisons\n#>         obs.dif critical.dif difference\n#> Z1-Z2 1.9444444     6.735838      FALSE\n#> Z1-Z3 0.1666667     9.037076      FALSE\n#> Z2-Z3 1.7777778     8.520237      FALSE"},{"path":"anova.html","id":"sec:anova2","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.3 Análisis de la varianza de varios factores","text":"La principal diferencia con el análisis de la varianza de un factor es que,\nademás de los efectos principales de cada uno de los factores, es decir,\ncuánto varía la media según los niveles del factor, se puede estudiar el\nefecto de las interacciones entre factores. Intuitivamente, la interacción\nentre factores es similar la que podemos observar en la vida diaria, por ejemplo un tranquilizante tiene un efecto positivo sobre el bienestar de una persona.\nUna copa de vino en determinadas circunstancias también. Pero utilizados conjuntamente, producen\nuna interacción que afecta negativamente en el bienestar de la persona.\nDel mismo modo, podemos observar que una variable dependiente \\(Y\\) toma\nmejores valores para ciertos niveles de los factores \\(X_1\\) y \\(X_2\\). Pero\nse deben estudiar las interacciones, porque puede ser que dichos niveles\ncombinados produzcan peor resultado.El modelo de dos factores, que en la literatura en inglés de encuentra\ncomo two-way anova, es el siguiente:\\[Y = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}+\\varepsilon,\\]donde \\((\\alpha\\beta)_{ij}\\) representa el efecto de la interacción,\ny el resto de términos tienen la misma interpretación que en el ANOVA\nde un factor (o one-way anova). En la tabla ANOVA se añaden nuevas\nfilas y contrastes para los efectos principales y las interacciones.\nLas hipótesis del modelo son las mismas, luego comprobamos normalidad\nde los residuos y homogeneidad de variazas. Para este último caso,\nutilizamos mejor el test de Levene que permite incluir el término de la interacción.En el ajuste del modelo y estimación de efectos, se toma como base\nel primer nivel de todos los factores.Para especificar modelos con más de un factor e interacción en R, ampliamos\nel lado derecho de la fórmula del modelo. Los nuevos efectos se añaden\n“sumando.” La interacción se expresa separando los factores con dos puntos.\nSi utilizamos el símbolo asterisco, entonces el modelo incluye todos los efectos\nprincipales y las interacciones. Por ejemplo, para dos factores y b,\nel modelo completo se puede expresar como *b o como + b + :b.En el apartado anterior hemos analizado por separado el Peso frente los factores\nFertilizante y Tierra. Pero deberíamos analizarlos en un modelo multifactorial.Las siguientes expresiones crean el modelo multifactorial, realizan los contrastes, calcula los efectos y representa las interacciones.\nFigura 10.5: Visualización de las interacciones entre iluminación y Tierra.\nLa tabla ANOVA nos muestra que el término de la interacción es altamente significativo,\ncon un p-valor muy bajo, incluso inferior \\(0.01\\). También el tipo de fertilizante.\nlo es el efecto principal de la Tierra, pero como este factor está en una\ninteracción significativa, deberíamos eliminarlo del modelo.Las hipótesis del modelo y el modo\nde proceder son análogos al caso unifactorial. Se verifica la normalidad de\nlos residuos y la homegenidad de varianzas. El mayor Peso medio de la planta se consigue con la combinación fertilizante y Tierra Z2.El gráfico de las interacciones (figura 10.5) muestra\nclaramente las grandes diferencias entre las combinaciones de factores.\nSi hubiera interacción, las líneas serían paralelas.\nEn el apartado anterior habíamos llegado la conclusión de que\nel mejor fertilizante para conseguir el peso más alto era el fertilizante “,” pero era indiferente utilizar tierra de una zona u otra.\nSi analizáramos las interacciones, podríamos cometer el error\nde utilizar para las plantas fertilizante y tierra de la zona Z3,\nlo que sería una pésima decisión ya que esta interacción baja drásticamente\nel peso de las plantas.El caso de dos factores con interacción se extiende fácilmente más de dos factores\nañadiendo términos la fórmula. obstante, las interacciones de más de dos\nfactores se suele despreciar. En el modelo resultante final se deberían eliminar\nlos efectos significativos, aunque manteniendo aquellos efectos principales\nque intervengan en alguna interacción.Si se cumplieran las hipótesis del modelo podríamos usar el contraste de\nKruskal-Wallis para los efectos principales. Para la interacción también hay\nmétodos paramétricos, aunque están tan extendidos, véase una revisión\nen Jos Feys.79 En muchas ocasiones una transformación de\nBox-Cox en la variable respuesta es suficiente para ajustar un modelo\nválido, véase ??.","code":"\nmodelo.aov3 <- aov(Peso ~ Fertilizante*Tierra, danova)\nsummary(modelo.aov3)\n#>                     Df Sum Sq Mean Sq F value  Pr(>F)   \n#> Fertilizante         2   3382  1691.2   9.674 0.00572 **\n#> Tierra               2     43    21.6   0.124 0.88521   \n#> Fertilizante:Tierra  4   4996  1248.9   7.145 0.00712 **\n#> Residuals            9   1573   174.8                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nshapiro.test(residuals(modelo.aov3))\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  residuals(modelo.aov3)\n#> W = 0.90975, p-value = 0.08516\ncar::leveneTest(Peso ~ Fertilizante*Tierra, danova)\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)\n#> group  8  0.5554 0.7902\n#>        9\ncoef(modelo.aov3)\n#>            (Intercept)          FertilizanteB \n#>              126.60000               22.20000 \n#>          FertilizanteC               TierraZ2 \n#>              -20.30000               37.50000 \n#>               TierraZ3 FertilizanteB:TierraZ2 \n#>              -13.10000              -77.56667 \n#> FertilizanteC:TierraZ2 FertilizanteB:TierraZ3 \n#>              -37.76667               -8.20000 \n#> FertilizanteC:TierraZ3 \n#>               33.80000\neffects::allEffects(modelo.aov3)\n#>  model: Peso ~ Fertilizante * Tierra\n#> \n#>  Fertilizante*Tierra effect\n#>             Tierra\n#> Fertilizante    Z1       Z2    Z3\n#>            A 126.6 164.1000 113.5\n#>            B 148.8 108.7333 127.5\n#>            C 106.3 106.0333 127.0\nwith(danova,\n     interaction.plot(x.factor = Fertilizante,\n                      trace.factor = Tierra,\n                      response = Peso, \n                      las = 1))"},{"path":"anova.html","id":"introducción-a-los-modelos-mixtos-efectos-fijos-y-efectos-aleatorios","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.4 Introducción a los modelos mixtos: efectos fijos y efectos aleatorios","text":"El modelo de análisis de la varianza y los contrastes\nvistos hasta ahora asumen que los individuos se han\nasignado aleatoriamente los distintos niveles.\nIncluso en estudios observacionales, podemos asumir\nque esta asignación se ha hecho de forma aleatoria y controlada,\ny son efectos fijos.Pero esto siempre se puede asumir, o directamente la\nnaturaleza del propio factor es aleatoria, y el tratamiento\nque le tenemos que dar los factores es distinto. En particular,\nlos efectos de estos factores son una constante que queramos\nestimar, sino variables aleatorias de las cuales queremos estudiar\nsu varianza en el modelo. El modelo para un factor fijo \\(\\alpha\\) y otro aleatorio\n\\(\\beta\\) sería el siguiente:\\[Y = \\mu + \\alpha_i + \\beta_j + \\varepsilon,\\]donde el efecto aleatorio \\(\\beta \\sim N(0, \\sigma^2_\\beta)\\). tiene\npor tanto sentido estimar el efecto, cuya media es cero, sino la variabilidad,\ny ver si es importante con respecto al resto de factores.Imaginemos por un momento que la variable Tierra de nuestro ejemplo se refiera la tierra de una parcela determinada que hemos podido elegir, sino que es algo aleatorio (por ejemplo en granjas experimentales distantes). Está claro que este factor es algo\nque podamos controlar, y por tanto su efecto es aleatorio.\nEn este caso tendríamos que ajustar un modelo mixto.\nEn este ejemplo vemos que el factor Tierra afecta poco o nada la variable\nrespuesta, ya que la varianza es prácticamente nula80. Podemos obtener una estimación de los efectos\ncon la función ranef del paquete lme4.Los modelos mixtos (o incluso aleatorios puros) tienen muchas aplicaciones\ntanto en modelos biológicos como en cualquier otro ámbito. Algunos ejemplos son:Efectos aleatorios puros, como el que se ha mostrado en el ejemplo.Efectos aleatorios puros, como el que se ha mostrado en el ejemplo.Modelos de panel, donde tenemos mediciones en diferentes periodos,\ny la unidad observable dentro del tiempo forman el factor aleatorioModelos de panel, donde tenemos mediciones en diferentes periodos,\ny la unidad observable dentro del tiempo forman el factor aleatorioMedidas repetidas, donde el individuo es el factor aleatorioMedidas repetidas, donde el individuo es el factor aleatorioModelos anidados y jerarquizados, donde unos niveles están dentro de otrosModelos anidados y jerarquizados, donde unos niveles están dentro de otrosComo se ha podido comprobar, el análisis de\nmodelos mixtos es tan sencillo como el de efectos fijos. Para una\nrevisión más completa se recomienda consultar el libro de J. J. Faraway.81","code":"\nlibrary(lme4)\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\nmodelo.mixto <- lmer(Peso ~ 1 + Fertilizante + (1|Tierra), danova)\n#> boundary (singular) fit: see ?isSingular\nsummary(modelo.mixto)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: Peso ~ 1 + Fertilizante + (1 | Tierra)\n#>    Data: danova\n#> \n#> REML criterion at convergence: 139.3\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.4130 -0.4437 -0.1580  0.5838  1.6924 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Tierra   (Intercept)   0.0     0      \n#>  Residual             440.8    21      \n#> Number of obs: 18, groups:  Tierra, 3\n#> \n#> Fixed effects:\n#>               Estimate Std. Error t value\n#> (Intercept)    143.167      8.571  16.703\n#> FertilizanteB  -17.950     12.122  -1.481\n#> FertilizanteC  -33.550     12.122  -2.768\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) FrtlzB\n#> FertilizntB -0.707       \n#> FertilizntC -0.707  0.500\n#> optimizer (nloptwrap) convergence code: 0 (OK)\n#> boundary (singular) fit: see ?isSingular\nranef(modelo.mixto)\n#> $Tierra\n#>    (Intercept)\n#> Z1           0\n#> Z2           0\n#> Z3           0\n#> \n#> with conditional variances for \"Tierra\""},{"path":"anova.html","id":"análisis-multivariante-de-la-varianza","chapter":"Capítulo 10 Análisis de la Varianza","heading":"10.5 Análisis multivariante de la varianza","text":"Hasta ahora hemos analizado el efecto que uno o varios\nfactores tienen sobre una única variable \\(Y\\).\nEn ocasiones, tenemos en el lado izquierdo de la formula\nun vector aleatorio \\(\\pmb{Y}\\) con \\(p\\) variables respuesta\n\\(Y_i, \\ldots, Y_p\\) con cierta estructura de correlación\ny queremos determinar si esta variable multivariante\nse comporta de forma distinta para los distintos niveles\nde las variables en el vector de variables independientes \\(\\pmb{X}\\).\nEl método calcula una matriz de errores y una matriz de\nhipótesis82,\ny mediante el cálculo de un estadístico (por defecto Pillai83)\nse contrasta la hipótesis.En el conjunto de datos de ejemplo tenemos otra variable en escala métrica que es\nla Pureza de un determinado compuesto en la planta. El siguiente ejemplo\nrealiza el análisis multivariante de la varianza para estas dos variables\nagrupadas en una matriz. El modelo se puede ajustar con la función aov,\no bien con la función maov, que es realmente un wrapper de la anterior.\nLos contrastes multivariantes se obtienen con la función summary.manova, si\nle añadimos .manova tenemos los contrastes univariantes para cada componente\ndel vector aleatorio \\(\\pmb{Y}\\). El resultado que obtenemos en este caso es muy similar\nal obtenido en el ejemplo univariante. En ocasiones puede suceder que\nlos contrastes univariantes son significativos pero sí lo es el contraste\nmultivariante.","code":"\nY <- as.matrix(danova[, c(\"Peso\", \"Pureza\")])\nmodelo.manova <- aov(Y ~ Fertilizante*Tierra, data = danova)\nsummary.manova(modelo.manova)\n#>                     Df  Pillai approx F num Df den Df\n#> Fertilizante         2 0.97640   4.2925      4     18\n#> Tierra               2 0.14749   0.3583      4     18\n#> Fertilizante:Tierra  4 1.14085   2.9877      8     18\n#> Residuals            9                               \n#>                      Pr(>F)  \n#> Fertilizante        0.01300 *\n#> Tierra              0.83494  \n#> Fertilizante:Tierra 0.02561 *\n#> Residuals                    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(modelo.manova)\n#>  Response Peso :\n#>                     Df Sum Sq Mean Sq F value   Pr(>F)   \n#> Fertilizante         2 3382.3 1691.17  9.6743 0.005724 **\n#> Tierra               2   43.2   21.61  0.1236 0.885213   \n#> Fertilizante:Tierra  4 4995.8 1248.95  7.1446 0.007121 **\n#> Residuals            9 1573.3  174.81                    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>  Response Pureza :\n#>                     Df Sum Sq Mean Sq F value    Pr(>F)    \n#> Fertilizante         2 411.96 205.980 31.0211 9.168e-05 ***\n#> Tierra               2   3.53   1.765  0.2658 0.7724260    \n#> Fertilizante:Tierra  4 496.88 124.219 18.7077 0.0002184 ***\n#> Residuals            9  59.76   6.640                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"doe.html","id":"doe","chapter":"Capítulo 11 Diseño de experimentos","heading":"Capítulo 11 Diseño de experimentos","text":"","code":""},{"path":"doe.html","id":"introducción-3","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.1 Introducción","text":"El Diseño y Análisis de Experimentos (que abreviaremos como DoE), como cualquier\notra técnica estadística, se basa en el estudio de la variabilidad. DoE es la\nherramienta más potente para la mejora, lo que ha llevado algunos autores \nllamarlo “jewel quality engineering” (Ver por ejemplo\nTheodore T. Allen84).En apartados anteriores del libro hemos aprendido las herramientas básicas\npara analizar la variabilidad de los datos. En este apartado vamos revisar\nlas técnicas de Diseño de Experimentos y\nsu posterior análisis. Demasiado menudo los esfuerzos se centran en intentar\nanalizar un experimento sin diseño, lo que provoca frustración en los equipos\ninvolucrados en el análisis de datos. Vamos mostrar la\nimportancia de la fase de diseño, así como su planificación y correcta\nejecución. obstante la parte de análisis es igualmente importante, sobre todo en\nlo que concierne la correcta interpretación de los resultados.","code":""},{"path":"doe.html","id":"bases-del-doe-origen-importancia-objetivos-y-requerimientos","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.2 Bases del DoE: origen, importancia, objetivos y requerimientos","text":"El DoE moderno surge principios del siglo XX de la mano de Ronald . Fisher\ncuando trabajaba en el “Rothamsted Experimental Station” en Inglaterra. Sus\nestudios se centraban en reducir la variación natural y prevenir la confusión\ncon la variación de los restantes efectos. En última instancia, detectar\nlas relaciones causa-efecto con el menor esfuerzo experimental.Básicamente, necesitamos el DoE frente \nestudios observacionales u otras estrategias como “un factor cada vez” para\nestudiar las interacciones y encontrar relaciones de causa-efecto con\nel menor uso de recursos posible.\nAsí, podremos tomar decisiones respaldadas por los datos.El objetivo del diseño de experimentos es encontrar los niveles de\nciertos factores que optimizan una determinada característica medible.\nEsto se consigue con un método sistemático85 que evita salidas en falso y respuestas\nincompletas. Mediante la reducción del error experimental se consigue evitar la confusión de los efectos y anular los efectos sin interés para el estudio.Para empezar, lo primero que necesitamos es definir los datos del problema\nobjeto de estudio y disponer de una forma de obtenerlos adecuadamente, en particular:Una variable respuesta en escala métricaFactores controlablesPosiblemente, otros factores aleatoriosEsta recogida de datos se debe realizar de forma sistemática y\nteniendo en cuenta los tres pilares del DoE: aleatorización, bloqueo y\nreplicación.","code":""},{"path":"doe.html","id":"importancia-del-diseño","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.3 Importancia del diseño","text":"Con la experimentación básicamente controlamos los niveles los que operan\nciertos factores controlables, la vez que se asignan dichos niveles\n(configuraciones, tratamientos, etc.) las unidades experimentales. Esto\npermite, unido las apropiadas estrategias de aleatorización, bloqueo y\nreplicación, realizar predicciones acerca del desempeño de un determinado\nproceso. Estas predicciones así establecidas serán el resultado de la\nidentificación de una relación causa-efecto, que se puede conseguir\nsimplemente analizando datos recogidos sin diseño. En los estudios observacionales:Recogemos informaciónNo controlamos factoresAnálisis descriptivosDescubrir relacionesMientras que con experimentos diseñados:Se controlan los factoresSe analizan efectosIncluidas las interaccionesSe verifica la relación causa-efectoSi la experimentación se lleva cabo variando una vez cada factor, buscando el\nvalor óptimo para la respuesta para cada factor individualmente dejando fijos\nel resto arbitrariamente, estaremos obviando un aspecto\nfundamental: el efecto de las interacciones. La interacción es el efecto que tiene un factor distintos niveles de otros factores. Por otra parte, el número de\nexperimentos necesarios para llegar conclusiones válidas es mucho mayor\n(y por tanto el experimento más costoso). Con diseño de Experimentos obtenemos el mayor número de combinaciones posibles para estimar interacciones, con el mínimo número de experimentos.El análisis de datos, por muy sofisticado que sea, puede nunca\narreglar un experimento mal diseñado (chapucero, según Lawson)Sometimes thing \ncan poorly designed\nexperiment try find \ndied \nR.. FisherAs know Murphy’s Law, anything can go wrong , analysis \ndata can never compensate botched experimentsLawson86El análisis de la varianza sin diseño de experimentos tiene algunas\nlimitaciones importantes. Sin Diseño de Experimentos, los datos pueden ser inconsistentes o incompletos, al incluir factores de ruido o Factores latentes.\nSi tenemos variables correlacionadas, y alguna de ellas se\nmide, su efecto puede quedar enmascarado por las otras, como en el ejemplo de la figura 11.1, donde si miramos solo la relación de la variable respuesta\ncon el factor 1 (gráfico de la izquierda), podemos llegar la conclusión\nerrónea de que el factor 1 es determinante. Pero podría ser que la causa real\nsea el factor 2, que ha sido medido y está muy correlacionado con el factor 1.\nEn el gráfico de la derecha vemos que la variable respuesta crece en el mismo sentido que los factores 1 y 2, pero podría ser que el factor 2, medido al principio, sea la causa, y el que realmente se ha medido.\nFigura 11.1: Efecto de medir un factor\nPor otra parte, el rango de valores de la variable respuesta está limitado por su\nrango normal de operación, que puede ocultar relaciones más amplias. En la figura\n11.2, el gráfico de la derecha se corresponde con el rango de\nvariación normal de\nlos factores de un proceso. En el de la izquierda, ampliamos el rango de\nposibles valores de la variable, y vemos una relación más clara, que queda\noculta en el otro caso.\nFigura 11.2: Efecto de la limitación del rango de valores\n","code":""},{"path":"doe.html","id":"planificación-de-la-experimentación","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.4 Planificación de la experimentación","text":"El conocimiento de la materia (subject matter knowledge) en cuestión es fundamental para desarrollar\ncambios que resulten en mejoras. Sin embargo, es\nnecesario otro tipo de conocimiento (profound knowledge), en el que se\nincluye la Estadística. Combinar ambos conocimientos, lleva una\nmayor capacidad de mejora.\nEstas ideas, originarias de Deming, se recogen en R. Moen, T. Nolan, L. Provost.87\nAlgunas capacidades necesarias fruto de esta combinación son:Entender las interdependencias entre los sistemas donde se lleva cabo\nla experimentación;Entender la relación entre las predicciones y el conocimiento del\nsistema que se quiere cambiar;Entender el efecto temporal de los cambios;Entender la importancia de la estabilidad del proceso;Entender la extrapolación de los resultados de las pruebas para mejorar\nel sistema.En general, se pueden seguir tres estrategias de planificación para el diseño de\nexperimentos. Sin planificación se pueden ir cambiando niveles de factores cada\nvez y haciendo pruebas (ensayo-error), definitivamente poco efectivo. Una\nplanificación completa desde el inicio puede llevar explorar alternativas\nsurgidas durante la experimentación, y por tanto cumplir los objetivos. La\nestrategia óptima la secuencia, es decir, llevar cabo un número de experimentos al inicio, cuyas\nconclusiones supondrán la planificación de una segunda fase donde centrarnos en\nlos factores realmente relevantes y hacer análisis más detallados y precisos. En las\nprimeras fases se suelen realizar diseños de screening para descartar\nfactores significativos. En realidad, es la aplicación del método científico,\nen un proceso iterativo de aprendizaje como se muestra en la figura 11.3.\nFigura 11.3: Método iterativo de aprendizaje\nEn Moen, Nolan, Provost88 se propone el ciclo PDSA (Plan--Study-Act) para la mejora que se muestra en la figura 11.4. Básicamente consiste en:Planifica un cambio o prueba, dirigido la mejoraLleva cabo el cambio o prueba (corto alcance)Estudia el resultado: ¿qué aprendido? ¿qué ha ido mal?Actúa:\nAdopta el cambio\nAbandónalo\nEmpieza el ciclo de nuevo\nAdopta el cambioAbandónaloEmpieza el ciclo de nuevo¡Documenta todas las acciones de mejora!\nFigura 11.4: Ciclo PDSA para la mejora\nUna buena forma de empezar el ciclo es partir de un análisis de causa y\nefecto, por ejemplo con un diagrama de Ishikawa como el que aparece en la\nfigura 11.5.\nFigura 11.5: Ejemplo diagrama de causa-efecto\nLo siguiente probablemente sería determinar el presupuesto/recursos disponibles,\nen especial determinar el número de experimentos que se pueden realizar\nrealísticamente.Hasta ahora, hemos ido mencionando algunos conceptos básicos del diseño de\nexperimentos. Ahora vamos definirlos un poco más formalmente.variable respuesta: La variable de interés que pretendemos\nmejorar. Será una cuantificación de alguna característica de\ncalidad, en sentido amplio.factor: Variable independiente que puede ser causa de la respuesta.\nLa inferencia que haremos con DoE será confirmar o rechazar esta\nhipótesis.variable de bloque: Variable que tiene interés en la investigación,\npero puede influir en la respuesta. Mediante la formación de bloques\nconfundimos su efecto con los factores que realmente nos interesan.variable ruido: Variable que puede influir en la respuesta, pero de la\nque tenemos control.nivel: Valor que fijamos de un factor. En variables\ncualitativas, una categoría. En variables cuantitativas, un valor numérico\ndeterminado fijado con antelación.\nmenudo se le llama también tratamiento.unidad experimental: La división más pequeña posible de unidades de un\nexperimento tal que dos cualesquiera se les pueden aplicar distintas\ncombinaciones de factores y niveles.unidad observable: Cada uno de los elementos que forman la unidad\nexperimental. veces, un tratamiento se puede aplicar un solo\nelemento, sino varios la vez.bloque: Grupos de unidades experimentales que son tratados de forma\nsimilar en el experimento.efecto: El principal resultado de interés del experimento: qué pasa con\nla variable respuesta.réplica: Repetición de un experimento sobre una misma combinación de\nfactores y niveles, diferentes unidades experimentales.repetición: Repetición de la medición de la respuesta con las mismas\ncondiciones experimentales, la misma unidad experimental.aleatorización: Asignación de niveles y bloques unidades\nexperimentales de forma aleatoriaAl utilizar un modelo para simplificar una realidad, estamos cometiendo un\nerror. El error experimental es aquel que se debe exclusivamente las réplicas\nde las mismas condiciones experimentales. En cada diseño el error experimental\nse calcula de una forma distinta, de forma que se separa de la variabilidad total\npara ver cuánta variaación se debe al modelo y poder así tomar decisiones. Así, en el modelo:\\[Y = f(X) + \\varepsilon\\]Y es la variable respuestaX es el conjunto de variables predictivas\\(f\\) función lineal, exponencial, etc.\\(\\varepsilon\\) es una variable aleatoriaSe separa el error de la variabilidad total.Los siguientes principios son cruciales la hora de diseñar el experimento.Aleatorización. Los tratamientos deben ser asignados de forma aleatoria las\nunidades experimentales. Esto incluye bloques, factores controlables, anidamientos,\netc.Aleatorización. Los tratamientos deben ser asignados de forma aleatoria las\nunidades experimentales. Esto incluye bloques, factores controlables, anidamientos,\netc.Formación de bloques. Cuando se puedan replicar exactamente las condiciones\nexperimentales (por ejemplo, días diferentes), se deben organizar en bloques.Formación de bloques. Cuando se puedan replicar exactamente las condiciones\nexperimentales (por ejemplo, días diferentes), se deben organizar en bloques.Réplicas. Para poder estimar el error experimental y hacer contrastes de\nhipótesis, es necesario tener más de una corrida de cada combinación de tratamientos.Réplicas. Para poder estimar el error experimental y hacer contrastes de\nhipótesis, es necesario tener más de una corrida de cada combinación de tratamientos.Lawson89 propone la siguiente checklist la hora de planificar\nexperimentos:Definir objetivosIdentificar unidades experimentalesDefinir variable respuesta medible y con sentidoIdentificar los factores controlables y latentesEjecutar pruebas pilotoHacer diagrama de flujo para cada experimentoElegir el diseño experimentalDeterminar el número de réplicas necesariasAleatorizar las condiciones experimentales las unidades experimentalesDefinir método de análisis de datosCalendario y presupuesto para la ejecución","code":""},{"path":"doe.html","id":"tipos-de-diseños-de-experimentos","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5 Tipos de diseños de experimentos","text":"","code":""},{"path":"doe.html","id":"experimentos-con-un-factor","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5.1 Experimentos con un factor","text":"Podemos comparar una variable distintos niveles de un solo\nfactor. El contraste de la \\(t\\) de Student es la técnica\nutilizada para dos niveles. Para más niveles, utilizamos\nel análisis de la varianza de un factor (véase sec:anova1).\nCuando hay algún factor más que es de interés,\npero puede afectar la variable resupesta, se debe introducir\ncomo variable de bloque. Los diseños de cuadrados latinos\ny cuadrados greco-latinos se utilizan para introducir dos o tres\nfactores de bloque respectivamente.El diseño experimental para el ANOVA de un factor sigue las siguientes pautas:Se quiere estudiar el efecto de un solo factor sobre una población. hay otros\nfactores controlables que puedan influir.Se quiere estudiar el efecto de un solo factor sobre una población. hay otros\nfactores controlables que puedan influir.Se realiza el plan de recogida de datos, posiblemente con prueba piloto.Se realiza el plan de recogida de datos, posiblemente con prueba piloto.Se decide el número de unidades experimentales del experimento.Se decide el número de unidades experimentales del experimento.Se asignan aleatoriamente las unidades los niveles del factor.Se asignan aleatoriamente las unidades los niveles del factor.Se recogen los datos (experimento físico, cuestionario, etc.)Se recogen los datos (experimento físico, cuestionario, etc.)Se realiza un análisis descriptivo, sobre todo gráfico, de los datos recogidos.Se realiza un análisis descriptivo, sobre todo gráfico, de los datos recogidos.Los datos se verifican y se preparan adecuadamente para el análisis.Los datos se verifican y se preparan adecuadamente para el análisis.Se ajusta el modelo.Se ajusta el modelo.Se comprueba la validez del modelo. Si es válido, se busca modelo alternativo de análisis.Se comprueba la validez del modelo. Si es válido, se busca modelo alternativo de análisis.Se estiman los parámetros.Se estiman los parámetros.Se comprueba las hipótesis principal.Se comprueba las hipótesis principal.Si hay diferencias, se realizan comparaciones por pares.Si hay diferencias, se realizan comparaciones por pares.Se comprueba la significación práctica y se obtienen conclusiones o se toman\ndecisiones.Se comprueba la significación práctica y se obtienen conclusiones o se toman\ndecisiones.","code":""},{"path":"doe.html","id":"diseños-multifactoriales","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5.2 Diseños multifactoriales","text":"Cuando analizamos más de un factor varios niveles, aplicamos\nlo explicado en el apartado 10.3. Recordemos que en\nestos diseños es de vital importancia estudiar las interacciones.","code":""},{"path":"doe.html","id":"diseños-factoriales-a-dos-niveles-2k","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5.3 Diseños factoriales a dos niveles \\(2^k\\)","text":"Un tipo especial de diseño multifactorial es aquél en el que\ntodos los factores tienen solamente dos niveles. El número\nde experimentos necesarios para probar todas las combinaciones\nde niveles para \\(k\\) factores es \\(2^k\\), de ahi su nombre.Diseño factorial \\(2^2\\)Modelo:\\[y_{ijk} = \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\varepsilon_{ijk}\\]Datos:Aleatorizar tratamientosRealizar \\(k\\) réplicasNúmero de experimentos: \\(k\\times 2^2\\)Análisis:\n- Efectos principales\n- InteracciónEn el siguiente código se analiza un experimento con dos factores y B dos niveles, + y + -. Se muestran los gráficos de los efectos.Vemos que los dos efectos principales son significativos, pero lo es la interacción. Podemos eliminar ese término del modelo para así ganar grados de libertad y tener una mejor estimación del error.Diseño factorial \\(2^3\\)Modelo:\\[y_{ijkl} = \\alpha_i + \\beta_j + \\gamma_k + \\alpha\\beta_{ij} \n+ \\alpha\\gamma_{ik}+ \\beta\\gamma_{jk} + \\alpha\\beta\\gamma_{ijk} + \\varepsilon_{ijkl}\\]Datos:\n- Aleatorizar tratamientos\n- \\(l\\) réplicas (o )\n- Número de experimentos: \\(l\\times 2^3\\)Análisis:Efectos principalesInteracciones (más de dos difícil de ver)Eliminar significativas para aumentar precisiónEn el siguiente ejemplo, analizamos tres factores, pero omitimos la interacción de orden 3. Después, podríamos eliminar las interacciones menos significativas para quedarnos con el modelo más sencillo.Diseño factorial \\(2^k\\)Siguiendo la misma estructura que los dos anteriores, con más efectos principales y más interacciones, pero más allá de 3 es muy difícil que se produzcan, y más difícil de interpretar.\nEl número de experimentos necesarios aumenta exponencialmente, y se suelen preferir experimentos fraccionados. Cuando hay grados de libertad suficientes para realizar contrastes se utilizan herramientas gráficas para seleccionar efectos significativos (Pareto y gráfico normal)Para la Formación de bloques, se confunden con efectos de interacciones de orden superior, multiplicando los signos y dividiendo en dos bloques","code":"\nlibrary(xtable)\nlibrary(DoE.base)\nlibrary(effects)\nlibrary(reshape2)\ndatosf22 <- scan(text = \"\n-   -   28\n-   -   25  \n-   -   27\n+   -   36\n+   -   32\n+   -   32\n-   +   18\n-   +   19\n-   +   23\n+   +   31\n+   +   30\n+   +   29  \n\", \n    what = list(character(), character(), numeric()),\n    sep = \"\\t\") \ndatosf22 <- as.data.frame(datosf22)\ncolnames(datosf22) <- c(\"A\", \"B\", \"respuesta\")\ndatosf22$replica <- rep(1:3, 4)\nlibrary(knitr)\nkable(dcast(datosf22, A + B ~ replica, value.var = \"respuesta\"))\nmodelof22 <- lm(respuesta ~ A + B + A*B, data = datosf22)\n# kable(anova(modelof22))\nanova(modelof22)\n#> Analysis of Variance Table\n#> \n#> Response: respuesta\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> A          1 208.333 208.333 53.1915 8.444e-05 ***\n#> B          1  75.000  75.000 19.1489  0.002362 ** \n#> A:B        1   8.333   8.333  2.1277  0.182776    \n#> Residuals  8  31.333   3.917                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlibrary(lattice)\ntrellis.par.set(background = list(col = \"white\"))\nplot(effect(term = \"A\", mod = modelof22))\n#> NOTE: A is not a high-order term in the model\nplot(effect(term = \"B\", mod = modelof22))\n#> NOTE: B is not a high-order term in the model\nplot(effect(term = \"A:B\", mod = modelof22))\nmodelof22 <- lm(respuesta ~ A + B, data = datosf22)\nanova(modelof22)\n#> Analysis of Variance Table\n#> \n#> Response: respuesta\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> A          1 208.333 208.333  47.269 7.265e-05 ***\n#> B          1  75.000  75.000  17.017  0.002578 ** \n#> Residuals  9  39.667   4.407                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nplot(effect(term = \"A\", mod = modelof22))\nplot(effect(term = \"B\", mod = modelof22))\ndatosf23 <- scan(text = \"\n-   -   -   60\n+   -   -   72  \n-   +   -   54\n+   +   -   68\n-   -   +   52\n+   -   +   83\n-   +   +   45\n+   +   +   80\n\", \n    what = list(character(), character(), character(), numeric()),\n    sep = \"\\t\") \ndatosf23 <- as.data.frame(datosf23)\ncolnames(datosf23) <- c(\"T\", \"C\", \"K\", \"rendimiento\")\nkable(dcast(datosf23, T + C + K ~ ., value.var = \"rendimiento\"))\nmodelof23 <- lm(rendimiento ~ T + C + K + T*C + T*K + C*K, data = datosf23)\nanova(modelof23)\n#> Analysis of Variance Table\n#> \n#> Response: rendimiento\n#>           Df Sum Sq Mean Sq F value  Pr(>F)  \n#> T          1 1058.0  1058.0    2116 0.01384 *\n#> C          1   50.0    50.0     100 0.06345 .\n#> K          1    4.5     4.5       9 0.20483  \n#> T:C        1    4.5     4.5       9 0.20483  \n#> T:K        1  200.0   200.0     400 0.03180 *\n#> C:K        1    0.0     0.0       0 1.00000  \n#> Residuals  1    0.5     0.5                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nplot(effect(term = \"T\", mod = modelof23))\n#> NOTE: T is not a high-order term in the model\n\nplot(effect(term = \"C\", mod = modelof23))\n#> NOTE: C is not a high-order term in the model\nplot(effect(term = \"K\", mod = modelof23))\n#> NOTE: K is not a high-order term in the model\nplot(effect(term = \"T:C\", mod = modelof23))\nplot(effect(term = \"T:K\", mod = modelof23))\nplot(effect(term = \"C:K\", mod = modelof23))"},{"path":"doe.html","id":"diseños-fraccionales","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5.4 Diseños fraccionales","text":"Los diseños factoriales fraccionales \\(2^{k-p}\\) utilizan solo una fracción de su equivalente factorial. En estos diseños se confunden los efectos principales con las interacciones de mayor orden. De esta forma, se puede realizar screening de muchos factores con pocos experimentos, y una vez eliminados del modelo los efectos significativos se estima mejor el error.","code":""},{"path":"doe.html","id":"diseños-avanzados","chapter":"Capítulo 11 Diseño de experimentos","heading":"11.5.5 Diseños avanzados","text":"Existen otros diseños avanzados que se tratan en este texto, como son:Plackett BurmanDiseños anidadosSplit-plotMedidas repetidasSuperficie respuesta","code":""},{"path":"regresion.html","id":"regresion","chapter":"Capítulo 12 Modelos de regresión","heading":"Capítulo 12 Modelos de regresión","text":"En preparación.Regresión lineal simpleRegresión linealRegresión lineal múltipleOtros modelos*\n(GLM, GAM, …)","code":""},{"path":"introc.html","id":"introc","chapter":"Capítulo 13 Introducción","heading":"Capítulo 13 Introducción","text":"En preparación.Historia de la calidadEstadística y calidadGestión de la calidadMejora de procesos vs control de calidadMetodologíasIntro Six Sigma*","code":""},{"path":"spc.html","id":"spc","chapter":"Capítulo 14 Control Estadístico de Procesos","heading":"Capítulo 14 Control Estadístico de Procesos","text":"En preparación.Intro SPCGráficos de controlCapacidad y rendimiento","code":""},{"path":"aceptacion.html","id":"aceptacion","chapter":"Capítulo 15 Inspección por muestreo","heading":"Capítulo 15 Inspección por muestreo","text":"En preparación.IntroPlanes para atributosPlanes para variables","code":""},{"path":"símbolos-abreviaturas-y-acrónimos.html","id":"símbolos-abreviaturas-y-acrónimos","chapter":"Apéndice A Símbolos, abreviaturas y acrónimos","heading":"Apéndice A Símbolos, abreviaturas y acrónimos","text":"","code":""},{"path":"símbolos-abreviaturas-y-acrónimos.html","id":"acrónimos","chapter":"Apéndice A Símbolos, abreviaturas y acrónimos","heading":"A.1 Acrónimos","text":"","code":""},{"path":"símbolos-abreviaturas-y-acrónimos.html","id":"letras-griegas","chapter":"Apéndice A Símbolos, abreviaturas y acrónimos","heading":"A.2 Letras griegas","text":"\\(^*\\) Mayúsculas","code":""},{"path":"símbolos-abreviaturas-y-acrónimos.html","id":"símbolos","chapter":"Apéndice A Símbolos, abreviaturas y acrónimos","heading":"A.3 Símbolos","text":"","code":""},{"path":"tablas.html","id":"tablas","chapter":"Apéndice B Tablas estadísticas","heading":"Apéndice B Tablas estadísticas","text":"","code":""},{"path":"tablas.html","id":"distribución-normal-1","chapter":"Apéndice B Tablas estadísticas","heading":"B.1 Distribución normal","text":"La siguiente tabla contiene la probabilidad de la cola inferior de la distribución normal estándar \\(Z\\sim N(0;1)\\),\nes decir \\(F(z)=P[Z\\leq z].\\).","code":""},{"path":"tablas.html","id":"resumen-modelos-de-distribución-de-probabilidad","chapter":"Apéndice B Tablas estadísticas","heading":"B.2 Resumen modelos de distribución de probabilidad","text":"","code":""},{"path":"repaso.html","id":"repaso","chapter":"Apéndice C Repaso","heading":"Apéndice C Repaso","text":"Este apéndice cubre algunas cuestiones matemáticas básicas que el lector\nde este libro con seguridad habrá aprendido con anterioridad. Se incluyen\ncomo referencia para facilitar el repaso aquellos que lo necesiten.","code":""},{"path":"repaso.html","id":"logaritmos-y-exponenciales","chapter":"Apéndice C Repaso","heading":"C.1 Logaritmos y exponenciales","text":"","code":""},{"path":"repaso.html","id":"combinatoria","chapter":"Apéndice C Repaso","heading":"C.2 Combinatoria","text":"Una de las definiciones de probabilidad implica contar\nel número de veces que puede ocurrir un suceso determinado. Por tanto,\nen muchas ocasiones el cálculo de probabilidades empieza contando las\nposibilidades de que ocurra un suceso. La Combinatoria es la parte de la\nMatemática discreta que nos ayuda en esta tarea. Incluimos un breve\nresumen con ejemplos de las fórmulas más habituales y su cálculo con R.","code":""},{"path":"repaso.html","id":"ejemplo-ilustrativo","chapter":"Apéndice C Repaso","heading":"C.2.1 Ejemplo ilustrativo","text":"Habitualmente se utilizan ejemplos de juegos de azar para introducir el\ncálculo de probabilidades, como lanzamiendo de monedas y dados, o\ncombinaciones de cartas en barajas de naipes. Para darle un enfoque\npráctico, utilizaremos lo largo del módulo un ejemplo ilustrativo que,\naunque totalmente inventado, se puede encontrar el lector\nen el futuro con ligeras variaciones según su ámbito de actuación.\nUtilizaremos en lo posible las cifras usadas en los problemas de azar\npara ver la utilidad de aquéllos ejemplos en casos más prácticos.Datos básicos:52 posibles usuarios de un servicio52 posibles usuarios de un servicioLa mitad son mujeresLa mitad son mujeres4 directivos, 12 mandos, resto operarios4 directivos, 12 mandos, resto operarios13 jóvenes, 26 adultos, 13 mayores (5, 18 y 3 mujeres en cada\ngrupo respectivamente)13 jóvenes, 26 adultos, 13 mayores (5, 18 y 3 mujeres en cada\ngrupo respectivamente)1 de cada seis hombres contratará el servicio (el doble si es mujer)1 de cada seis hombres contratará el servicio (el doble si es mujer)Nótese cómo podemos traducir el concepto de\nservicio cualquier ámbito: usuarios de salud o educación, enfermos de\nuna determinada patología, equipos de una infraestructura, etc. Asimismo\nlas categorías pueden ser cualesquiera aplicables los elementos de los\nconjuntos.","code":""},{"path":"repaso.html","id":"principio-básico-de-conteo","chapter":"Apéndice C Repaso","heading":"C.2.2 Principio básico de conteo","text":"Definición: Realizamos \\(k\\) experimentos sucesivamente, cada\nuno de ellos con \\(n_i\\) posibles resultados (\\(=1, \\ldots, k\\)). Entonces\nel número total de resultados posibles es:\\[n_1\\cdot n_2, \\cdot \\ldots \\cdot n_k\\]Ejemplo: Resultados posibles si tomamos al azar un individuo\ny observamos su grupo de edad y si contratará o el servicio.Código","code":"\n3*2\n#> [1] 6"},{"path":"repaso.html","id":"permutaciones","chapter":"Apéndice C Repaso","heading":"C.2.3 Permutaciones","text":"Definición: De cuántas formas posibles podemos ordenar un\nconjunto de \\(n\\) elementos sin repetirlos.\\[P_n = n! = n\\cdot(n-1)\\cdot(n-2)\\cdot\\ldots\\cdot 2\\cdot 1\\]Ejemplo: De cuántas formas podemos ordenar un conjunto de\ntres individuos, uno de cada categoría laboral.Código","code":"\nfactorial(3)\n#> [1] 6"},{"path":"repaso.html","id":"variaciones-muestreo-sin-reemplazamiento","chapter":"Apéndice C Repaso","heading":"C.2.4 Variaciones (muestreo sin reemplazamiento)","text":"Definición: De cuántas formas posibles podemos seleccionar\nuna muestra de \\(n\\) elementos de un conjunto total de \\(m\\), sin que se\nrepitan. Una ordenación distinta, es una posibilidad distinta.\\[V_{m,n} = m\\cdot(m-1)\\cdot(m-2)\\cdot\\ldots\\cdot (m-n+1) = \\frac{m!}{(m-n)!}\\]Ejemplo: De cuántas formas podemos seleccionar una muestra\nde 5 individuos en nuestro conjunto de 52 sin que se repitan (por\nejemplo para asignar un ranking)Código","code":"\nfactorial(52)/factorial(52-5)\n#> [1] 311875200"},{"path":"repaso.html","id":"variaciones-con-repetición-muestreo-con-reemplazamiento","chapter":"Apéndice C Repaso","heading":"C.2.5 Variaciones con repetición (muestreo con reemplazamiento)","text":"Definición: De cuántas formas posibles podemos seleccionar\nuna muestra de \\(n\\) elementos de un conjunto total de \\(m\\), pudiéndose\nrepetir. Una ordenación distinta, es una posibilidad distinta.\n\\[\\mathit{VR}_{m,n} = m^n\\]Ejemplo: De cuántas formas podemos seleccionar una muestra\nde 5 individuos en nuestro conjunto de 52 pudiéndose repetir (por\nejemplo para asignar premios consecutivamente)Código","code":"\n52^5\n#> [1] 380204032"},{"path":"repaso.html","id":"combinaciones-muestras-equivalentes","chapter":"Apéndice C Repaso","heading":"C.2.6 Combinaciones (muestras equivalentes)","text":"Definición: De cuántas formas posibles podemos seleccionar\nuna muestra de \\(n\\) elementos de un conjunto total de \\(m\\), sin importar\nel orden.\\[\\mathit{C}_{m,n} = \\binom{m}{n} = \\frac{m!}{n!(m-n)!}\\]\\(\\binom{m}{n}\\) se lee m sobre n, y se le conoce como número combinatorio.\nAlgunas propiedades importantes de los números combinatorios:\\[\\binom{m}{m} = \\binom{m}{0} = 1.\\]\n\\[\\binom{m}{1} = \\binom{m}{m-1} = m.\\]\n\\[\\binom{m}{n} + \\binom{m}{n+1} = \\binom{m+1}{n+1}\\]\nPor otra parte, por convenio se tiene que:\\[0!=1,\\]\\[\\text{si } <b \\implies \\binom{}{b} = 0.\\]Ejemplo: De cuántas formas podemos seleccionar una muestra\nde 5 individuos en nuestro conjunto de 52 sin importar el orden (por\nejemplo para asignar premios de una sola vez)Código","code":"\nchoose(52, 5)\n#> [1] 2598960"},{"path":"repaso.html","id":"combinaciones-y-permutaciones-con-repetición","chapter":"Apéndice C Repaso","heading":"C.2.7 Combinaciones y permutaciones con repetición","text":"Las combinaciones y\npermutaciones también se pueden dar con repetición, siendo las fórmulas\npara calcularlas las siguientes:\\[\\mathit{CR}_{m,n}= \\mathit{C}_{m+n-1,n}= \\frac{(m+n-1)!}{n!\\cdot(m-1)!}\\]\n\\[\\mathit{PR} = \\frac{n!}{!\\cdot b!\\cdot \\ldots\\cdot z!}\\]La primera situación es aquella en la que los\nelementos se pueden repetir, pero nos importa el orden en que lo\nhagan. La segunda aparece cuando el elemento del conjunto total de\nelementos aparece \\(\\) veces, y así sucesivamente.","code":""},{"path":"ampliación.html","id":"ampliación","chapter":"Apéndice D Ampliación","heading":"Apéndice D Ampliación","text":"En este apéndice se incluyen temas avanzados que pueden ser útiles al lector\nmás allá de un curso básico de estadística para ciencias o ingeniería, y\nque se han incluido en el cuerpo de los capítulos para mantener el nivel\nde una asignatura de grado.","code":""},{"path":"ampliación.html","id":"función-característica","chapter":"Apéndice D Ampliación","heading":"D.1 Función característica","text":"","code":""},{"path":"ampliación.html","id":"cambio-de-variable","chapter":"Apéndice D Ampliación","heading":"D.2 Cambio de variable","text":"","code":""},{"path":"ampliación.html","id":"variables-aleatorias-unidimensionales-mixtas","chapter":"Apéndice D Ampliación","heading":"D.3 Variables aleatorias unidimensionales mixtas","text":"","code":""},{"path":"ampliación.html","id":"variables-aleatorias-bidimensionales-mixtas","chapter":"Apéndice D Ampliación","heading":"D.4 Variables aleatorias bidimensionales mixtas","text":"","code":""},{"path":"ampliación.html","id":"algunos-modelos-de-distribución-continuos-más","chapter":"Apéndice D Ampliación","heading":"D.5 Algunos modelos de distribución continuos más","text":"","code":""},{"path":"ampliación.html","id":"distribución-beta","chapter":"Apéndice D Ampliación","heading":"D.5.1 Distribución Beta","text":"La distribución Beta se utiliza en problemas de inferencia relativos proporciones, especialmente en inferencia bayesiana.\\[X \\sim \\mathit{}(\\alpha, \\beta)\\]Función de densidad\\[f(x) = \n\\begin{cases}\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta -1} & \\text{si } 0 < x < 1\\\\\n0 & \\text{resto } \n\\end{cases}\\]En matemáticas, la función Gamma (\\(\\Gamma\\)) es una integral indefinida que tiene entre otras las siguientes propiedades:$() = _0x{} e^{-x} dx, > 0 $\\(\\Gamma(\\alpha + 1) = \\alpha \\Gamma(\\alpha)\\)\\(n \\\\mathbb{N}-\\{0\\} \\implies \\Gamma(n) = (n-1)!\\)\\(\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\)** Características**Esperanza: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\)Varianza: \\(\\mathit{Var}[X] = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta+1)}\\)Caso particular: \\(\\mathit{}(1,1) = U(0,1)\\).Ejemplo\\(X\\): Proporción de clientes que contratarán el servicio\\(X\\sim \\mathit{}(1, 5)\\)Código","code":"\nmibeta <- function(x) dbeta(x, 1, 5)\ncurve(mibeta, lwd = 2)"},{"path":"ampliación.html","id":"distribución-gamma","chapter":"Apéndice D Ampliación","heading":"D.5.2 Distribución Gamma","text":"La distribución Gamma se utiliza, entre otros, para modelizar tiempos de espera hasta que suceden \\(\\alpha\\) eventos en un proceso de Poisson. De hecho, en inferencia bayesiana gamma es la distribución priori de la distribución de Poisson.\\[X \\sim \\mathit{Ga}(, b)\\]Función de densidad\\[f(x) =\n\\begin{cases}\n\\frac{b^}{\\Gamma()}x^{-1}{e}^{-bx} & \\text{si } 0 < x < \\infty\\\\\n0 & \\text{resto }\n\\end{cases}\\]CaracterísticasEsperanza: \\(E[X] = \\frac{}{b}\\)Varianza: \\(\\mathit{Var}[X] = \\frac{}{b^2}\\)$() = _0x{} e^{-x} dx $La exponencial es un caso particularCódigo","code":"\n\n\nmigamma <- function(x, a) dgamma(x, a, 2)\ncurve(migamma(x, 1), lwd = 2, xlim = c(0,10), \n      main = \"Distribución Gamma b = 2\")\ncurve(migamma(x, 2), lwd = 2, add = TRUE, lty = 2)\ncurve(migamma(x, 4), lwd = 2, add = TRUE, lty = 3)\nlegend(x = 6, y = 2, c(\"a = 1\", \"a = 2\", \"a = 4\"), lty = 1:3)"},{"path":"ampliación.html","id":"distribución-de-weibull","chapter":"Apéndice D Ampliación","heading":"D.5.3 Distribución de Weibull","text":"La distribución Gamma presenta algunos inconventientes al modelizar tiempos de vida, y por eso algunas veces se prefiere la distribución de Weibull, que básicamente sirve para lo mismo. Véase  para los detalles.\\[X \\sim \\mathit{}(, b) \\]Función de densidad\n\\[f(x) =\n\\begin{cases}\n\\frac{}{b}\\left (\\frac{x}{b} \\right)^{-1}e^{-(x/b)^} & \\text{si } x > 0\\\\\n0 & \\text{resto }\n\\end{cases}\\]CaracterísticasEsperanza: \\(E[X] =b \\Gamma\\left (1 + \\frac{1}{} \\right )\\)Varianza: \\(\\mathit{Var}[X] = b^2 \\left ( \\Gamma \\left (  1 + \\frac{2}{} \\right  )  - \\left ( \\Gamma \\left (1 + \\frac{2}{} \\right ) \\right )^2 \\right )\\)Código","code":"\nmiweibull <- function(x, a) dweibull(x, a, 2)\ncurve(miweibull(x, 1), lwd = 2, xlim = c(0,5), \n      ylim = c(0, 1),\n      main = \"Distribución Weibull b = 2\")\ncurve(miweibull(x, 2), lwd = 2, add = TRUE, lty = 2)\ncurve(miweibull(x, 5), lwd = 2, add = TRUE, lty = 3)\nlegend(x = 4, y = 1, c(\"a = 1\", \"a = 2\", \"a = 5\"), lty = 1:3)"},{"path":"ampliación.html","id":"modelos-de-distribución-de-probabilidad-multivariantes","chapter":"Apéndice D Ampliación","heading":"D.6 Modelos de distribución de probabilidad multivariantes","text":"","code":""},{"path":"ampliación.html","id":"modelos-de-distribución-de-probabilidad-relacionadas-con-la-normal","chapter":"Apéndice D Ampliación","heading":"D.7 Modelos de distribución de probabilidad relacionadas con la normal","text":"","code":""},{"path":"ampliación.html","id":"simulación-de-variables-aleatorias","chapter":"Apéndice D Ampliación","heading":"D.8 Simulación de variables aleatorias","text":"\\(U(0;\\; 1)\\): Generador de probabilidades aleatorias. Dada cualquier función de distribución \\(F\\), se pueden generar valores de esa VA obteniendo \\(F^{-1}(U(0;\\; 1))\\)","code":""},{"path":"demostraciones.html","id":"demostraciones","chapter":"Apéndice E Demostraciones","heading":"Apéndice E Demostraciones","text":"Em este apéndice se incluyen aquellas demostraciones de teoremas y propiedades\nincluidas en los capítulos para mantener el carácter práctico del mismo.","code":""},{"path":"demostraciones.html","id":"variable-aleatoria-discreta","chapter":"Apéndice E Demostraciones","heading":"E.1 Variable aleatoria discreta","text":"","code":""},{"path":"demostraciones.html","id":"función-de-probabilidad-1","chapter":"Apéndice E Demostraciones","heading":"E.1.1 Función de probabilidad","text":"","code":""},{"path":"demostraciones.html","id":"esperanza","chapter":"Apéndice E Demostraciones","heading":"E.1.2 Esperanza","text":"","code":""},{"path":"demostraciones.html","id":"varianza","chapter":"Apéndice E Demostraciones","heading":"E.1.3 Varianza","text":"","code":""},{"path":"creditos.html","id":"creditos","chapter":"Apéndice F Créditos","heading":"Apéndice F Créditos","text":"Los gráficos y diagramas generados son creación y propiedad del autor, salvo que se\nindique lo contrario. Su licencia de uso es la misma que la del resto de la\nobra, véase el Prefacio.La imagen de la portada es de dominio público, obtenida en pixabay.com, gracias al\nusuario Manuchi.Las imágenes de tipo clipart usadas en esta obra y las fotografías atribuidas\npertenecen al dominio público gracias openclipart.org, unplash.com o pixabay.com.R logo (c) 2016 R Foundation.","code":""},{"path":"referencias.html","id":"referencias","chapter":"Referencias","heading":"Referencias","text":"","code":""}]
