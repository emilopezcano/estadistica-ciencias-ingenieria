
```{r, include=FALSE}
library(tidyverse)
library(broom)
library(flextable)
library(knitr)
```

# Diseño de experimentos {#doe}

## Introducción

El Diseño y Análisis de Experimentos (que abreviaremos como DoE), como cualquier
otra técnica estadística, se basa en el estudio de la variabilidad. DoE es la
herramienta más potente para la mejora, lo que ha llevado a algunos autores a
llamarlo "the jewel of quality engineering" (Ver por ejemplo
@allen2010).

En capítulos anteriores del libro hemos aprendido las herramientas básicas 
para **analizar la variabilidad** de los datos. En este capítulo vamos a revisar 
las técnicas de Diseño de Experimentos y
su posterior análisis. Demasiado a menudo los esfuerzos se centran en intentar
analizar un experimento sin diseño, lo que provoca frustración en los equipos
involucrados en el análisis de datos. Vamos a mostrar la
importancia de la fase de diseño, así como su planificación y correcta
ejecución. No obstante la parte de análisis es igualmente importante, sobre todo en
lo que concierne a la correcta interpretación de los resultados y la toma de decisiones informada.



:::{.rmdcafe data-latex=""}
En su discurso presidencial ante el Primer Congreso Estadístico de la India, Sir Ronald Fisher (1890-1962) dijo:

>_"To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of"_. 

O, dicho en otras palabras, cuando analizamos los datos del experimento sin haber participado en el diseño, lo más que podemos hacerle es la autopsia.
:::


## Bases del DoE: origen, importancia, objetivos y requerimientos


El DoE moderno surge a principios del siglo XX de la mano de Ronald A. Fisher
cuando trabajaba en el "Rothamsted Experimental Station" en Inglaterra. Sus
estudios se centraban en reducir la variación natural y prevenir la confusión 
con la variación de los restantes efectos. En última instancia, detectar
las relaciones causa-efecto con el menor esfuerzo experimental.



Básicamente, necesitamos el DoE frente a
estudios observacionales u otras estrategias como "un factor cada vez" para
estudiar las interacciones y encontrar relaciones de causa-efecto con 
el menor uso de recursos posible.
Así, podremos tomar decisiones respaldadas por los datos.




El objetivo del diseño de experimentos es encontrar los niveles de
ciertos factores que optimizan una determinada característica medible. 
Esto se consigue con un método sistemático^[En realidad, el método científico.] que evita salidas en falso y respuestas
incompletas. Mediante la reducción del error experimental se consigue evitar la confusión de los efectos y anular los efectos sin interés para el estudio. 





Para empezar, lo primero que necesitamos es definir los datos del problema
objeto de estudio y disponer de una forma de obtenerlos adecuadamente, en particular:

- Una variable respuesta en escala métrica.
- Factores controlables.
- Posiblemente, otros factores aleatorios.

Esta recogida de datos se debe realizar de forma sistemática y 
teniendo en cuenta los tres pilares del DoE: **aleatorización**, **bloqueo** y
**replicación**.




## Importancia del diseño {#doe.importancia}

Con la experimentación básicamente controlamos los niveles a los que operan
ciertos factores controlables, a la vez que se asignan dichos niveles
(configuraciones, tratamientos, etc.) a las unidades experimentales. Esto
permite, unido a las apropiadas estrategias de aleatorización, bloqueo y
replicación, realizar predicciones acerca del desempeño de un determinado
proceso. Estas predicciones así establecidas serán el resultado de la
identificación de una relación causa-efecto, que no se puede conseguir
simplemente analizando datos recogidos sin diseño. En los **estudios observacionales**:

  - Recogemos información (o simplemente "está ahí").
  - No controlamos factores.
  - Se realizan análisis descriptivos.
  - Se descubren **relaciones** mediante inferencia.

Mientras que con **experimentos diseñados**:

  - Se controlan los factores y su asignación a los elementos en estudio (sujetos, cosas, plantas, ...)
  - Se analizan los efectos en la variable respuesta.
  - Se analizan las interacciones de los factores.
  - Se verifica la relación **causa-efecto**.




Si la experimentación se lleva a cabo variando una vez cada factor, buscando el
valor óptimo para la respuesta para cada factor individualmente dejando fijos
el resto arbitrariamente, estaremos obviando un aspecto
fundamental: el efecto de las interacciones. La interacción es el efecto que tiene un factor a distintos niveles de otros factores. 

Por otra parte, el número de 
experimentos necesarios para llegar a conclusiones válidas es mucho mayor
(y por tanto el experimento más costoso). Mediante un experimento diseñado obtenemos el mayor número de combinaciones posibles para estimar interacciones, con el mínimo número de experimentos.


::: {.rmdcafe data-latex=""}

El análisis de datos, por muy sofisticado que sea, no puede nunca
_arreglar_ un experimento mal diseñado (chapucero, según Lawson)

>  As we know from Murphy's Law, if anything can go wrong it will, and analysis of
  data can never compensate for botched experiments  
    @lawson2015
:::


El análisis de la varianza (ver capítulo \@ref(anova)) sin diseño de experimentos tiene algunas
limitaciones importantes. Sin Diseño de Experimentos, los datos pueden ser inconsistentes o incompletos, al no incluir factores de ruido o Factores latentes.
Si tenemos variables correlacionadas, y alguna de ellas no se
mide, su efecto puede quedar enmascarado por las otras, como en el ejemplo de la figura \@ref(fig:c1), donde si miramos solo la relación de la variable respuesta
con el factor 1 (gráfico de la izquierda), podemos llegar a la conclusión
errónea de que el factor 1 es determinante. Pero podría ser que la causa real
sea el factor 2, que no ha sido medido y está muy correlacionado con el factor 1. 
En el gráfico de la derecha vemos que la variable respuesta crece en el mismo sentido que los factores 1 y 2, pero podría ser que el factor 2, no medido al principio, sea la causa, y no el que realmente se ha medido.


```{r c1, fig.height=4, fig.width=8, fig.cap="Efecto de no medir un factor", fig.align='center', echo=FALSE, out.width="100%", purl = FALSE}
library(ggplot2)
set.seed(666)
factor1 <- rnorm(20, 80)
factor2 <- sapply(factor1/3, function(x){rnorm(1,x,0.5)})
response <- sapply(factor1+2*factor2, function(x){rnorm(1,x,0.1)})
qconf <- qplot(factor1, factor2, size = response) + 
  theme_bw() +
  labs(title = "Factor 1 frente a Factor 2",
       x = "Factor 1",
       y = "Factor 2",
       size = "Respuesta")
qhid <- qplot(factor1, response) + 
  theme_bw() +
  labs(title = "Variable respuesta frente a Factor 1",
       x = "Factor 1",
       y = "Respuesta")
gridExtra::grid.arrange(qhid, qconf, nrow = 1, widths = c(0.45, 0.55))
```


Por otra parte, el rango de valores de la variable respuesta está limitado por su
rango normal de operación, que puede ocultar relaciones más amplias. En la figura
\@ref(fig:valuesrange1), el gráfico de la izquierda se corresponde con el rango de 
variación normal de
los factores de un proceso. En el de la derecha, ampliamos el rango de
posibles valores de la variable, y vemos algunos patrones que quedan
ocultos en el otro caso. Por ejemplo, en los valores inferiores la pendiente es más pronunciada, mientras que en los valores superiores parece que
se empieza a invertir la tendencia.

```{r valuesrange1, echo=FALSE, fig.height=4, fig.cap="Efecto de la limitación del rango de valores", fig.align='center', message=FALSE, out.width="100%", purl = FALSE}
set.seed(66)
x = rnorm(100, 10)
rangoop <- data.frame(x,
                      y = sapply(x, function(x){rnorm(1,x)})) |> 
  dplyr::mutate(out = ifelse(x < 9 | x > 11, 1, 0))
p1 <- rangoop |> 
  dplyr::filter(x >= 9, x <= 11) |> 
  ggplot(aes(x, y)) + 
  geom_point() +
  geom_smooth() +
  labs(title = "Rango normal de operación") +
  theme_bw()
p2 <- rangoop |> 
  ggplot(aes(x, y, col = out)) + 
  geom_point() + 
  # xlim(8, 12) +
  geom_smooth() +
  labs(title = "Rango ampliado por el experimento") +
  theme_bw() +
  theme(legend.position = "none")

gridExtra::grid.arrange(p1, p2, nrow = 1)

# plot(y ~ x, xlab = "X", ylab = "Y", main = "Rango ampliado")
# abline(v = 9)
# abline(v = 11)
# plot(y ~ x, xlim=c(9,11), ylim = c(7,13), main = "Rango normal de operación")
```




## Planificación de la experimentación

El conocimiento de la materia en cuestión (_subject matter knowledge_) es fundamental para desarrollar
cambios que resulten en mejoras. Sin embargo, es
necesario otro tipo de conocimiento (_profound knowledge_), en el que se
incluye la Estadística. Combinar ambos conocimientos, lleva a una
mayor capacidad de mejora.
Estas ideas, originarias de Deming, se recogen en @moen2012.
Algunas capacidades necesarias fruto de esta combinación son:

  - Entender las interdependencias entre los sistemas donde se lleva a cabo
  la experimentación.
  - Entender la relación entre las predicciones y el conocimiento del
  sistema que se quiere cambiar.
  - Entender el efecto temporal de los cambios.
  - Entender la importancia de la estabilidad del proceso.
  - Entender la extrapolación de los resultados de las pruebas para mejorar
  el sistema.



En general, se pueden seguir tres estrategias de planificación para el diseño de
experimentos:

1. Sin planificación. Se pueden ir cambiando niveles de factores cada
vez y haciendo pruebas (ensayo-error). Esto es definitivamente poco efectivo. 
2. Planificación completa desde el inicio. Si es muy rígida, nos puede llevar a no explorar alternativas
que puedan surgir durante la experimentación. Y por tanto a no cumplir los objetivos. 
3. Planificación **secuencial**. Esta es la
estrategia óptima. Se llevan a cabo un número de experimentos al inicio, cuyas
conclusiones supondrán la planificación de una segunda fase donde centrarnos en
los factores realmente relevantes y hacer análisis más detallados y precisos. En las
primeras fases se suelen realizar diseños de _screening_ para descartar
factores no significativos. En las siguientes fases, se amplían los experimentos solo con los factores significativos. 

En realidad, esta estrategia secuencial es la aplicación del método científico,
en un proceso iterativo de aprendizaje como se muestra en la figura \@ref(fig:metodo). El experimento partirá de un objetivo que planteará ciertas hipótesis. Por deducción, se diseña el experimento y se obtienen datos. Estos datos confirman o no las hipótesis mediante la inducción, y nos hace plantearnos nuevas hipótesis. Así, hasta llegar a la conclusión y/o decisión final.



```{r metodo, echo=FALSE, fig.cap="Método iterativo de aprendizaje", fig.align="center", purl = FALSE}
par(mai=c(0,0,0,0), bg="white")
plot(1:10, type = "n", axes = FALSE, xlab = "", ylab = "",
    xlim = c(-1, 12))
lines(c(0,10), c(2,2))
lines(c(0,10), c(8,8))
text(5, 8.5, "Datos")
text(5, 1.5, "Idea (modelo, hipótesis, ...)")
points(c(-1, -0.5, 0), rep(5, 3), pch = 20)
points(c(10, 10.5, 11), rep(5, 3), pch = 20)
points(c(2, 5, 8), rep(2.1, 3), pch = 20)
points(c(3.5, 6.5, 8.5), rep(7.9, 3), pch = 20)
arrows(0, 5, 2, 2.1)
arrows(2, 2.1, 3.5, 7.9)
text(2, 6, "Deducción", cex =0.8)
arrows(3.5, 7.9, 5, 2.1)
text(4, 4, "Inducción", cex =0.8)
arrows(5, 2.1, 6.5, 7.9)
text(5.5, 6, "Deducción", cex =0.8)
arrows(6.5, 7.9, 8, 2.1)
text(7, 4, "Inducción", cex =0.8)
arrows(8, 2.1, 8.5, 7.9)
text(8.5, 6, "Deducción", cex =0.8)
arrows(8.5, 7.9, 10, 5)
```


En @moen2012 se propone el ciclo PDSA (_Plan-Do-Study-Act_) para la mejora que se muestra en la figura \@ref(fig:ciclo). Básicamente consiste en:

1. Planifica un cambio o prueba, dirigido a la **mejora**.
2. Lleva a cabo el cambio o prueba, preferiblemente de corto alcance.
3. Estudia el resultado: ¿qué has aprendido? ¿qué ha ido mal?
4. Actúa:
    - Adopta el cambio.
    - Abandónalo.
    - Empieza el ciclo de nuevo.
  
Una cuestión fundamental es la necesidad de **documentar** todas las acciones de mejora, así como las lecciones aprendidas durante el proceso.


```{r ciclo, echo=FALSE, fig.cap="Ciclo PDSA para la mejora", fig.align='center', purl = FALSE}
library(grid)
grid.newpage()
grid.rect(gp=gpar(fill="white"))
grid.circle(r=0.45)
grid.text("Plan", 0.7, 0.7, gp=gpar(cex=2))
grid.text("Do", 0.7, 0.3, gp=gpar(cex=2))
grid.text("Study", 0.3, 0.3, gp=gpar(cex=2))
grid.text("Act", 0.3, 0.7, gp=gpar(cex=2))
grid.lines(c(0.95, 0.95), c(0.51, 0.49), arrow = arrow(type = "closed"), 
    gp = gpar(fill = "black"))
grid.lines(c(0.51, 0.49), c(0.07, 0.07), arrow = arrow(type = "closed"), 
    gp = gpar(fill = "black"))
grid.lines(c(0.05, 0.05), c(0.49, 0.51), arrow = arrow(type = "closed"), 
    gp = gpar(fill = "black"))
grid.lines(c(0.49, 0.51), c(0.93, 0.93), arrow = arrow(type = "closed"), 
    gp = gpar(fill = "gray90"))
grid.lines(c(0.5, 0.5), c(0.07, 0.93))
grid.lines(c(0.05, 0.95), c(0.5, 0.5))
```



Una buena forma de empezar el ciclo es a partir de un análisis de causa y
efecto, por ejemplo con un diagrama de Ishikawa o de espina de pescado (_fishbone_) como el que aparece en la
figura \@ref(fig:cefecto). Con esta técnica se identifica cuál es la variable respuesta que queremos investigar (efecto, cabeza del pescado) y cuáles son los posibles factores que pueden estar relacionados, y de los que en última instancia queremos averiguar si son la causa del efecto (espinas).

```{r cefecto, echo=FALSE, fig.align="center", fig.cap="Ejemplo diagrama de causa-efecto", results='hide', fig.show='hold', purl = FALSE}
library(SixSigma)
example(ss.ceDiag)
```

Lo siguiente probablemente sería determinar el presupuesto/recursos disponibles,
en especial determinar el número de experimentos que se pueden realizar
realísticamente.



Hasta ahora, hemos ido mencionando algunos conceptos básicos del diseño de
experimentos. Ahora vamos a definirlos un poco más formalmente.

- **Variable respuesta**:  La variable de interés que pretendemos
  optimizar o determinar sus causas. En el ámbito industrial, será una cuantificación de alguna característica de
  calidad, en sentido amplio.
- **Factor**: Variable independiente que puede ser causa de la respuesta.
  La inferencia que haremos con DoE será confirmar o rechazar esta
  hipótesis, así como la cuantificación de este efecto.
- **Variable de bloque**: Variable que no tiene interés en la investigación,
  pero puede influir en la respuesta. Mediante la formación de bloques
  aleatorios confundimos su efecto con los factores que realmente nos interesan.
- **Variable ruido**: Variable que puede influir en la respuesta, pero de la
  que no tenemos control.
- **Nivel**: Valor que fijamos de un factor. En variables
  cualitativas, es una categoría. En variables cuantitativas, un valor numérico
  determinado fijado con antelación.
  A menudo se le llama también tratamiento, aunque cuando se estudia más de un factor sería más apropiado hablar de tratamiento como combinación de los niveles de los factores.
- **Unidad experimental**: La división más pequeña posible de unidades de un
  experimento tal que a dos cualesquiera se les pueden aplicar distintas
  combinaciones de factores y niveles.
- **Unidad observable**: Cada uno de los elementos que forman la unidad
  experimental. A veces, un tratamiento no se puede aplicar a un solo
  elemento, sino a varios a la vez.
- **Bloque**: Grupos de unidades experimentales que son tratados de forma
  similar en el experimento.
- **Efecto**: El principal resultado de interés del experimento: qué pasa con
  la variable respuesta para cada nivel de un factor o combinación de factores.
- **Réplica**: Repetición de un experimento sobre una misma combinación de
  factores y niveles, a **diferentes** unidades experimentales.
- **Repetición**: Repetición de la medición de la respuesta con las mismas
  condiciones experimentales, a **la misma** unidad experimental.
- **Aleatorización**: Asignación de niveles y bloques a unidades
  experimentales de forma aleatoria.




Al utilizar un modelo para simplificar la realidad, estamos cometiendo un
error. El **error experimental** es aquel que se debe exclusivamente a las réplicas
de las mismas condiciones experimentales. En cada diseño el error experimental
se calcula de una forma distinta, de forma que se separa de la variabilidad total
para ver cuánta variación se debe al modelo y poder así tomar decisiones. Así, en el modelo general:

$$Y = f(\mathbf{X}) + \varepsilon,$$

- $Y$ es la variable respuesta.
- $\mathbf{X}$ es el conjunto de variables predictivas.
- $f$ es una función (lineal, exponencial, etc.)
- $\varepsilon$ es una variable aleatoria que recoge todo lo que no explica la función $f$ de la variable respuesta $Y$.


Los siguientes principios son cruciales a la hora de diseñar el experimento.

- **Aleatorización**. Los tratamientos deben ser asignados de forma aleatoria a las 
unidades experimentales. Esto incluye bloques, factores controlables, anidamientos, 
etc.

- **Formación de bloques**. Cuando no se puedan replicar exactamente las condiciones 
experimentales (por ejemplo, días diferentes), se deben organizar en bloques.

- **Réplicas**. Para poder estimar el error experimental y hacer contrastes de 
hipótesis, es necesario tener más de una _corrida_ de cada combinación de tratamientos.




@lawson2015 propone la siguiente _checklist_ a la hora de planificar
experimentos:

1. Definir objetivos.
2. Identificar unidades experimentales.
3. Definir variable respuesta medible y con sentido.
4. Identificar los factores controlables y latentes.
5. Ejecutar pruebas piloto.
6. Hacer diagrama de flujo para cada experimento.
7. Elegir el **diseño experimental**. Esta parte es crucial.
8. Determinar el número de réplicas necesarias.
9. Aleatorizar las condiciones experimentales a las unidades experimentales.
10. Definir método de análisis de datos.
11. Calendario y presupuesto para la ejecución.

Es aconsejable recoger toda esta información en un formulario estandarizado que forme parte de la documentación del experimento o proyecto de mejora.




## Tipos de diseños de experimentos

Existen una enorme cantidad de posibles tipos de diseños experimentales. La decisión sobre cuál utilizar en cada situación concreta depende principalmente de los objetivos de la investigación y de las características de las unidades experimentales (de ahí la importancia de caracterizarlas de forma precisa durante el diseño). La figura \@ref(fig:tiposdisenos) muestra la categorización que hace @lawson2015. A continuación se enumeran estos  diseños con una breve explicación de cada uno.

```{r tiposdisenos, fig.align='center', echo = FALSE, fig.cap="Esquema de diseños experimentales en Lawson (2015)", out.width="100%", purl = FALSE}
library(DiagrammeR)
grViz("
digraph one_factor{
  graph [layout=dot, rankdir = TB, compound = true, fontsize = 10, color = crimson]
  node [shape = box]
  'Propósito\ndel diseño'; 'RSE'; LSD;RCD  'RCB';  'GCB'; 'PBIB,PTIB'; 'BIB'; 'Tamaño de bloque' ;'FRSE\nNSE\nSNSE'; 'CRFD\nCRFF\nPB,OA'; CRRS; 'SLD\nSCD\nEVD'; RCBF; RBSP; 'CRSP\nSPFF'; 'RSSP\nEESPRS'; SPMPV; BRS; PCBF; CCBF
  
  node [shape = diamond]
  'Unidades experimentales'
  'Factores de bloque'
  
  
  'Unidades experimentales' -> 'Factores de bloque' [label = 'Heterogéneos'] 
  'Unidades experimentales' -> 'CRD' [label = 'Homogéneos'] 
  'Factores de bloque' -> 'Tamaño de bloque' [label = 'Uno'] 
  'Factores de bloque' -> 'LSD' [label = 'Dos'] 
  LSD -> RCD
  'Tamaño de bloque' -> 'RCB' [label = 'Grande'] 
  'Tamaño de bloque' -> 'PBIB,PTIB' [label = 'Pequeño'] 
  'Propósito\ndel diseño' -> RSE [label = 'Estudiar varianzas'] 
  RCB -> GCB
  
    'Propósito\ndel diseño' -> 'Unidades experimentales' [label = 'Estimar efectos\nde los factores'] 

  
  
  subgraph cluster0{
    label = 'Un factor';
    RSE
    CRD
    LSD
    RCD
    RCB
    GCB
    'PBIB,PTIB'
    'BIB'
  }
  RSE -> 'FRSE\nNSE\nSNSE'
  
  subgraph cluster1{
  label = 'Varios factores';
    'FRSE\nNSE\nSNSE'
        CRD -> 'Tipo factor'
    'Tipo factor' -> 'CRFD\nCRFF\nPB,OA' [label = 'Categórico']
    'Tipo factor' -> CRRS [label = 'Continuo']
    'Tipo factor' -> 'SLD\nSCD\nEVD' [label = 'Mezcla']
    GCB -> RCBF
    'PBIB,PTIB' -> 'BIB'
    BIB -> Factores
    Factores -> BRS [label = 'Continuos']
    Factores -> PCBF [label = 'Categóricos']
    PCBF -> CCBF
  } 
  subgraph cluster2{
  label = 'Varios factores (algunos difícil de variar)';
    RCBF -> RBSP
    'CRFD\nCRFF\nPB,OA' -> 'CRSP\nSPFF'
    'CRRS' -> 'RSSP\nEESPRS'
    'SLD\nSCD\nEVD' -> SPMPV
    
  } 
}  
")
# library(htmltools)
# html_print(HTML(DiagrammeRsvg::export_svg(g)))
# print(g)
```



* **CRD**, _Completely Randomized Design_: **Diseño completamente aleatorizado**. Este diseño se debería utilizar cuando solo hay un factor en estudio y las unidades experimentales son homogéneas. El análisis se corresponde con el ANOVA estudiado en el capítulo \@ref(anova).

* **CRFD**, _Completely Randomized Factorial Design_: **Diseño factorial completamente aleatorizado**. Este diseño se utiliza cuando se estudia más de un factor a la vez. Permite estudiar las interacciones entre los factores. El análisis se corresponde también con el ANOVA estudiado en el capítulo \@ref(anova). Un caso particular son los diseños a dos niveles o $2^k$, que veremos en detalle más adelante.

* **CRRS**, _Completely Randomized Response Surface_: **Superficie respuesta completamente aleatorizado**. Este diseño se utiliza cuando se estudian dos o tres factores continuos en los que puede aparecer curvatura y por tanto los valores que optimizan la respuesta pueden no estar en el perímetro de la región de diseño. Se suelen aplicar en la fase de caracterización de un experimento, después de haber seleccionado en la fase de cribado los factores más significativos.


## Experimentos con un factor (CRD)

Podemos comparar una variable a distintos niveles de un solo
factor. El contraste de la $t$ de Student es la técnica
utilizada para dos niveles. Para más niveles, utilizamos
el análisis de la varianza de un factor (véase el capítulo \@ref(anova).


El diseño experimental para el ANOVA de un factor sigue las siguientes pautas:

1. Se quiere estudiar el efecto de un solo factor sobre una población. No hay otros
factores controlables que puedan influir.

2. Se realiza el plan de recogida de datos, posiblemente con prueba piloto.

3. Se decide el número de unidades experimentales del experimento.

4. Se asignan **aleatoriamente** las unidades a los niveles del factor.

5. Se recogen los datos (experimento físico, cuestionario, etc.)

6. Se realiza un análisis descriptivo, sobre todo gráfico, de los datos recogidos. 

7. Los datos se verifican y se preparan adecuadamente para el análisis.

7. Se ajusta el modelo.

8. Se comprueba la validez del modelo. Si no es válido, se busca modelo alternativo de análisis.

9. Se estiman los parámetros.

10. Se comprueba la hipótesis principal.

11. Si hay diferencias, se realizan comparaciones por pares.

12. Se comprueba la significación práctica y se obtienen conclusiones o se toman
decisiones.













## Diseños factoriales (CRFD)

Cuando analizamos más de un factor a varios niveles, 
el número total de unidades experimentales necesarias para tener una ejecución completa del experimento será $n_1\times n_2, \cdots, n_k$,
donde $n_i$ es el número de niveles del factor $i$, y el número total de factores es $k$. Para tener dos réplicas completas del experimento necesitaremos el doble, y así sucesivamente.

Para el análisis de diseños equilibrados, aplicamos
lo explicado en el apartado \@ref(sec:anova2) para el ANOVA de dos factores con interacción. Recordemos que en
estos diseños es de vital importancia estudiar las interacciones.
El modelo se extiende inmediatamente a más de dos factores, en los
que las interacciones pueden ser dos a dos, tres a tres, etc. No obstante,
las interacciones de orden superior a 3 rara vez son significativas y se suelen descartar casi siempre.

A veces no es posible tener diseños equilibrados, es decir, con el mismo
número de réplicas en cada "celda" o cruce de niveles de factores. En estos casos, para hacer los contrastes correctos debemos utilizar la función `Anova()` del paquete {car} [@R-car], que calcula las sumas de cuadrados tipo II y tipo III, y los contrastes correctos. Las sumas de cuadrados del diseño equilibrado se suele llamar tipo I. Véase @lawson2015 para una explicación más detallada, o la entrada "Diferentes sumas de cuadrados en ANOVA" de "El blog de los herreros"^[https://erre-que-erre-paco.blogspot.com/2016/12/diferentes-sumas-de-cuadrados-en-anova.html].


:::{.rmdejemplo data-latex=""}
A modo ilustrativo, el siguiente código genera aleatoriamente datos de un supuesto experimento con tres factores: A (con niveles A1, A2 y A3), B (con niveles B1, B2 y B3) y C (con niveles C1 y C2). Las tablas ANOVA del diseño equilibrado son idénticas para los dos métodos. Pero si quitamos uno de los datos (puede ser una situación realista en un experimento tener un valor perdido), entonces las sumas de cuadrados con los distintos métodos no son iguales, y podrían llevar a decisiones distintas.
:::


```{r}
library(car)
set.seed(666)
A <-  paste0("A", 1:3)
B <-  paste0("B", 1:3)
C <-  paste0("C", 1:2)
noeq <- data.frame(
  expand.grid(list(A = A, B = B, C = C)),
  respuesta = rnorm(3*18, 100, 10)
) |> 
  mutate(respuesta = round(if_else(C == "C2", 
                                   respuesta+rnorm(54, 8, 1), 
                                   respuesta), 1))

noeq |> aov(respuesta ~ ., data = _) |> summary()
noeq |> aov(respuesta ~ ., data = _) |> Anova()

noeq$respuesta[54] <- NA
library(car)
noeq |> aov(respuesta ~ ., data = _) |> summary()
noeq |> aov(respuesta ~ ., data = _) |> Anova()
```


A medida que aumenta el número de factores a estudiar, el número de unidades experimentales a utilizar puede hacerse enorme, sobre todo si tenemos más de dos niveles. Para poder analizar un número grande de factores a modo de _screening_ o cribado, se utiliza a menudo un tipo especial de diseños factoriales con solo dos niveles por cada factor, que veremos a continuación por separado dada su importancia.


## Diseños factoriales a dos niveles $2^k$

Un tipo especial de diseño factorial es aquél en el que
todos los factores tienen solamente dos niveles. Es válido tanto para factores puramente categóricos como para factores de tipo continuo. En este último caso, se fijan un valor "bajo" y otro "alto" para el factor, que normalmente se representan como `-` y `+` respectivamente. 
Con posterioridad a la etapa de cribado se pueden añadir más puntos al diseño para afinar en la estimación.
En el caso de atributos, estos niveles alto y bajo serán arbitrarios, pero
en cualquiera de los dos casos, se deberían fijar de acuerdo al conocimiento en la materia para que haya una diferencia potencial entre los niveles.

El número
de experimentos necesarios para probar todas las combinaciones
de niveles para $k$ factores es $2^k$, de ahí su nombre. Por ejemplo,
en un experimento en el que quisiéramos analizar 5 factores con 4 niveles cada uno, necesitaríamos $4\times 4\times 4\times 4\times  4=4^5= 1024$ unidades experimentales, para cada réplica, mientras que en un experimento
$2^5$ serán suficientes 32.

La popularidad de los diseños $2^k$ se debe, entre otras, a las siguientes
causas:

* Como acabamos de ver, con pocas observaciones se pueden analizar muchos factores. Esto nos será muy útil en las fases iniciales para descartar factores que no tienen ningún efecto en la respuesta.
* Son fáciles de utilizar y el análisis se puede realizar con métodos gráficos.
* Estos diseños han demostrado funcionar muy bien con sistemas complejos e incluso no lineales a través del estudio de las interacciones.
* Se pueden analizar incluso más factores utilizando diseños fraccionales.

Los datos de los experimentos factoriales $2^k$ se pueden representar de forma exhaustiva con la llamada "matriz de diseño" o en forma de tablas. La forma estándar de la matriz de diseño contiene en la primera columna signos alternos `-` y `+`. En la siguiente columna se alternarán dos signos `-` y dos signos `+`, y así sucesivamente hasta la última columna con el último factor, que tendrá la primera mitad de signos `-` y la otra mitad de signos `+`. En la matriz de diseño se identifica el orden estándar, pero las observaciones **nunca** se recogerán en el orden de la matriz de diseño. A cada unidad experimental se le asignará una de las combinaciones **aleatoriamente**. Esto se puede hacer con medios mecánicos o con generación de números pseudoaleatorios con el ordenador. En la propia tabla se puede incorporar un identificador de unidad experimental convenientemente aleatorizado. Una vez recogidos los datos, estos se pueden añadir a la tabla, bien cada uno de los datos de las réplicas, o resumidos con la media.

La tabla \@ref(tab:matrizest) muestra una tabla con todos los datos de un hipotético experimento con tres factores sin réplicas, creado con el siguiente código. Se fija la semilla aleatoria para garantizar la reproducibilidad.

```{r matrizest}
set.seed(1)
A <- B <- C <- c("-", "+")
dm <- expand.grid(list(A = A, B = B, C = C)) |> 
  mutate(`Unidad experimental (aleatoria)` = sample(1:8),
         `Respuesta (simulación)` = round(rnorm(8, 10, 2), 1)) |> 
  rownames_to_column("Orden estándar") 
dm |>
  flextable() |> 
  set_caption("Ejemplo de matriz de diseño y datos sin réplicas")
```

La información de la tabla completa se puede compactar poniendo los factores en filas y columnas y los datos en las celdas con los cruces. También es habitual encontrar la representación geométrica en forma de cuadrados (2 factores), cubos (3 factores) o hipercubos (más de 3 factores). Se pueden ver algunos ejemplos en @lawson2015 y @moen2012.

Por la naturaleza dicotómica de los factores, la estimación de los factores es muy sencilla, resultando además que:

$$
\alpha_1 = -\alpha_2;\; \beta_1=-\beta_2.
$$


### Diseño factorial $2^2$

Es el diseño más sencillo que podemos hacer, y solo necesitamos $2^2=4$ unidades experimentales para cada réplica del experimento.

El modelo matemático sería el siguiente:

$$
y_{ijk} = \mu + 
\alpha_i + \beta_j + \alpha\beta_{ij} + \varepsilon_{ijk},
$$

donde $i, j$ toman dos valores, $\{-, +\}$, y $k$ representa las réplicas del experimento, $k = 1, \ldots, n$. Así, el número total de unidades experimentales es $n\times 2^2$. Si solo tenemos una réplica $(n=1)$, entonces podremos estimar los efectos pero no podremos hacer contrastes de hipótesis sobre su significatividad.


::: {.rmdpractica data-latex=""}
En el siguiente código se analiza un experimento con dos factores A y B a dos niveles, `+` y + `-`. Se muestran los gráficos de los efectos.
:::

```{r, message=FALSE}
library(effects)
datosf22 <- scan(text = "
-	-	28
-	-	25	
-	-	27
+	-	36
+	-	32
+	-	32
-	+	18
-	+	19
-	+	23
+	+	31
+	+	30
+	+	29	
", 
    what = list(character(), character(), numeric()),
    sep = "\t") 
datosf22 <- as.data.frame(datosf22)
colnames(datosf22) <- c("A", "B", "respuesta")
datosf22$replica <- rep(1:3, 4)
datosf22 |> 
  pivot_wider(id_cols = c("A", "B"), 
              names_from = "replica", 
              values_from = "respuesta") |> kable()
modelof22 <- lm(respuesta ~ A + B + A:B, data = datosf22)
anova(modelof22)
```


```{r}
library(lattice)
trellis.par.set(background = list(col = "white"))
plot(effect(term = "A", mod = modelof22))
```

```{r}
plot(effect(term = "B", mod = modelof22))
```

```{r}
plot(effect(term = "A:B", mod = modelof22))
```


```{r}
model.tables(aov(modelof22))
```


::: {.rmdejemplo data-latex=""}
Vemos que los dos efectos principales son significativos, pero no lo es la interacción. Podemos eliminar ese término del modelo para así ganar grados de libertad y tener una mejor estimación del error.
:::

```{r}
modelof22 <- lm(respuesta ~ A + B, data = datosf22)
anova(modelof22)
```

```{r}
plot(effect(term = "A", mod = modelof22))
```

```{r}
plot(effect(term = "B", mod = modelof22))
```

### Diseño factorial $2^3$

Modelo:

$$
y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + \alpha\beta_{ij} 
+ \alpha\gamma_{ik}+ \beta\gamma_{jk} + \alpha\beta\gamma_{ijk} + \varepsilon_{ijkl}
$$

De forma similar al diseño $2^2$, obtendríamos estimadores para los efectos principales y las interacciones. Si no tenemos réplicas, podemos omitir la interacción de orden 3 ya que rara vez es significativa, y así podremos estudiar los contrastes del resto de efectos.


::: {.rmdejemplo data-latex=""}
En el siguiente ejemplo, analizamos tres factores, pero omitimos la interacción de orden 3. Después, podríamos eliminar las interacciones menos significativas para quedarnos con el modelo más sencillo.
:::


```{r}
datosf23 <- scan(text = "
-	-	-	60
+	-	-	72	
-	+	-	54
+	+	-	68
-	-	+	52
+	-	+	83
-	+	+	45
+	+	+	80
", 
    what = list(character(), character(), character(), numeric()),
    sep = "\t") 
datosf23 <- as.data.frame(datosf23)
colnames(datosf23) <- c("T", "C", "K", "rendimiento")
kable(datosf23)
modelof23 <- lm(rendimiento ~ T + C + K + T:C + T:K + C:K, data = datosf23)
anova(modelof23)
```

```{r}
plot(effect(term = "T", mod = modelof23))

plot(effect(term = "C", mod = modelof23))
plot(effect(term = "K", mod = modelof23))
plot(effect(term = "T:C", mod = modelof23))
plot(effect(term = "T:K", mod = modelof23))
plot(effect(term = "C:K", mod = modelof23))
```

```{r}
model.tables(aov(modelof23))
```


### Diseño factorial $2^k$

Siguiendo la misma estructura que los dos anteriores, con más efectos principales y más interacciones, pero más allá de 3 es muy difícil que se produzcan, y más difícil de interpretar.
El número de experimentos necesarios aumenta exponencialmente, y se suelen preferir experimentos fraccionados. Cuando no hay grados de libertad suficientes para realizar contrastes se utilizan herramientas gráficas para seleccionar efectos significativos, como el gráfico quantil-quantil de la distribución normal.


:::{.rmdejemplo data-latex=""}
El siguiente ejemplo aparece en @lawson2015. Se estudian 4 factores y no tenemos réplicas. Entonces, obtenemos estimadores pero no contrastes.
:::


```{r}
library(daewr)
modf <- lm(y~A*B*C*D, data = chem)
summary(modf)
LGB(coef(modf)[-1])
```

El método de Lenth se basa en un contraste de significación que puede encontrarse detallado en @conklin2022design. Se calcula un _margin of error_ (ME). Los efectos más allá de ese margen serían significativos. Como se realizan muchos tests, algunos pueden ser identificados como significativos sin serlo. El _simultaneous margin of error_ (SME) tiene en cuenta esta multiplicidad de contrastes.

```{r}
LenthPlot(coef(modf))
```


:::{.rmdpractica data-latex=""}
A continuación se muestran algunas visualizaciones diferentes con el paquete {emmeans}. Primero reducimos el modelo a los efectos significativos
:::

```{r}
library(emmeans)
modf2 <- lm(y~A*B, data = chem)
medias <- emmeans(modf2, ~ A*B)
plot(medias)
emmip(modf2, A~B)
```


### Número de réplicas

Wheeler (1974) estableció la siguiente fórmula aproximada para estimar el número total de unidades experimentales para conseguir una potencia del 95% en el experimento:

$$
N = ((8\sigma)/\Delta)^2,
$$

donde $\sigma$ es la desviación típica del error experimental, y $\Delta$ es el tamaño del efecto que consideraríamos significativo en la práctica. $N$ sería el número total de unidades experimentales, entonces el número de réplicas será $N/2^k$.

### Validación del modelo

Los diseños factoriales son modelos lineales, y como tales deben cumplir:

* Normalidad de los residuos
* Varianza constante

Utilizamos las funciones `shapiro.test()` y `car::leveneTest()` respectivamente para verificarlas. También podemos obtener los gráficos de los residuos con la función `plot()`.


```{r}
modf2 |> residuals() |> shapiro.test()
car::leveneTest(y~A*B, data = chem |> mutate(across(c("A", "B"), factor)))
```


## Diseño con bloques


Cuando hay algún factor más que no es de interés,
pero puede afectar a la variable respuesta, se debe introducir
como variable de bloque. Los diseños de cuadrados latinos
y cuadrados greco-latinos se utilizan para introducir dos o tres
factores de bloque respectivamente en diseños factoriales de un factor.

En los diseños factoriales $2^k$, lo que se hace es "confundir" el efecto de alguna interacción de orden superior con el efecto del bloque. 

Tomemos el ejemplo simulado de la matriz de diseño al inicio de este apartado. Los niveles de las interacciones se asignan multiplicando los signos de los niveles de cada tratamiento.

```{r}
dm |> 
  select(1:4) |> 
  mutate(AB = if_else(as.numeric(paste0(A, "1"))*
                        as.numeric(paste0(B, "1")) >0, 
                      "+", "-"),
         AC = if_else(as.numeric(paste0(A, "1"))*
                        as.numeric(paste0(C, "1")) >0, 
                      "+", "-"),
         BC = if_else(as.numeric(paste0(C, "1"))*
                        as.numeric(paste0(B, "1")) >0, 
                      "+", "-"),
         ABC = if_else(as.numeric(paste0(A, "1"))*
                        as.numeric(paste0(B, "1"))*
                        as.numeric(paste0(C, "1")) >0, 
                      "+", "-")) |> 
  kable()
```

En este caso, podríamos dividir el experimento en dos bloques: uno con las unidades experimentales que se corresponden con los tratamientos 1, 4, 6 y 7 (signo menos de la interacción ABC) y otro con los otros cuatro. De esta forma, el efecto del bloque queda "bloqueado". El bloque puede ser cualquier circunstancia que pueda afectar a la respuesta pero que no nos interesa y normalmente no podemos controlar. Por ejemplo, si se trata de un proceso que requiere un horno, cada bandeja del horno puede dar resultados ligeramente distintos, pero lo que nos interesa es en cada bandeja cómo afectan los factores que sí controlamos.


## Diseños fraccionales

Los diseños factoriales fraccionales $2^{k-p}$ utilizan solo una fracción de su equivalente factorial. En estos diseños se confunden los efectos principales con las interacciones de mayor orden. De esta forma, se puede realizar _screening_ de muchos factores con pocos experimentos, y una vez eliminados del modelo los efectos no significativos se estima mejor el error.


## Otros diseños

Existen otros diseños avanzados que no se tratan en este texto, como son:

  - Plackett and Burman
  - Diseños anidados
  - Split-plot
  - Medidas repetidas

## Diseños de superficie respuesta

En los experimentos de superficie respuesta, los factores en estudio son variables continuas. Es evidente que no podemos estudiar todos los niveles posibles de este tipo de factores, puesto que son infinitos. Sería inviable también realizar los experimentos con un alto número de niveles posibles desde el principio. Por eso, generalmente la metodología de superficie respuesta es realmente el último paso en un diseño experimental con factores continuos. Una secuenciación lógica para un experimento con muchos factores sería la siguiente:

1. Si existen datos históricos, realizar un estudio descriptivo para identificar relaciones.
2. Apoyarse también en la teoría para determinar posibles factores que afecten a la respuesta.
3. En una primera fase de _screening_, analizar muchos factores con diseños factoriales fraccionales.
4. Con lo aprendido de la fase de cribado, descartar los factores no significativos y estimar los efectos.
5. Aplicar la metodología de superficie respuesta:
  * Ampliar los niveles de los factores a estudiar.
  * Ajustar un modelo cuadrático.
  * Optimizar la ecuación del modelo cuadrático para obtener los valores óptimos de los parámetros.
  
Con los modelos a dos niveles, elegiremos como mejor combinación de factores los extremos utilizados como niveles `-` y `+` de cada factor, es decir, en el **perímetro** del experimento. Esto será correcto cuando la relación sea lineal. Pero si la relación es "curvilínea", entonces el óptimo puede estar fuera de los valores probados, que tendremos que buscar ajustando una función no lineal $f$. Para el caso más simple de dos factores $x_1$, $x_2$:

$$y = f(x_1, x_2) + \varepsilon.$$

Esta función se puede aproximar por series de Taylor entorno al centro de la región de diseño, de forma que, en general para $k$ factores:

$$y = \beta_0+\sum\limits_{i=1}^k\beta_i X_i + \sum\limits_{i=1}^k\beta_{ii} X_i^2+\mathop{\sum^k\sum^k}_{i<j}\beta_{ij} X_iX_j+\varepsilon.$$

En el diseño $2^k$, los niveles serán el valor mínimo y el valor máximo elegidos para el experimento (que serán dos valores numéricos. Aquí, las variables $X_i$ estarán "escaladas" a -1 y 1 de la siguiente forma:

$$X_i = \frac{\text{Nivel factor}-\frac{\text{Nivel}_{+} + \text{Nivel}_{-}}{2}}{\frac{\text{Nivel}_{+} - \text{Nivel}_{-}}{2}}.$$


:::{.rmdejemplo data-latex=""}
Si tenemos un experimento con temperaturas fijadas en 10 y 20 grados centígrados:

$$X_{10}=\frac{10-\frac{10+20}{2}}{\frac{20-10}{2}}= \frac{-5}{5}= -1$$
$$X_{20}=\frac{20-\frac{10+20}{2}}{\frac{20-10}{2}}= \frac{5}{5}= +1$$
:::

Esta **forma cuadrática** puede recoger una gran variedad de situaciones no lineales en la región de diseño. Ahora bien, son necesarios al menos tres puntos para recoger la curvatura. Entonces, si partimos de un diseño de cribado $2^k$ o $2^{k-p}$, lo que haremos será ampliar el experimento tomando nuevos valores. Una técnica habitual es el diseño CCD (_Central Composite Design_). Para cada factor, se utilizan cinco niveles:

1. Los dos utilizados en el diseño $2^k$.
2. Un punto central en cada factor $(X_i=0)$.
3. Dos puntos fuera de la región del diseño $2^k$, o puntos axiales. En general, se toma el punto $\alpha = \sqrt{k}$ y el $-\alpha$

El número de unidades experimentales adicionales puede ser muy grande si se identifican más de 3 factores determinantes. Para 3 factores, un CCD necesitará 19 unidades experimentales. O lo que es lo mismo, $19-2^3 = 11$ experimentos adicionales.

Antes de ampliar el experimento con nuevos puntos, se pueden hacer comprobaciones para ver si el modelo cuadrático es previsible que mejore el resultado. Si el modelo lineal es adecuado, los puntos axiales y centrales adicionales no serán necesarios. Esto se puede hacer con el paquete `rsm` de R.


:::{.rmdejemplo data-latex=""}
El conjunto de datos `cement` del paquete {daewr} contiene 20 mediciones de un experimento de superficie respuesta. 
:::


```{r cement}
library(daewr)
cement
```

```{r}
library(rsm)
grout.lin <- rsm(y ~ SO(x1, x2, x3), data = cement, subset = Block == 1)
anova(grout.lin)
```

En esta salida, la variabilidad se descompone en tres partes:

* FO: Los tres términos lineales simultáneamente
* TWI: Los tres términos de interacción simultáneamente
* PQ: Términos cuadráticos, es decir, si se aleja de la linealidad

En el test F en el término PQ la hipótesis nula es que los datos se ajustan al modelo lineal. Como el p-valor es pequeño, se rechaza esta hipótesis y el modelo cuadrático es más adecuado. Entonces concluiríamos que es necesario completar el experimento con los puntos axiales.

Una vez se ajusta un modelo de superficie respuesta con los datos de un experimento, querremos determinar los valores óptimos de los factores. La forma más sencilla es mediante gráficos de contorno (o curvas de nivel) para cada para de factores. Métodos más precisos de optimización nos darán valores numéricos.



:::{.rmdejemplo data-latex=""}
El conjunto de datos `Treb` del paquete {daewr} contiene datos experimentales de un fundíbulo (arma medieval de asedio) en el que se mide la distancia dependiendo de tres factores: longitud del brazo (A), contrapeso (B) y peso del proyectil (C). En este caso se ha realizado un diseño de Box-Behnken. El punto estacionario sería un candidato a ser el óptimo. No obstante hay que comprobar si está en la región de diseño (y si es un máximo o un mínimo). El método ridge buscará el máximo en un radio determinado (hasta cubrir la región de diseño).
:::

```{r}
Treb
treb.quad <- rsm(y ~ SO(x1, x2, x3), data = Treb)
summary(treb.quad)
```


```{r}
par(mfrow=c(2,2))
contour(treb.quad, ~ x1+x2+x3)
```


```{r}
par(mfrow=c(2,2))
contour(treb.quad, ~ x1+x2+x3, at = xs(treb.quad))
```


```{r}
par(mfrow=c(2,2))
persp(treb.quad, ~x1+x2+x3, zlab = "Distancia")
```

```{r}
ridge <- steepest(treb.quad, dist = seq(0, 1.412, by=0.1), descent = FALSE)
ridge
```

Una última opción sería la optimización no lineal de la ecuación de regresión.


Los diseños de superficie respuesta también se pueden plantear sin fase de screening, si se tienen muy claros los factores a estudiar.

```{r}
library(daewr)
Design<-DefScreen(m=8,c=0)
Design
```


```{r}
library(daewr)
design<-DefScreen(m=5,c=0,center=2,randomize=FALSE)
design
```

```{r}
library(daewr)
Smeso<-c(241,295,260,338,320,265,275,248,92.5,383,313,305,304)
FitDefSc(Smeso,design,alpha=.05)
```

```{r}
library(daewr)
FitDefSc(Smeso,design,alpha=.1)
```



